{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "name": "mini_projeto_02",
      "provenance": [],
      "collapsed_sections": [
        "rd6Re8EWU606",
        "LIXPdRHLU620",
        "4oiTFImQU622",
        "wRVK2uXjU63o",
        "h5tKnxesU63p",
        "tY7r3842U63r",
        "Fh5KIclXU64W",
        "Yd2y6eHEU64w",
        "1Bq_Qxd_U64x",
        "Xthu2oJXU66D"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/viniciuspdasilva/pos-ia/blob/master/mini_projeto_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlVIbaUXU6yb",
        "colab_type": "text"
      },
      "source": [
        "# Classificação de Sentimentos com Redes Neurais\n",
        "\n",
        "Este notebook foi adaptado do [github da Udacity para o Deep Learning Nanodegree](https://github.com/udacity/deep-learning-v2-pytorch) com a metodologia de Andrew Trask."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEwlx0bxU6ye",
        "colab_type": "text"
      },
      "source": [
        "### Visão Geral do Tutorial\n",
        "\n",
        "- [Analisando o Dataset](#lesson_1)\n",
        "- [Desenvolvendo uma \"Teoria de Predição\"](#lesson_2)\n",
        "- [**PROJETO 1**: Rápida Validação da Teoria](#project_1)\n",
        "\n",
        "\n",
        "- [Transformando Texto em Números](#lesson_3)\n",
        "- [**PROJETO 2**: Criando Dados de Entrada/Saída](#project_2)\n",
        "\n",
        "\n",
        "- [**PROJETO 3**: Construindo Nossa Rede Neural](#project_3)\n",
        "\n",
        "\n",
        "- [Entendendo Ruído da Rede Neural](#lesson_4)\n",
        "- [**PROJETO 4**: Acelerando o Aprendizado através da Redução de Ruído](#project_4)\n",
        "\n",
        "\n",
        "- [Mais Redução de Ruído](#lesson_5)\n",
        "- [**PROJETO 5**: Reduzindo o Ruído através da Redução Estratégica do Vocabulário](#project_5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbpresent": {
          "id": "56bb3cba-260c-4ebe-9ed6-b995b4c72aa3"
        },
        "id": "GjrRzPxCU6yg",
        "colab_type": "text"
      },
      "source": [
        "# Analisando o Dataset<a id='lesson_1'></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbpresent": {
          "id": "eba2b193-0419-431e-8db9-60f34dd3fe83"
        },
        "id": "HHBetCEyU6yi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "ed63e8fd-d451-4502-9da5-12939acd9202"
      },
      "source": [
        "!mkdir data\n",
        "!wget -c https://github.com/agungsantoso/deep-learning-v2-pytorch/raw/master/sentiment-rnn/data/labels.txt\n",
        "!wget -c https://github.com/agungsantoso/deep-learning-v2-pytorch/raw/master/sentiment-rnn/data/reviews.txt\n",
        "!mv *.txt data/\n",
        "def pretty_print_review_and_label(i):\n",
        "    print(labels[i] + \"\\t:\\t\" + reviews[i][:80] + \"...\")\n",
        "\n",
        "g = open('data/reviews.txt','r')\n",
        "reviews = list(map(lambda x:x[:-1], g.readlines()))\n",
        "g.close()\n",
        "\n",
        "g = open('data/labels.txt','r')\n",
        "labels = list(map(lambda x:x[:-1].upper(), g.readlines()))\n",
        "g.close()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-30 13:44:33--  https://github.com/agungsantoso/deep-learning-v2-pytorch/raw/master/sentiment-rnn/data/labels.txt\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/agungsantoso/deep-learning-v2-pytorch/master/sentiment-rnn/data/labels.txt [following]\n",
            "--2020-06-30 13:44:34--  https://raw.githubusercontent.com/agungsantoso/deep-learning-v2-pytorch/master/sentiment-rnn/data/labels.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 225000 (220K) [text/plain]\n",
            "Saving to: ‘labels.txt’\n",
            "\n",
            "labels.txt          100%[===================>] 219.73K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2020-06-30 13:44:35 (3.99 MB/s) - ‘labels.txt’ saved [225000/225000]\n",
            "\n",
            "--2020-06-30 13:44:38--  https://github.com/agungsantoso/deep-learning-v2-pytorch/raw/master/sentiment-rnn/data/reviews.txt\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/agungsantoso/deep-learning-v2-pytorch/master/sentiment-rnn/data/reviews.txt [following]\n",
            "--2020-06-30 13:44:38--  https://raw.githubusercontent.com/agungsantoso/deep-learning-v2-pytorch/master/sentiment-rnn/data/reviews.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 33678267 (32M) [text/plain]\n",
            "Saving to: ‘reviews.txt’\n",
            "\n",
            "reviews.txt         100%[===================>]  32.12M  40.8MB/s    in 0.8s    \n",
            "\n",
            "2020-06-30 13:44:43 (40.8 MB/s) - ‘reviews.txt’ saved [33678267/33678267]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCEjgRaCU6yw",
        "colab_type": "text"
      },
      "source": [
        "**Nota:** Os dados em `reviews.txt` que estamos utilizando já passaram por algumas etapas de pré-processamento e contêm apenas caracteres em letras minúsculas. Isso deve ser feito para podermos lidar com palavras iguais escritas de formas diferentes, como `The`, `the` e `THE`, que são convertidas para `the`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIDKSrQPU6yx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9247e938-0978-4ad7-9cf5-f15276391c2b"
      },
      "source": [
        "len(reviews)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbpresent": {
          "id": "bb95574b-21a0-4213-ae50-34363cf4f87f"
        },
        "id": "SaBU8TKxU6y4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "4782458c-8502-4f6f-d78f-5b0793c82498"
      },
      "source": [
        "reviews[0]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbpresent": {
          "id": "e0408810-c424-4ed4-afb9-1735e9ddbd0a"
        },
        "id": "ZXA9DIS9U6y_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "74798449-819d-4a1f-e2d6-3c65fcb0696b"
      },
      "source": [
        "labels[0]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'POSITIVE'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzj-POxZU6zH",
        "colab_type": "text"
      },
      "source": [
        "# Desenvolvendo uma \"Teoria de Predição\"<a id='lesson_2'></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbpresent": {
          "id": "e67a709f-234f-4493-bae6-4fb192141ee0"
        },
        "id": "wB5z2K3dU6zJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "b797cc5e-911f-4f50-d1c2-99870e62b300"
      },
      "source": [
        "print(\"labels.txt \\t : \\t reviews.txt\\n\")\n",
        "pretty_print_review_and_label(2137)\n",
        "pretty_print_review_and_label(12816)\n",
        "pretty_print_review_and_label(6267)\n",
        "pretty_print_review_and_label(21934)\n",
        "pretty_print_review_and_label(5297)\n",
        "pretty_print_review_and_label(4998)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "labels.txt \t : \t reviews.txt\n",
            "\n",
            "NEGATIVE\t:\tthis movie is terrible but it has some good effects .  ...\n",
            "POSITIVE\t:\tadrian pasdar is excellent is this film . he makes a fascinating woman .  ...\n",
            "NEGATIVE\t:\tcomment this movie is impossible . is terrible  very improbable  bad interpretat...\n",
            "POSITIVE\t:\texcellent episode movie ala pulp fiction .  days   suicides . it doesnt get more...\n",
            "NEGATIVE\t:\tif you haven  t seen this  it  s terrible . it is pure trash . i saw this about ...\n",
            "POSITIVE\t:\tthis schiffer guy is a real genius  the movie is of excellent quality and both e...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RzqA5u_U6zR",
        "colab_type": "text"
      },
      "source": [
        "# Projeto 1: Rápida Validação da Teoria<a id='project_1'></a>\n",
        "\n",
        "Tenha em mente que a forma como resolveremos nossos problemas nesses projetos é apenas uma de várias possíveis.\n",
        "\n",
        "Um link extra que pode vir a ser útil em seus estudos é a documentação da classe [Counter](https://docs.python.org/2/library/collections.html#collections.Counter)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hz1uCDjIU6zS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "import numpy as np"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMIrPGtvU6zd",
        "colab_type": "text"
      },
      "source": [
        "Iremos criar 3 objetos `Counter`: um para palavras de reviews positivas; uma para palavras de reviews negativas; e um para todas as palavras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQ0O8p-hU6zg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive_counts = Counter()\n",
        "negative_counts = Counter()\n",
        "total_counts = Counter()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoGENZjxU6zm",
        "colab_type": "text"
      },
      "source": [
        "Agora, iremos examinar todas as reviews. Para cada palavra em uma review positiva, iremos incrementar o contador para aquela palavra tanto no contador positivo quando no contador totarl. Faremos o mesmo processo para as palavras de reviews negativas.\n",
        "\n",
        "**Nota:** Utilizaremos a função `split(' ')` para dividir as reviews em palavras individualmente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLaH_qf9U6zn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(reviews)):\n",
        "    if(labels[i] == 'POSITIVE'):\n",
        "        for word in reviews[i].split(' '):\n",
        "            positive_counts[word] += 1\n",
        "            total_counts[word] += 1\n",
        "    else:\n",
        "        for word in reviews[i].split(' '):\n",
        "            negative_counts[word] += 1\n",
        "            total_counts[word] += 1"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56HQri1TU6zt",
        "colab_type": "text"
      },
      "source": [
        "As duas células abaixo listam todas as palavras utilizadas nas reviews positivas e nas negativas, ordenadas das mais frequentes para as menos frequentes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQ6O-njdU6zu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b3dd40fd-ef00-40a4-a889-2816ceac6078"
      },
      "source": [
        "positive_counts.most_common()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('', 550468),\n",
              " ('the', 173324),\n",
              " ('.', 159654),\n",
              " ('and', 89722),\n",
              " ('a', 83688),\n",
              " ('of', 76855),\n",
              " ('to', 66746),\n",
              " ('is', 57245),\n",
              " ('in', 50215),\n",
              " ('br', 49235),\n",
              " ('it', 48025),\n",
              " ('i', 40743),\n",
              " ('that', 35630),\n",
              " ('this', 35080),\n",
              " ('s', 33815),\n",
              " ('as', 26308),\n",
              " ('with', 23247),\n",
              " ('for', 22416),\n",
              " ('was', 21917),\n",
              " ('film', 20937),\n",
              " ('but', 20822),\n",
              " ('movie', 19074),\n",
              " ('his', 17227),\n",
              " ('on', 17008),\n",
              " ('you', 16681),\n",
              " ('he', 16282),\n",
              " ('are', 14807),\n",
              " ('not', 14272),\n",
              " ('t', 13720),\n",
              " ('one', 13655),\n",
              " ('have', 12587),\n",
              " ('be', 12416),\n",
              " ('by', 11997),\n",
              " ('all', 11942),\n",
              " ('who', 11464),\n",
              " ('an', 11294),\n",
              " ('at', 11234),\n",
              " ('from', 10767),\n",
              " ('her', 10474),\n",
              " ('they', 9895),\n",
              " ('has', 9186),\n",
              " ('so', 9154),\n",
              " ('like', 9038),\n",
              " ('about', 8313),\n",
              " ('very', 8305),\n",
              " ('out', 8134),\n",
              " ('there', 8057),\n",
              " ('she', 7779),\n",
              " ('what', 7737),\n",
              " ('or', 7732),\n",
              " ('good', 7720),\n",
              " ('more', 7521),\n",
              " ('when', 7456),\n",
              " ('some', 7441),\n",
              " ('if', 7285),\n",
              " ('just', 7152),\n",
              " ('can', 7001),\n",
              " ('story', 6780),\n",
              " ('time', 6515),\n",
              " ('my', 6488),\n",
              " ('great', 6419),\n",
              " ('well', 6405),\n",
              " ('up', 6321),\n",
              " ('which', 6267),\n",
              " ('their', 6107),\n",
              " ('see', 6026),\n",
              " ('also', 5550),\n",
              " ('we', 5531),\n",
              " ('really', 5476),\n",
              " ('would', 5400),\n",
              " ('will', 5218),\n",
              " ('me', 5167),\n",
              " ('had', 5148),\n",
              " ('only', 5137),\n",
              " ('him', 5018),\n",
              " ('even', 4964),\n",
              " ('most', 4864),\n",
              " ('other', 4858),\n",
              " ('were', 4782),\n",
              " ('first', 4755),\n",
              " ('than', 4736),\n",
              " ('much', 4685),\n",
              " ('its', 4622),\n",
              " ('no', 4574),\n",
              " ('into', 4544),\n",
              " ('people', 4479),\n",
              " ('best', 4319),\n",
              " ('love', 4301),\n",
              " ('get', 4272),\n",
              " ('how', 4213),\n",
              " ('life', 4199),\n",
              " ('been', 4189),\n",
              " ('because', 4079),\n",
              " ('way', 4036),\n",
              " ('do', 3941),\n",
              " ('made', 3823),\n",
              " ('films', 3813),\n",
              " ('them', 3805),\n",
              " ('after', 3800),\n",
              " ('many', 3766),\n",
              " ('two', 3733),\n",
              " ('too', 3659),\n",
              " ('think', 3655),\n",
              " ('movies', 3586),\n",
              " ('characters', 3560),\n",
              " ('character', 3514),\n",
              " ('don', 3468),\n",
              " ('man', 3460),\n",
              " ('show', 3432),\n",
              " ('watch', 3424),\n",
              " ('seen', 3414),\n",
              " ('then', 3358),\n",
              " ('little', 3341),\n",
              " ('still', 3340),\n",
              " ('make', 3303),\n",
              " ('could', 3237),\n",
              " ('never', 3226),\n",
              " ('being', 3217),\n",
              " ('where', 3173),\n",
              " ('does', 3069),\n",
              " ('over', 3017),\n",
              " ('any', 3002),\n",
              " ('while', 2899),\n",
              " ('know', 2833),\n",
              " ('did', 2790),\n",
              " ('years', 2758),\n",
              " ('here', 2740),\n",
              " ('ever', 2734),\n",
              " ('end', 2696),\n",
              " ('these', 2694),\n",
              " ('such', 2590),\n",
              " ('real', 2568),\n",
              " ('scene', 2567),\n",
              " ('back', 2547),\n",
              " ('those', 2485),\n",
              " ('though', 2475),\n",
              " ('off', 2463),\n",
              " ('new', 2458),\n",
              " ('your', 2453),\n",
              " ('go', 2440),\n",
              " ('acting', 2437),\n",
              " ('plot', 2432),\n",
              " ('world', 2429),\n",
              " ('scenes', 2427),\n",
              " ('say', 2414),\n",
              " ('through', 2409),\n",
              " ('makes', 2390),\n",
              " ('better', 2381),\n",
              " ('now', 2368),\n",
              " ('work', 2346),\n",
              " ('young', 2343),\n",
              " ('old', 2311),\n",
              " ('ve', 2307),\n",
              " ('find', 2272),\n",
              " ('both', 2248),\n",
              " ('before', 2177),\n",
              " ('us', 2162),\n",
              " ('again', 2158),\n",
              " ('series', 2153),\n",
              " ('quite', 2143),\n",
              " ('something', 2135),\n",
              " ('cast', 2133),\n",
              " ('should', 2121),\n",
              " ('part', 2098),\n",
              " ('always', 2088),\n",
              " ('lot', 2087),\n",
              " ('another', 2075),\n",
              " ('actors', 2047),\n",
              " ('director', 2040),\n",
              " ('family', 2032),\n",
              " ('between', 2016),\n",
              " ('own', 2016),\n",
              " ('m', 1998),\n",
              " ('may', 1997),\n",
              " ('same', 1972),\n",
              " ('role', 1967),\n",
              " ('watching', 1966),\n",
              " ('every', 1954),\n",
              " ('funny', 1953),\n",
              " ('doesn', 1935),\n",
              " ('performance', 1928),\n",
              " ('few', 1918),\n",
              " ('bad', 1907),\n",
              " ('look', 1900),\n",
              " ('re', 1884),\n",
              " ('why', 1855),\n",
              " ('things', 1849),\n",
              " ('times', 1832),\n",
              " ('big', 1815),\n",
              " ('however', 1795),\n",
              " ('actually', 1790),\n",
              " ('action', 1789),\n",
              " ('going', 1783),\n",
              " ('bit', 1757),\n",
              " ('comedy', 1742),\n",
              " ('down', 1740),\n",
              " ('music', 1738),\n",
              " ('must', 1728),\n",
              " ('take', 1709),\n",
              " ('saw', 1692),\n",
              " ('long', 1690),\n",
              " ('right', 1688),\n",
              " ('fun', 1686),\n",
              " ('fact', 1684),\n",
              " ('excellent', 1683),\n",
              " ('around', 1674),\n",
              " ('didn', 1672),\n",
              " ('without', 1671),\n",
              " ('thing', 1662),\n",
              " ('thought', 1639),\n",
              " ('got', 1635),\n",
              " ('each', 1630),\n",
              " ('day', 1614),\n",
              " ('feel', 1597),\n",
              " ('seems', 1596),\n",
              " ('come', 1594),\n",
              " ('done', 1586),\n",
              " ('beautiful', 1580),\n",
              " ('especially', 1572),\n",
              " ('played', 1571),\n",
              " ('almost', 1566),\n",
              " ('want', 1562),\n",
              " ('yet', 1556),\n",
              " ('give', 1553),\n",
              " ('pretty', 1549),\n",
              " ('last', 1543),\n",
              " ('since', 1519),\n",
              " ('different', 1504),\n",
              " ('although', 1501),\n",
              " ('gets', 1490),\n",
              " ('true', 1487),\n",
              " ('interesting', 1481),\n",
              " ('job', 1470),\n",
              " ('enough', 1455),\n",
              " ('our', 1454),\n",
              " ('shows', 1447),\n",
              " ('horror', 1441),\n",
              " ('woman', 1439),\n",
              " ('tv', 1400),\n",
              " ('probably', 1398),\n",
              " ('father', 1395),\n",
              " ('original', 1393),\n",
              " ('girl', 1390),\n",
              " ('point', 1379),\n",
              " ('plays', 1378),\n",
              " ('wonderful', 1372),\n",
              " ('far', 1358),\n",
              " ('course', 1358),\n",
              " ('john', 1350),\n",
              " ('rather', 1340),\n",
              " ('isn', 1328),\n",
              " ('ll', 1326),\n",
              " ('later', 1324),\n",
              " ('dvd', 1324),\n",
              " ('whole', 1310),\n",
              " ('war', 1310),\n",
              " ('d', 1307),\n",
              " ('found', 1306),\n",
              " ('away', 1306),\n",
              " ('screen', 1305),\n",
              " ('nothing', 1300),\n",
              " ('year', 1297),\n",
              " ('once', 1296),\n",
              " ('hard', 1294),\n",
              " ('together', 1280),\n",
              " ('set', 1277),\n",
              " ('am', 1277),\n",
              " ('having', 1266),\n",
              " ('making', 1265),\n",
              " ('place', 1263),\n",
              " ('might', 1260),\n",
              " ('comes', 1260),\n",
              " ('sure', 1253),\n",
              " ('american', 1248),\n",
              " ('play', 1245),\n",
              " ('kind', 1244),\n",
              " ('perfect', 1242),\n",
              " ('takes', 1242),\n",
              " ('performances', 1237),\n",
              " ('himself', 1230),\n",
              " ('worth', 1221),\n",
              " ('everyone', 1221),\n",
              " ('anyone', 1214),\n",
              " ('actor', 1203),\n",
              " ('three', 1201),\n",
              " ('wife', 1196),\n",
              " ('classic', 1192),\n",
              " ('goes', 1186),\n",
              " ('ending', 1178),\n",
              " ('version', 1168),\n",
              " ('star', 1149),\n",
              " ('enjoy', 1146),\n",
              " ('book', 1142),\n",
              " ('nice', 1132),\n",
              " ('everything', 1128),\n",
              " ('during', 1124),\n",
              " ('put', 1118),\n",
              " ('seeing', 1111),\n",
              " ('least', 1102),\n",
              " ('house', 1100),\n",
              " ('high', 1095),\n",
              " ('watched', 1094),\n",
              " ('loved', 1087),\n",
              " ('men', 1087),\n",
              " ('night', 1082),\n",
              " ('anything', 1075),\n",
              " ('believe', 1071),\n",
              " ('guy', 1071),\n",
              " ('top', 1063),\n",
              " ('amazing', 1058),\n",
              " ('hollywood', 1056),\n",
              " ('looking', 1053),\n",
              " ('main', 1044),\n",
              " ('definitely', 1043),\n",
              " ('gives', 1031),\n",
              " ('home', 1029),\n",
              " ('seem', 1028),\n",
              " ('episode', 1023),\n",
              " ('audience', 1020),\n",
              " ('sense', 1020),\n",
              " ('truly', 1017),\n",
              " ('special', 1011),\n",
              " ('second', 1009),\n",
              " ('short', 1009),\n",
              " ('fan', 1009),\n",
              " ('mind', 1005),\n",
              " ('human', 1001),\n",
              " ('recommend', 999),\n",
              " ('full', 996),\n",
              " ('black', 995),\n",
              " ('help', 991),\n",
              " ('along', 989),\n",
              " ('trying', 987),\n",
              " ('small', 986),\n",
              " ('death', 985),\n",
              " ('friends', 981),\n",
              " ('remember', 974),\n",
              " ('often', 970),\n",
              " ('said', 966),\n",
              " ('favorite', 962),\n",
              " ('heart', 959),\n",
              " ('early', 957),\n",
              " ('left', 956),\n",
              " ('until', 955),\n",
              " ('script', 954),\n",
              " ('let', 954),\n",
              " ('maybe', 937),\n",
              " ('today', 936),\n",
              " ('live', 934),\n",
              " ('less', 934),\n",
              " ('moments', 933),\n",
              " ('others', 929),\n",
              " ('brilliant', 926),\n",
              " ('shot', 925),\n",
              " ('liked', 923),\n",
              " ('become', 916),\n",
              " ('won', 915),\n",
              " ('used', 910),\n",
              " ('style', 907),\n",
              " ('mother', 895),\n",
              " ('lives', 894),\n",
              " ('came', 893),\n",
              " ('stars', 890),\n",
              " ('cinema', 889),\n",
              " ('looks', 885),\n",
              " ('perhaps', 884),\n",
              " ('read', 882),\n",
              " ('enjoyed', 879),\n",
              " ('boy', 875),\n",
              " ('drama', 873),\n",
              " ('highly', 871),\n",
              " ('given', 870),\n",
              " ('playing', 867),\n",
              " ('use', 864),\n",
              " ('next', 859),\n",
              " ('women', 858),\n",
              " ('fine', 857),\n",
              " ('effects', 856),\n",
              " ('kids', 854),\n",
              " ('entertaining', 853),\n",
              " ('need', 852),\n",
              " ('line', 850),\n",
              " ('works', 848),\n",
              " ('someone', 847),\n",
              " ('mr', 836),\n",
              " ('simply', 835),\n",
              " ('picture', 833),\n",
              " ('children', 833),\n",
              " ('face', 831),\n",
              " ('keep', 831),\n",
              " ('friend', 831),\n",
              " ('dark', 830),\n",
              " ('overall', 828),\n",
              " ('certainly', 828),\n",
              " ('minutes', 827),\n",
              " ('wasn', 824),\n",
              " ('history', 822),\n",
              " ('finally', 820),\n",
              " ('couple', 816),\n",
              " ('against', 815),\n",
              " ('son', 809),\n",
              " ('understand', 808),\n",
              " ('lost', 807),\n",
              " ('michael', 805),\n",
              " ('else', 801),\n",
              " ('throughout', 798),\n",
              " ('fans', 797),\n",
              " ('city', 792),\n",
              " ('reason', 789),\n",
              " ('written', 787),\n",
              " ('production', 787),\n",
              " ('several', 784),\n",
              " ('school', 783),\n",
              " ('based', 781),\n",
              " ('rest', 781),\n",
              " ('try', 780),\n",
              " ('dead', 776),\n",
              " ('hope', 775),\n",
              " ('strong', 768),\n",
              " ('white', 765),\n",
              " ('tell', 759),\n",
              " ('itself', 758),\n",
              " ('half', 753),\n",
              " ('person', 749),\n",
              " ('sometimes', 746),\n",
              " ('past', 744),\n",
              " ('start', 744),\n",
              " ('genre', 743),\n",
              " ('beginning', 739),\n",
              " ('final', 739),\n",
              " ('town', 738),\n",
              " ('art', 734),\n",
              " ('humor', 732),\n",
              " ('game', 732),\n",
              " ('yes', 731),\n",
              " ('idea', 731),\n",
              " ('late', 730),\n",
              " ('becomes', 729),\n",
              " ('despite', 729),\n",
              " ('able', 726),\n",
              " ('case', 726),\n",
              " ('money', 723),\n",
              " ('child', 721),\n",
              " ('completely', 721),\n",
              " ('side', 719),\n",
              " ('camera', 716),\n",
              " ('getting', 714),\n",
              " ('instead', 712),\n",
              " ('soon', 702),\n",
              " ('under', 700),\n",
              " ('viewer', 699),\n",
              " ('age', 697),\n",
              " ('days', 696),\n",
              " ('stories', 696),\n",
              " ('felt', 694),\n",
              " ('simple', 694),\n",
              " ('roles', 693),\n",
              " ('video', 688),\n",
              " ('name', 683),\n",
              " ('either', 683),\n",
              " ('doing', 677),\n",
              " ('turns', 674),\n",
              " ('wants', 671),\n",
              " ('close', 671),\n",
              " ('title', 669),\n",
              " ('wrong', 668),\n",
              " ('went', 666),\n",
              " ('james', 665),\n",
              " ('evil', 659),\n",
              " ('budget', 657),\n",
              " ('episodes', 657),\n",
              " ('relationship', 655),\n",
              " ('fantastic', 653),\n",
              " ('piece', 653),\n",
              " ('david', 651),\n",
              " ('turn', 648),\n",
              " ('murder', 646),\n",
              " ('parts', 645),\n",
              " ('brother', 644),\n",
              " ('absolutely', 643),\n",
              " ('head', 643),\n",
              " ('experience', 642),\n",
              " ('eyes', 641),\n",
              " ('sex', 638),\n",
              " ('direction', 637),\n",
              " ('called', 637),\n",
              " ('directed', 636),\n",
              " ('lines', 634),\n",
              " ('behind', 633),\n",
              " ('sort', 632),\n",
              " ('actress', 631),\n",
              " ('lead', 630),\n",
              " ('oscar', 628),\n",
              " ('including', 627),\n",
              " ('example', 627),\n",
              " ('known', 625),\n",
              " ('musical', 625),\n",
              " ('chance', 621),\n",
              " ('score', 620),\n",
              " ('already', 619),\n",
              " ('feeling', 619),\n",
              " ('hit', 619),\n",
              " ('voice', 615),\n",
              " ('moment', 612),\n",
              " ('living', 612),\n",
              " ('low', 610),\n",
              " ('supporting', 610),\n",
              " ('ago', 609),\n",
              " ('themselves', 608),\n",
              " ('reality', 605),\n",
              " ('hilarious', 605),\n",
              " ('jack', 604),\n",
              " ('told', 603),\n",
              " ('hand', 601),\n",
              " ('quality', 600),\n",
              " ('moving', 600),\n",
              " ('dialogue', 600),\n",
              " ('song', 599),\n",
              " ('happy', 599),\n",
              " ('matter', 598),\n",
              " ('paul', 598),\n",
              " ('light', 594),\n",
              " ('future', 593),\n",
              " ('entire', 592),\n",
              " ('finds', 591),\n",
              " ('gave', 589),\n",
              " ('laugh', 587),\n",
              " ('released', 586),\n",
              " ('expect', 584),\n",
              " ('fight', 581),\n",
              " ('particularly', 580),\n",
              " ('cinematography', 579),\n",
              " ('police', 579),\n",
              " ('whose', 578),\n",
              " ('type', 578),\n",
              " ('sound', 578),\n",
              " ('view', 573),\n",
              " ('enjoyable', 573),\n",
              " ('number', 572),\n",
              " ('romantic', 572),\n",
              " ('husband', 572),\n",
              " ('daughter', 572),\n",
              " ('documentary', 571),\n",
              " ('self', 570),\n",
              " ('superb', 569),\n",
              " ('modern', 569),\n",
              " ('took', 569),\n",
              " ('robert', 569),\n",
              " ('mean', 566),\n",
              " ('shown', 563),\n",
              " ('coming', 561),\n",
              " ('important', 560),\n",
              " ('king', 559),\n",
              " ('leave', 559),\n",
              " ('change', 558),\n",
              " ('somewhat', 555),\n",
              " ('wanted', 555),\n",
              " ('tells', 554),\n",
              " ('events', 552),\n",
              " ('run', 552),\n",
              " ('career', 552),\n",
              " ('country', 552),\n",
              " ('heard', 550),\n",
              " ('season', 550),\n",
              " ('greatest', 549),\n",
              " ('girls', 549),\n",
              " ('etc', 547),\n",
              " ('care', 546),\n",
              " ('starts', 545),\n",
              " ('english', 542),\n",
              " ('killer', 541),\n",
              " ('tale', 540),\n",
              " ('guys', 540),\n",
              " ('totally', 540),\n",
              " ('animation', 540),\n",
              " ('usual', 539),\n",
              " ('miss', 535),\n",
              " ('opinion', 535),\n",
              " ('easy', 531),\n",
              " ('violence', 531),\n",
              " ('songs', 530),\n",
              " ('british', 528),\n",
              " ('says', 526),\n",
              " ('realistic', 525),\n",
              " ('writing', 524),\n",
              " ('writer', 522),\n",
              " ('act', 522),\n",
              " ('comic', 521),\n",
              " ('thriller', 519),\n",
              " ('television', 517),\n",
              " ('power', 516),\n",
              " ('ones', 515),\n",
              " ('kid', 514),\n",
              " ('york', 513),\n",
              " ('novel', 513),\n",
              " ('alone', 512),\n",
              " ('problem', 512),\n",
              " ('attention', 509),\n",
              " ('involved', 508),\n",
              " ('kill', 507),\n",
              " ('extremely', 507),\n",
              " ('seemed', 506),\n",
              " ('hero', 505),\n",
              " ('french', 505),\n",
              " ('rock', 504),\n",
              " ('stuff', 501),\n",
              " ('wish', 499),\n",
              " ('begins', 498),\n",
              " ('taken', 497),\n",
              " ('sad', 497),\n",
              " ('ways', 496),\n",
              " ('richard', 495),\n",
              " ('knows', 494),\n",
              " ('atmosphere', 493),\n",
              " ('similar', 491),\n",
              " ('surprised', 491),\n",
              " ('taking', 491),\n",
              " ('car', 491),\n",
              " ('george', 490),\n",
              " ('perfectly', 490),\n",
              " ('across', 489),\n",
              " ('team', 489),\n",
              " ('eye', 489),\n",
              " ('sequence', 489),\n",
              " ('room', 488),\n",
              " ('due', 488),\n",
              " ('among', 488),\n",
              " ('serious', 488),\n",
              " ('powerful', 488),\n",
              " ('strange', 487),\n",
              " ('order', 487),\n",
              " ('cannot', 487),\n",
              " ('b', 487),\n",
              " ('beauty', 486),\n",
              " ('famous', 485),\n",
              " ('happened', 484),\n",
              " ('tries', 484),\n",
              " ('herself', 484),\n",
              " ('myself', 484),\n",
              " ('class', 483),\n",
              " ('four', 482),\n",
              " ('cool', 481),\n",
              " ('release', 479),\n",
              " ('anyway', 479),\n",
              " ('theme', 479),\n",
              " ('opening', 478),\n",
              " ('entertainment', 477),\n",
              " ('slow', 475),\n",
              " ('ends', 475),\n",
              " ('unique', 475),\n",
              " ('exactly', 475),\n",
              " ('easily', 474),\n",
              " ('level', 474),\n",
              " ('o', 474),\n",
              " ('red', 474),\n",
              " ('interest', 472),\n",
              " ('happen', 471),\n",
              " ('crime', 470),\n",
              " ('viewing', 468),\n",
              " ('sets', 467),\n",
              " ('memorable', 467),\n",
              " ('stop', 466),\n",
              " ('group', 466),\n",
              " ('problems', 463),\n",
              " ('dance', 463),\n",
              " ('working', 463),\n",
              " ('sister', 463),\n",
              " ('message', 463),\n",
              " ('knew', 462),\n",
              " ('mystery', 461),\n",
              " ('nature', 461),\n",
              " ('bring', 460),\n",
              " ('believable', 459),\n",
              " ('thinking', 459),\n",
              " ('brought', 459),\n",
              " ('mostly', 458),\n",
              " ('disney', 457),\n",
              " ('couldn', 457),\n",
              " ('society', 456),\n",
              " ('lady', 455),\n",
              " ('within', 455),\n",
              " ('blood', 454),\n",
              " ('parents', 453),\n",
              " ('upon', 453),\n",
              " ('viewers', 453),\n",
              " ('meets', 452),\n",
              " ('form', 452),\n",
              " ('peter', 452),\n",
              " ('tom', 452),\n",
              " ('usually', 452),\n",
              " ('soundtrack', 452),\n",
              " ('local', 450),\n",
              " ('certain', 448),\n",
              " ('follow', 448),\n",
              " ('whether', 447),\n",
              " ('possible', 446),\n",
              " ('emotional', 445),\n",
              " ('killed', 444),\n",
              " ('above', 444),\n",
              " ('de', 444),\n",
              " ('god', 443),\n",
              " ('middle', 443),\n",
              " ('needs', 442),\n",
              " ('happens', 442),\n",
              " ('flick', 442),\n",
              " ('masterpiece', 441),\n",
              " ('period', 440),\n",
              " ('major', 440),\n",
              " ('named', 439),\n",
              " ('haven', 439),\n",
              " ('particular', 438),\n",
              " ('th', 438),\n",
              " ('earth', 437),\n",
              " ('feature', 437),\n",
              " ('stand', 436),\n",
              " ('words', 435),\n",
              " ('typical', 435),\n",
              " ('elements', 433),\n",
              " ('obviously', 433),\n",
              " ('romance', 431),\n",
              " ('jane', 430),\n",
              " ('yourself', 427),\n",
              " ('showing', 427),\n",
              " ('brings', 426),\n",
              " ('fantasy', 426),\n",
              " ('guess', 423),\n",
              " ('america', 423),\n",
              " ('unfortunately', 422),\n",
              " ('huge', 422),\n",
              " ('indeed', 421),\n",
              " ('running', 421),\n",
              " ('talent', 420),\n",
              " ('stage', 419),\n",
              " ('started', 418),\n",
              " ('leads', 417),\n",
              " ('sweet', 417),\n",
              " ('japanese', 417),\n",
              " ('poor', 416),\n",
              " ('deal', 416),\n",
              " ('incredible', 413),\n",
              " ('personal', 413),\n",
              " ('fast', 412),\n",
              " ('became', 410),\n",
              " ('deep', 410),\n",
              " ('hours', 409),\n",
              " ('giving', 408),\n",
              " ('nearly', 408),\n",
              " ('dream', 408),\n",
              " ('clearly', 407),\n",
              " ('turned', 407),\n",
              " ('obvious', 406),\n",
              " ('near', 406),\n",
              " ('cut', 405),\n",
              " ('surprise', 405),\n",
              " ('era', 404),\n",
              " ('body', 404),\n",
              " ('hour', 403),\n",
              " ('female', 403),\n",
              " ('five', 403),\n",
              " ('note', 399),\n",
              " ('learn', 398),\n",
              " ('truth', 398),\n",
              " ('except', 397),\n",
              " ('feels', 397),\n",
              " ('match', 397),\n",
              " ('tony', 397),\n",
              " ('filmed', 394),\n",
              " ('clear', 394),\n",
              " ('complete', 394),\n",
              " ('street', 393),\n",
              " ('eventually', 393),\n",
              " ('keeps', 393),\n",
              " ('older', 393),\n",
              " ('lots', 393),\n",
              " ('buy', 392),\n",
              " ('william', 391),\n",
              " ('stewart', 391),\n",
              " ('fall', 390),\n",
              " ('joe', 390),\n",
              " ('meet', 390),\n",
              " ('unlike', 389),\n",
              " ('talking', 389),\n",
              " ('shots', 389),\n",
              " ('rating', 389),\n",
              " ('difficult', 389),\n",
              " ('dramatic', 388),\n",
              " ('means', 388),\n",
              " ('situation', 386),\n",
              " ('wonder', 386),\n",
              " ('present', 386),\n",
              " ('appears', 386),\n",
              " ('subject', 386),\n",
              " ('comments', 385),\n",
              " ('general', 383),\n",
              " ('sequences', 383),\n",
              " ('lee', 383),\n",
              " ('points', 382),\n",
              " ('earlier', 382),\n",
              " ('gone', 379),\n",
              " ('check', 379),\n",
              " ('suspense', 378),\n",
              " ('recommended', 378),\n",
              " ('ten', 378),\n",
              " ('third', 377),\n",
              " ('business', 377),\n",
              " ('talk', 375),\n",
              " ('leaves', 375),\n",
              " ('beyond', 375),\n",
              " ('portrayal', 374),\n",
              " ('beautifully', 373),\n",
              " ('single', 372),\n",
              " ('bill', 372),\n",
              " ('plenty', 371),\n",
              " ('word', 371),\n",
              " ('whom', 370),\n",
              " ('falls', 370),\n",
              " ('scary', 369),\n",
              " ('non', 369),\n",
              " ('figure', 369),\n",
              " ('battle', 369),\n",
              " ('using', 368),\n",
              " ('return', 368),\n",
              " ('doubt', 367),\n",
              " ('add', 367),\n",
              " ('hear', 366),\n",
              " ('solid', 366),\n",
              " ('success', 366),\n",
              " ('jokes', 365),\n",
              " ('oh', 365),\n",
              " ('touching', 365),\n",
              " ('political', 365),\n",
              " ('hell', 364),\n",
              " ('awesome', 364),\n",
              " ('boys', 364),\n",
              " ('sexual', 362),\n",
              " ('recently', 362),\n",
              " ('dog', 362),\n",
              " ('please', 361),\n",
              " ('wouldn', 361),\n",
              " ('straight', 361),\n",
              " ('features', 361),\n",
              " ('forget', 360),\n",
              " ('setting', 360),\n",
              " ('lack', 360),\n",
              " ('married', 359),\n",
              " ('mark', 359),\n",
              " ('social', 357),\n",
              " ('interested', 356),\n",
              " ('adventure', 356),\n",
              " ('actual', 355),\n",
              " ('terrific', 355),\n",
              " ('sees', 355),\n",
              " ('brothers', 355),\n",
              " ('move', 354),\n",
              " ('call', 354),\n",
              " ('various', 353),\n",
              " ('theater', 353),\n",
              " ('dr', 353),\n",
              " ('animated', 352),\n",
              " ('western', 351),\n",
              " ('baby', 350),\n",
              " ('space', 350),\n",
              " ('leading', 348),\n",
              " ('disappointed', 348),\n",
              " ('portrayed', 346),\n",
              " ('aren', 346),\n",
              " ('screenplay', 345),\n",
              " ('smith', 345),\n",
              " ('towards', 344),\n",
              " ('hate', 344),\n",
              " ('noir', 343),\n",
              " ('outstanding', 342),\n",
              " ('decent', 342),\n",
              " ('kelly', 342),\n",
              " ('directors', 341),\n",
              " ('journey', 341),\n",
              " ('none', 340),\n",
              " ('looked', 340),\n",
              " ('effective', 340),\n",
              " ('storyline', 339),\n",
              " ('caught', 339),\n",
              " ('sci', 339),\n",
              " ('fi', 339),\n",
              " ('cold', 339),\n",
              " ('mary', 339),\n",
              " ('rich', 338),\n",
              " ('charming', 338),\n",
              " ('popular', 337),\n",
              " ('rare', 337),\n",
              " ('manages', 337),\n",
              " ('harry', 337),\n",
              " ('spirit', 336),\n",
              " ('appreciate', 335),\n",
              " ('open', 335),\n",
              " ('moves', 334),\n",
              " ('basically', 334),\n",
              " ('acted', 334),\n",
              " ('inside', 333),\n",
              " ('boring', 333),\n",
              " ('century', 333),\n",
              " ('mention', 333),\n",
              " ('deserves', 333),\n",
              " ('subtle', 333),\n",
              " ('pace', 333),\n",
              " ('familiar', 332),\n",
              " ('background', 332),\n",
              " ('ben', 331),\n",
              " ('creepy', 330),\n",
              " ('supposed', 330),\n",
              " ('secret', 329),\n",
              " ('die', 328),\n",
              " ('jim', 328),\n",
              " ('question', 327),\n",
              " ('effect', 327),\n",
              " ('natural', 327),\n",
              " ('impressive', 326),\n",
              " ('rate', 326),\n",
              " ('language', 326),\n",
              " ('saying', 325),\n",
              " ('intelligent', 325),\n",
              " ('telling', 324),\n",
              " ('realize', 324),\n",
              " ('material', 324),\n",
              " ('scott', 324),\n",
              " ('singing', 323),\n",
              " ('dancing', 322),\n",
              " ('visual', 321),\n",
              " ('adult', 321),\n",
              " ('imagine', 321),\n",
              " ('kept', 320),\n",
              " ('office', 320),\n",
              " ('uses', 319),\n",
              " ('pure', 318),\n",
              " ('wait', 318),\n",
              " ('stunning', 318),\n",
              " ('review', 317),\n",
              " ('previous', 317),\n",
              " ('copy', 317),\n",
              " ('seriously', 317),\n",
              " ('reading', 316),\n",
              " ('create', 316),\n",
              " ('hot', 316),\n",
              " ('created', 316),\n",
              " ('magic', 316),\n",
              " ('somehow', 316),\n",
              " ('stay', 315),\n",
              " ('attempt', 315),\n",
              " ('escape', 315),\n",
              " ('crazy', 315),\n",
              " ('air', 315),\n",
              " ('frank', 315),\n",
              " ('hands', 314),\n",
              " ('filled', 313),\n",
              " ('expected', 312),\n",
              " ('average', 312),\n",
              " ('surprisingly', 312),\n",
              " ('complex', 311),\n",
              " ('quickly', 310),\n",
              " ('successful', 310),\n",
              " ('studio', 310),\n",
              " ('plus', 309),\n",
              " ('male', 309),\n",
              " ('co', 307),\n",
              " ('images', 306),\n",
              " ('casting', 306),\n",
              " ('following', 306),\n",
              " ('minute', 306),\n",
              " ('exciting', 306),\n",
              " ('members', 305),\n",
              " ('follows', 305),\n",
              " ('themes', 305),\n",
              " ('german', 305),\n",
              " ('reasons', 305),\n",
              " ('e', 305),\n",
              " ('touch', 304),\n",
              " ('edge', 304),\n",
              " ('free', 304),\n",
              " ('cute', 304),\n",
              " ('genius', 304),\n",
              " ('outside', 303),\n",
              " ('reviews', 302),\n",
              " ('admit', 302),\n",
              " ('ok', 302),\n",
              " ('younger', 302),\n",
              " ('fighting', 301),\n",
              " ('odd', 301),\n",
              " ('master', 301),\n",
              " ('recent', 300),\n",
              " ('thanks', 300),\n",
              " ('break', 300),\n",
              " ('comment', 300),\n",
              " ('apart', 299),\n",
              " ('emotions', 298),\n",
              " ('lovely', 298),\n",
              " ('begin', 298),\n",
              " ('doctor', 297),\n",
              " ('party', 297),\n",
              " ('italian', 297),\n",
              " ('la', 296),\n",
              " ('missed', 296),\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8z8ZaItpU6z3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fe5edc26-abf4-4047-9bc4-d8a0b9443e81"
      },
      "source": [
        "negative_counts.most_common()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('', 561462),\n",
              " ('.', 167538),\n",
              " ('the', 163389),\n",
              " ('a', 79321),\n",
              " ('and', 74385),\n",
              " ('of', 69009),\n",
              " ('to', 68974),\n",
              " ('br', 52637),\n",
              " ('is', 50083),\n",
              " ('it', 48327),\n",
              " ('i', 46880),\n",
              " ('in', 43753),\n",
              " ('this', 40920),\n",
              " ('that', 37615),\n",
              " ('s', 31546),\n",
              " ('was', 26291),\n",
              " ('movie', 24965),\n",
              " ('for', 21927),\n",
              " ('but', 21781),\n",
              " ('with', 20878),\n",
              " ('as', 20625),\n",
              " ('t', 20361),\n",
              " ('film', 19218),\n",
              " ('you', 17549),\n",
              " ('on', 17192),\n",
              " ('not', 16354),\n",
              " ('have', 15144),\n",
              " ('are', 14623),\n",
              " ('be', 14541),\n",
              " ('he', 13856),\n",
              " ('one', 13134),\n",
              " ('they', 13011),\n",
              " ('at', 12279),\n",
              " ('his', 12147),\n",
              " ('all', 12036),\n",
              " ('so', 11463),\n",
              " ('like', 11238),\n",
              " ('there', 10775),\n",
              " ('just', 10619),\n",
              " ('by', 10549),\n",
              " ('or', 10272),\n",
              " ('an', 10266),\n",
              " ('who', 9969),\n",
              " ('from', 9731),\n",
              " ('if', 9518),\n",
              " ('about', 9061),\n",
              " ('out', 8979),\n",
              " ('what', 8422),\n",
              " ('some', 8306),\n",
              " ('no', 8143),\n",
              " ('her', 7947),\n",
              " ('even', 7687),\n",
              " ('can', 7653),\n",
              " ('has', 7604),\n",
              " ('good', 7423),\n",
              " ('bad', 7401),\n",
              " ('would', 7036),\n",
              " ('up', 6970),\n",
              " ('only', 6781),\n",
              " ('more', 6730),\n",
              " ('when', 6726),\n",
              " ('she', 6444),\n",
              " ('really', 6262),\n",
              " ('time', 6209),\n",
              " ('had', 6142),\n",
              " ('my', 6015),\n",
              " ('were', 6001),\n",
              " ('which', 5780),\n",
              " ('very', 5764),\n",
              " ('me', 5606),\n",
              " ('see', 5452),\n",
              " ('don', 5336),\n",
              " ('we', 5328),\n",
              " ('their', 5278),\n",
              " ('do', 5236),\n",
              " ('story', 5208),\n",
              " ('than', 5183),\n",
              " ('been', 5100),\n",
              " ('much', 5078),\n",
              " ('get', 5037),\n",
              " ('because', 4966),\n",
              " ('people', 4806),\n",
              " ('then', 4761),\n",
              " ('make', 4722),\n",
              " ('how', 4688),\n",
              " ('could', 4686),\n",
              " ('any', 4658),\n",
              " ('into', 4567),\n",
              " ('made', 4541),\n",
              " ('first', 4306),\n",
              " ('other', 4305),\n",
              " ('well', 4254),\n",
              " ('too', 4174),\n",
              " ('them', 4165),\n",
              " ('plot', 4154),\n",
              " ('movies', 4080),\n",
              " ('acting', 4056),\n",
              " ('will', 3993),\n",
              " ('way', 3989),\n",
              " ('most', 3919),\n",
              " ('him', 3858),\n",
              " ('after', 3838),\n",
              " ('its', 3655),\n",
              " ('think', 3643),\n",
              " ('also', 3608),\n",
              " ('characters', 3600),\n",
              " ('off', 3567),\n",
              " ('watch', 3550),\n",
              " ('character', 3506),\n",
              " ('did', 3506),\n",
              " ('why', 3463),\n",
              " ('being', 3393),\n",
              " ('better', 3358),\n",
              " ('know', 3334),\n",
              " ('over', 3316),\n",
              " ('seen', 3265),\n",
              " ('ever', 3263),\n",
              " ('never', 3259),\n",
              " ('your', 3233),\n",
              " ('where', 3219),\n",
              " ('two', 3173),\n",
              " ('little', 3096),\n",
              " ('films', 3077),\n",
              " ('here', 3027),\n",
              " ('m', 3000),\n",
              " ('nothing', 2990),\n",
              " ('say', 2982),\n",
              " ('end', 2954),\n",
              " ('something', 2942),\n",
              " ('should', 2920),\n",
              " ('many', 2909),\n",
              " ('does', 2871),\n",
              " ('thing', 2866),\n",
              " ('show', 2862),\n",
              " ('ve', 2829),\n",
              " ('scene', 2816),\n",
              " ('scenes', 2785),\n",
              " ('these', 2724),\n",
              " ('go', 2717),\n",
              " ('didn', 2646),\n",
              " ('great', 2640),\n",
              " ('watching', 2640),\n",
              " ('re', 2620),\n",
              " ('doesn', 2601),\n",
              " ('through', 2560),\n",
              " ('such', 2544),\n",
              " ('man', 2516),\n",
              " ('worst', 2480),\n",
              " ('actually', 2449),\n",
              " ('actors', 2437),\n",
              " ('life', 2429),\n",
              " ('back', 2424),\n",
              " ('while', 2418),\n",
              " ('director', 2405),\n",
              " ('funny', 2336),\n",
              " ('going', 2319),\n",
              " ('still', 2283),\n",
              " ('another', 2254),\n",
              " ('look', 2247),\n",
              " ('now', 2237),\n",
              " ('old', 2215),\n",
              " ('those', 2212),\n",
              " ('real', 2170),\n",
              " ('few', 2158),\n",
              " ('love', 2152),\n",
              " ('horror', 2150),\n",
              " ('before', 2147),\n",
              " ('want', 2141),\n",
              " ('minutes', 2126),\n",
              " ('pretty', 2115),\n",
              " ('best', 2094),\n",
              " ('though', 2091),\n",
              " ('same', 2081),\n",
              " ('script', 2074),\n",
              " ('work', 2027),\n",
              " ('every', 2025),\n",
              " ('seems', 2023),\n",
              " ('least', 2011),\n",
              " ('enough', 1997),\n",
              " ('down', 1988),\n",
              " ('original', 1983),\n",
              " ('guy', 1964),\n",
              " ('got', 1952),\n",
              " ('around', 1943),\n",
              " ('part', 1942),\n",
              " ('lot', 1892),\n",
              " ('anything', 1874),\n",
              " ('find', 1860),\n",
              " ('new', 1854),\n",
              " ('again', 1849),\n",
              " ('isn', 1849),\n",
              " ('point', 1845),\n",
              " ('things', 1839),\n",
              " ('fact', 1839),\n",
              " ('give', 1823),\n",
              " ('makes', 1814),\n",
              " ('take', 1800),\n",
              " ('thought', 1798),\n",
              " ('d', 1770),\n",
              " ('whole', 1768),\n",
              " ('long', 1761),\n",
              " ('years', 1759),\n",
              " ('however', 1740),\n",
              " ('gets', 1714),\n",
              " ('making', 1695),\n",
              " ('cast', 1694),\n",
              " ('big', 1662),\n",
              " ('might', 1658),\n",
              " ('interesting', 1648),\n",
              " ('money', 1638),\n",
              " ('us', 1628),\n",
              " ('right', 1625),\n",
              " ('far', 1619),\n",
              " ('quite', 1596),\n",
              " ('without', 1595),\n",
              " ('come', 1595),\n",
              " ('almost', 1574),\n",
              " ('ll', 1567),\n",
              " ('action', 1566),\n",
              " ('awful', 1557),\n",
              " ('kind', 1539),\n",
              " ('reason', 1534),\n",
              " ('am', 1530),\n",
              " ('looks', 1528),\n",
              " ('must', 1522),\n",
              " ('done', 1510),\n",
              " ('comedy', 1504),\n",
              " ('someone', 1490),\n",
              " ('trying', 1486),\n",
              " ('wasn', 1484),\n",
              " ('poor', 1481),\n",
              " ('boring', 1478),\n",
              " ('instead', 1478),\n",
              " ('saw', 1475),\n",
              " ('away', 1469),\n",
              " ('girl', 1463),\n",
              " ('probably', 1444),\n",
              " ('believe', 1434),\n",
              " ('sure', 1433),\n",
              " ('looking', 1430),\n",
              " ('stupid', 1428),\n",
              " ('anyone', 1418),\n",
              " ('times', 1406),\n",
              " ('maybe', 1404),\n",
              " ('world', 1404),\n",
              " ('rather', 1394),\n",
              " ('terrible', 1391),\n",
              " ('may', 1390),\n",
              " ('last', 1390),\n",
              " ('since', 1388),\n",
              " ('let', 1385),\n",
              " ('tv', 1382),\n",
              " ('hard', 1374),\n",
              " ('between', 1374),\n",
              " ('waste', 1358),\n",
              " ('woman', 1356),\n",
              " ('feel', 1354),\n",
              " ('effects', 1348),\n",
              " ('half', 1341),\n",
              " ('own', 1333),\n",
              " ('young', 1317),\n",
              " ('music', 1316),\n",
              " ('idea', 1312),\n",
              " ('sense', 1306),\n",
              " ('bit', 1298),\n",
              " ('having', 1280),\n",
              " ('book', 1278),\n",
              " ('found', 1267),\n",
              " ('put', 1263),\n",
              " ('series', 1263),\n",
              " ('goes', 1256),\n",
              " ('worse', 1249),\n",
              " ('said', 1230),\n",
              " ('comes', 1224),\n",
              " ('role', 1222),\n",
              " ('main', 1220),\n",
              " ('else', 1199),\n",
              " ('everything', 1197),\n",
              " ('yet', 1196),\n",
              " ('low', 1189),\n",
              " ('screen', 1188),\n",
              " ('supposed', 1186),\n",
              " ('actor', 1185),\n",
              " ('either', 1183),\n",
              " ('budget', 1179),\n",
              " ('ending', 1179),\n",
              " ('audience', 1178),\n",
              " ('set', 1177),\n",
              " ('family', 1170),\n",
              " ('left', 1169),\n",
              " ('completely', 1168),\n",
              " ('both', 1158),\n",
              " ('wrong', 1155),\n",
              " ('always', 1151),\n",
              " ('course', 1148),\n",
              " ('place', 1148),\n",
              " ('seem', 1147),\n",
              " ('watched', 1142),\n",
              " ('day', 1132),\n",
              " ('simply', 1130),\n",
              " ('shot', 1126),\n",
              " ('mean', 1117),\n",
              " ('special', 1102),\n",
              " ('dead', 1101),\n",
              " ('three', 1094),\n",
              " ('house', 1085),\n",
              " ('oh', 1084),\n",
              " ('night', 1083),\n",
              " ('read', 1082),\n",
              " ('less', 1067),\n",
              " ('high', 1066),\n",
              " ('year', 1064),\n",
              " ('camera', 1061),\n",
              " ('worth', 1057),\n",
              " ('our', 1056),\n",
              " ('try', 1051),\n",
              " ('horrible', 1046),\n",
              " ('sex', 1046),\n",
              " ('video', 1043),\n",
              " ('black', 1039),\n",
              " ('although', 1036),\n",
              " ('couldn', 1036),\n",
              " ('once', 1033),\n",
              " ('rest', 1022),\n",
              " ('dvd', 1021),\n",
              " ('line', 1018),\n",
              " ('played', 1017),\n",
              " ('fun', 1007),\n",
              " ('during', 1006),\n",
              " ('production', 1003),\n",
              " ('everyone', 1002),\n",
              " ('play', 993),\n",
              " ('mind', 990),\n",
              " ('version', 989),\n",
              " ('kids', 989),\n",
              " ('seeing', 988),\n",
              " ('american', 980),\n",
              " ('given', 978),\n",
              " ('used', 969),\n",
              " ('performance', 968),\n",
              " ('especially', 963),\n",
              " ('together', 963),\n",
              " ('tell', 959),\n",
              " ('women', 958),\n",
              " ('start', 956),\n",
              " ('need', 955),\n",
              " ('second', 953),\n",
              " ('takes', 950),\n",
              " ('each', 950),\n",
              " ('wife', 944),\n",
              " ('dialogue', 942),\n",
              " ('use', 940),\n",
              " ('problem', 938),\n",
              " ('star', 934),\n",
              " ('unfortunately', 931),\n",
              " ('himself', 929),\n",
              " ('doing', 926),\n",
              " ('death', 922),\n",
              " ('name', 921),\n",
              " ('lines', 919),\n",
              " ('killer', 914),\n",
              " ('getting', 913),\n",
              " ('help', 905),\n",
              " ('couple', 902),\n",
              " ('fan', 902),\n",
              " ('head', 898),\n",
              " ('crap', 895),\n",
              " ('guess', 888),\n",
              " ('piece', 884),\n",
              " ('nice', 880),\n",
              " ('different', 878),\n",
              " ('school', 876),\n",
              " ('later', 875),\n",
              " ('entire', 869),\n",
              " ('shows', 860),\n",
              " ('next', 858),\n",
              " ('john', 858),\n",
              " ('short', 857),\n",
              " ('seemed', 857),\n",
              " ('hollywood', 850),\n",
              " ('home', 848),\n",
              " ('true', 846),\n",
              " ('person', 846),\n",
              " ('absolutely', 842),\n",
              " ('sort', 840),\n",
              " ('care', 839),\n",
              " ('understand', 836),\n",
              " ('plays', 835),\n",
              " ('felt', 834),\n",
              " ('written', 829),\n",
              " ('title', 828),\n",
              " ('men', 822),\n",
              " ('until', 821),\n",
              " ('flick', 816),\n",
              " ('decent', 815),\n",
              " ('face', 814),\n",
              " ('friends', 810),\n",
              " ('stars', 807),\n",
              " ('job', 807),\n",
              " ('case', 807),\n",
              " ('itself', 804),\n",
              " ('yes', 801),\n",
              " ('perhaps', 800),\n",
              " ('went', 797),\n",
              " ('wanted', 797),\n",
              " ('called', 796),\n",
              " ('annoying', 795),\n",
              " ('ridiculous', 790),\n",
              " ('tries', 790),\n",
              " ('laugh', 788),\n",
              " ('evil', 787),\n",
              " ('along', 786),\n",
              " ('top', 785),\n",
              " ('hour', 784),\n",
              " ('full', 783),\n",
              " ('came', 780),\n",
              " ('writing', 780),\n",
              " ('keep', 770),\n",
              " ('totally', 767),\n",
              " ('playing', 766),\n",
              " ('god', 765),\n",
              " ('won', 764),\n",
              " ('guys', 763),\n",
              " ('already', 762),\n",
              " ('gore', 757),\n",
              " ('direction', 748),\n",
              " ('save', 746),\n",
              " ('lost', 745),\n",
              " ('example', 744),\n",
              " ('sound', 742),\n",
              " ('war', 741),\n",
              " ('attempt', 735),\n",
              " ('car', 733),\n",
              " ('except', 733),\n",
              " ('moments', 732),\n",
              " ('blood', 732),\n",
              " ('obviously', 730),\n",
              " ('act', 729),\n",
              " ('remember', 728),\n",
              " ('kill', 727),\n",
              " ('truly', 726),\n",
              " ('white', 726),\n",
              " ('father', 726),\n",
              " ('b', 725),\n",
              " ('thinking', 720),\n",
              " ('ok', 716),\n",
              " ('finally', 716),\n",
              " ('turn', 711),\n",
              " ('quality', 701),\n",
              " ('lack', 698),\n",
              " ('style', 694),\n",
              " ('wouldn', 693),\n",
              " ('cheap', 691),\n",
              " ('none', 690),\n",
              " ('kid', 686),\n",
              " ('please', 686),\n",
              " ('boy', 685),\n",
              " ('seriously', 684),\n",
              " ('lead', 680),\n",
              " ('dull', 677),\n",
              " ('children', 676),\n",
              " ('starts', 675),\n",
              " ('stuff', 673),\n",
              " ('hope', 672),\n",
              " ('looked', 670),\n",
              " ('recommend', 669),\n",
              " ('under', 668),\n",
              " ('run', 667),\n",
              " ('killed', 667),\n",
              " ('enjoy', 666),\n",
              " ('others', 666),\n",
              " ('etc', 663),\n",
              " ('myself', 663),\n",
              " ('beginning', 662),\n",
              " ('girls', 662),\n",
              " ('against', 662),\n",
              " ('obvious', 660),\n",
              " ('small', 660),\n",
              " ('hell', 659),\n",
              " ('slow', 657),\n",
              " ('hand', 656),\n",
              " ('wonder', 652),\n",
              " ('lame', 652),\n",
              " ('becomes', 651),\n",
              " ('picture', 651),\n",
              " ('based', 650),\n",
              " ('early', 648),\n",
              " ('behind', 646),\n",
              " ('poorly', 644),\n",
              " ('avoid', 642),\n",
              " ('apparently', 640),\n",
              " ('complete', 640),\n",
              " ('happens', 639),\n",
              " ('anyway', 638),\n",
              " ('classic', 637),\n",
              " ('several', 636),\n",
              " ('despite', 635),\n",
              " ('certainly', 635),\n",
              " ('episode', 635),\n",
              " ('often', 631),\n",
              " ('cut', 630),\n",
              " ('writer', 630),\n",
              " ('mother', 628),\n",
              " ('predictable', 628),\n",
              " ('gave', 628),\n",
              " ('become', 627),\n",
              " ('close', 625),\n",
              " ('fans', 624),\n",
              " ('saying', 621),\n",
              " ('scary', 619),\n",
              " ('stop', 618),\n",
              " ('live', 618),\n",
              " ('wants', 617),\n",
              " ('self', 615),\n",
              " ('mr', 612),\n",
              " ('jokes', 611),\n",
              " ('friend', 611),\n",
              " ('cannot', 610),\n",
              " ('overall', 609),\n",
              " ('cinema', 604),\n",
              " ('child', 603),\n",
              " ('silly', 601),\n",
              " ('beautiful', 596),\n",
              " ('human', 595),\n",
              " ('expect', 594),\n",
              " ('liked', 593),\n",
              " ('happened', 592),\n",
              " ('bunch', 590),\n",
              " ('entertaining', 590),\n",
              " ('actress', 588),\n",
              " ('final', 588),\n",
              " ('says', 584),\n",
              " ('performances', 584),\n",
              " ('turns', 577),\n",
              " ('humor', 577),\n",
              " ('themselves', 576),\n",
              " ('eyes', 576),\n",
              " ('hours', 574),\n",
              " ('happen', 573),\n",
              " ('basically', 572),\n",
              " ('days', 572),\n",
              " ('running', 571),\n",
              " ('involved', 569),\n",
              " ('disappointed', 569),\n",
              " ('call', 569),\n",
              " ('directed', 568),\n",
              " ('group', 568),\n",
              " ('fight', 567),\n",
              " ('daughter', 566),\n",
              " ('talking', 566),\n",
              " ('body', 566),\n",
              " ('badly', 565),\n",
              " ('sorry', 565),\n",
              " ('throughout', 563),\n",
              " ('viewer', 563),\n",
              " ('yourself', 562),\n",
              " ('extremely', 562),\n",
              " ('interest', 561),\n",
              " ('heard', 561),\n",
              " ('violence', 561),\n",
              " ('shots', 559),\n",
              " ('side', 557),\n",
              " ('word', 556),\n",
              " ('art', 555),\n",
              " ('possible', 554),\n",
              " ('dark', 551),\n",
              " ('game', 551),\n",
              " ('hero', 550),\n",
              " ('alone', 549),\n",
              " ('son', 547),\n",
              " ('type', 547),\n",
              " ('leave', 547),\n",
              " ('gives', 546),\n",
              " ('parts', 546),\n",
              " ('single', 546),\n",
              " ('started', 545),\n",
              " ('female', 543),\n",
              " ('rating', 541),\n",
              " ('mess', 541),\n",
              " ('voice', 541),\n",
              " ('aren', 540),\n",
              " ('town', 540),\n",
              " ('drama', 538),\n",
              " ('definitely', 537),\n",
              " ('unless', 536),\n",
              " ('review', 534),\n",
              " ('effort', 533),\n",
              " ('weak', 533),\n",
              " ('able', 533),\n",
              " ('took', 531),\n",
              " ('non', 530),\n",
              " ('five', 530),\n",
              " ('matter', 529),\n",
              " ('usually', 529),\n",
              " ('michael', 528),\n",
              " ('feeling', 526),\n",
              " ('huge', 523),\n",
              " ('sequel', 522),\n",
              " ('soon', 521),\n",
              " ('exactly', 520),\n",
              " ('past', 519),\n",
              " ('turned', 518),\n",
              " ('police', 518),\n",
              " ('tried', 515),\n",
              " ('middle', 513),\n",
              " ('talent', 513),\n",
              " ('genre', 512),\n",
              " ('zombie', 510),\n",
              " ('ends', 509),\n",
              " ('history', 509),\n",
              " ('straight', 503),\n",
              " ('opening', 501),\n",
              " ('serious', 501),\n",
              " ('coming', 501),\n",
              " ('moment', 500),\n",
              " ('lives', 499),\n",
              " ('sad', 499),\n",
              " ('dialog', 498),\n",
              " ('particularly', 498),\n",
              " ('editing', 493),\n",
              " ('clearly', 492),\n",
              " ('beyond', 491),\n",
              " ('earth', 491),\n",
              " ('taken', 490),\n",
              " ('cool', 490),\n",
              " ('level', 489),\n",
              " ('dumb', 489),\n",
              " ('okay', 488),\n",
              " ('major', 487),\n",
              " ('fast', 485),\n",
              " ('premise', 485),\n",
              " ('joke', 484),\n",
              " ('stories', 484),\n",
              " ('wasted', 483),\n",
              " ('minute', 483),\n",
              " ('across', 482),\n",
              " ('mostly', 482),\n",
              " ('rent', 482),\n",
              " ('late', 481),\n",
              " ('falls', 481),\n",
              " ('fails', 481),\n",
              " ('mention', 478),\n",
              " ('theater', 475),\n",
              " ('stay', 472),\n",
              " ('sometimes', 472),\n",
              " ('hit', 468),\n",
              " ('talk', 467),\n",
              " ('fine', 467),\n",
              " ('die', 466),\n",
              " ('storyline', 465),\n",
              " ('pointless', 465),\n",
              " ('taking', 464),\n",
              " ('order', 462),\n",
              " ('brother', 461),\n",
              " ('whatever', 460),\n",
              " ('told', 460),\n",
              " ('wish', 458),\n",
              " ('room', 456),\n",
              " ('career', 455),\n",
              " ('appears', 455),\n",
              " ('write', 455),\n",
              " ('known', 454),\n",
              " ('husband', 454),\n",
              " ('living', 451),\n",
              " ('sit', 450),\n",
              " ('ten', 450),\n",
              " ('words', 449),\n",
              " ('monster', 448),\n",
              " ('chance', 448),\n",
              " ('hate', 444),\n",
              " ('novel', 444),\n",
              " ('add', 443),\n",
              " ('english', 443),\n",
              " ('somehow', 441),\n",
              " ('strange', 440),\n",
              " ('imdb', 438),\n",
              " ('actual', 438),\n",
              " ('total', 437),\n",
              " ('material', 437),\n",
              " ('killing', 437),\n",
              " ('ones', 437),\n",
              " ('knew', 436),\n",
              " ('king', 434),\n",
              " ('number', 434),\n",
              " ('using', 433),\n",
              " ('lee', 431),\n",
              " ('power', 431),\n",
              " ('shown', 431),\n",
              " ('works', 431),\n",
              " ('giving', 431),\n",
              " ('points', 430),\n",
              " ('possibly', 430),\n",
              " ('kept', 430),\n",
              " ('four', 429),\n",
              " ('local', 427),\n",
              " ('usual', 426),\n",
              " ('including', 425),\n",
              " ('problems', 424),\n",
              " ('ago', 424),\n",
              " ('opinion', 424),\n",
              " ('nudity', 423),\n",
              " ('age', 422),\n",
              " ('due', 421),\n",
              " ('roles', 420),\n",
              " ('writers', 419),\n",
              " ('decided', 419),\n",
              " ('near', 418),\n",
              " ('flat', 418),\n",
              " ('easily', 418),\n",
              " ('murder', 417),\n",
              " ('experience', 417),\n",
              " ('reviews', 416),\n",
              " ('imagine', 415),\n",
              " ('feels', 413),\n",
              " ('plain', 411),\n",
              " ('somewhat', 411),\n",
              " ('class', 410),\n",
              " ('score', 410),\n",
              " ('song', 409),\n",
              " ('bring', 409),\n",
              " ('whether', 409),\n",
              " ('otherwise', 408),\n",
              " ('whose', 408),\n",
              " ('average', 408),\n",
              " ('pathetic', 407),\n",
              " ('nearly', 407),\n",
              " ('knows', 407),\n",
              " ('zombies', 407),\n",
              " ('cinematography', 406),\n",
              " ('cheesy', 406),\n",
              " ('upon', 406),\n",
              " ('city', 405),\n",
              " ('space', 405),\n",
              " ('credits', 404),\n",
              " ('james', 403),\n",
              " ('lots', 403),\n",
              " ('change', 403),\n",
              " ('entertainment', 402),\n",
              " ('nor', 402),\n",
              " ('wait', 401),\n",
              " ('released', 400),\n",
              " ('needs', 399),\n",
              " ('shame', 398),\n",
              " ('attention', 396),\n",
              " ('comments', 394),\n",
              " ('bored', 393),\n",
              " ('free', 393),\n",
              " ('lady', 393),\n",
              " ('expected', 392),\n",
              " ('needed', 392),\n",
              " ('clear', 392),\n",
              " ('view', 391),\n",
              " ('development', 390),\n",
              " ('check', 390),\n",
              " ('doubt', 390),\n",
              " ('figure', 389),\n",
              " ('mystery', 389),\n",
              " ('excellent', 388),\n",
              " ('garbage', 388),\n",
              " ('sequence', 386),\n",
              " ('television', 386),\n",
              " ('o', 385),\n",
              " ('sets', 385),\n",
              " ('laughable', 384),\n",
              " ('potential', 384),\n",
              " ('robert', 382),\n",
              " ('light', 382),\n",
              " ('country', 382),\n",
              " ('documentary', 382),\n",
              " ('reality', 382),\n",
              " ('general', 381),\n",
              " ('ask', 381),\n",
              " ('comic', 380),\n",
              " ('fall', 380),\n",
              " ('begin', 380),\n",
              " ('footage', 379),\n",
              " ('stand', 379),\n",
              " ('forced', 379),\n",
              " ('trash', 379),\n",
              " ('remake', 379),\n",
              " ('thriller', 378),\n",
              " ('songs', 378),\n",
              " ('gay', 377),\n",
              " ('within', 377),\n",
              " ('hardly', 376),\n",
              " ('above', 375),\n",
              " ('gone', 375),\n",
              " ('george', 374),\n",
              " ('means', 373),\n",
              " ('sounds', 373),\n",
              " ('directing', 372),\n",
              " ('move', 372),\n",
              " ('david', 372),\n",
              " ('buy', 372),\n",
              " ('rock', 371),\n",
              " ('forward', 371),\n",
              " ('important', 371),\n",
              " ('hot', 370),\n",
              " ('haven', 370),\n",
              " ('filmed', 370),\n",
              " ('british', 370),\n",
              " ('heart', 369),\n",
              " ('reading', 369),\n",
              " ('fake', 369),\n",
              " ('incredibly', 368),\n",
              " ('weird', 368),\n",
              " ('hear', 368),\n",
              " ('enjoyed', 367),\n",
              " ('hilarious', 367),\n",
              " ('cop', 367),\n",
              " ('musical', 367),\n",
              " ('message', 366),\n",
              " ('happy', 366),\n",
              " ('pay', 366),\n",
              " ('laughs', 365),\n",
              " ('box', 365),\n",
              " ('suspense', 363),\n",
              " ('sadly', 363),\n",
              " ('eye', 362),\n",
              " ('third', 361),\n",
              " ('similar', 361),\n",
              " ('named', 361),\n",
              " ('modern', 360),\n",
              " ('failed', 359),\n",
              " ('events', 359),\n",
              " ('forget', 358),\n",
              " ('question', 358),\n",
              " ('male', 357),\n",
              " ('finds', 357),\n",
              " ('perfect', 356),\n",
              " ('spent', 355),\n",
              " ('sister', 355),\n",
              " ('feature', 354),\n",
              " ('result', 354),\n",
              " ('comment', 353),\n",
              " ('girlfriend', 353),\n",
              " ('sexual', 352),\n",
              " ('attempts', 351),\n",
              " ('neither', 351),\n",
              " ('richard', 351),\n",
              " ('screenplay', 350),\n",
              " ('elements', 350),\n",
              " ('spoilers', 349),\n",
              " ('brain', 348),\n",
              " ('filmmakers', 348),\n",
              " ('showing', 348),\n",
              " ('miss', 347),\n",
              " ('dr', 347),\n",
              " ('christmas', 347),\n",
              " ('cover', 345),\n",
              " ('red', 344),\n",
              " ('sequences', 344),\n",
              " ('typical', 343),\n",
              " ('excuse', 343),\n",
              " ('crazy', 342),\n",
              " ('ideas', 342),\n",
              " ('baby', 342),\n",
              " ('loved', 341),\n",
              " ('meant', 341),\n",
              " ('worked', 340),\n",
              " ('fire', 340),\n",
              " ('unbelievable', 339),\n",
              " ('follow', 339),\n",
              " ('theme', 337),\n",
              " ('barely', 336),\n",
              " ('producers', 336),\n",
              " ('twist', 336),\n",
              " ('plus', 336),\n",
              " ('appear', 336),\n",
              " ('directors', 335),\n",
              " ('team', 335),\n",
              " ('viewers', 333),\n",
              " ('leads', 332),\n",
              " ('tom', 332),\n",
              " ('slasher', 332),\n",
              " ('wrote', 331),\n",
              " ('villain', 331),\n",
              " ('gun', 331),\n",
              " ('working', 331),\n",
              " ('island', 330),\n",
              " ('strong', 330),\n",
              " ('open', 330),\n",
              " ('realize', 330),\n",
              " ('positive', 329),\n",
              " ('disappointing', 329),\n",
              " ('yeah', 329),\n",
              " ('quickly', 329),\n",
              " ('weren', 328),\n",
              " ('release', 328),\n",
              " ('simple', 328),\n",
              " ('honestly', 328),\n",
              " ('eventually', 327),\n",
              " ('period', 327),\n",
              " ('tells', 327),\n",
              " ('kills', 327),\n",
              " ('doctor', 327),\n",
              " ('nowhere', 326),\n",
              " ('list', 326),\n",
              " ('acted', 326),\n",
              " ('herself', 326),\n",
              " ('dog', 326),\n",
              " ('walk', 325),\n",
              " ('air', 324),\n",
              " ('apart', 324),\n",
              " ('makers', 323),\n",
              " ('subject', 323),\n",
              " ('learn', 322),\n",
              " ('fi', 322),\n",
              " ('sci', 319),\n",
              " ('bother', 319),\n",
              " ('admit', 319),\n",
              " ('jack', 318),\n",
              " ('disappointment', 318),\n",
              " ('hands', 318),\n",
              " ('note', 318),\n",
              " ('certain', 317),\n",
              " ('e', 317),\n",
              " ('value', 317),\n",
              " ('casting', 317),\n",
              " ('grade', 316),\n",
              " ('peter', 316),\n",
              " ('suddenly', 315),\n",
              " ('missing', 315),\n",
              " ('form', 313),\n",
              " ('stick', 313),\n",
              " ('previous', 313),\n",
              " ('break', 313),\n",
              " ('soundtrack', 312),\n",
              " ('surprised', 311),\n",
              " ('front', 311),\n",
              " ('expecting', 311),\n",
              " ('parents', 310),\n",
              " ('surprise', 310),\n",
              " ('relationship', 310),\n",
              " ('shoot', 309),\n",
              " ('today', 309),\n",
              " ('painful', 308),\n",
              " ('ways', 308),\n",
              " ('leaves', 308),\n",
              " ('ended', 308),\n",
              " ('creepy', 308),\n",
              " ('concept', 308),\n",
              " ('somewhere', 308),\n",
              " ('vampire', 308),\n",
              " ('spend', 307),\n",
              " ('th', 307),\n",
              " ('future', 306),\n",
              " ('difficult', 306),\n",
              " ('effect', 306),\n",
              " ('fighting', 306),\n",
              " ('street', 306),\n",
              " ('c', 305),\n",
              " ('america', 305),\n",
              " ('accent', 304),\n",
              " ('truth', 302),\n",
              " ('project', 302),\n",
              " ('joe', 301),\n",
              " ('f', 301),\n",
              " ('deal', 301),\n",
              " ('indeed', 301),\n",
              " ('biggest', 300),\n",
              " ('rate', 300),\n",
              " ('paul', 299),\n",
              " ('japanese', 299),\n",
              " ('utterly', 298),\n",
              " ('begins', 298),\n",
              " ('redeeming', 298),\n",
              " ('college', 298),\n",
              " ('york', 297),\n",
              " ('fairly', 297),\n",
              " ('disney', 297),\n",
              " ('crew', 296),\n",
              " ('create', 296),\n",
              " ('cartoon', 296),\n",
              " ('revenge', 296),\n",
              " ('co', 295),\n",
              " ('outside', 295),\n",
              " ('computer', 295),\n",
              " ('interested', 295),\n",
              " ('stage', 295),\n",
              " ('considering', 294),\n",
              " ('speak', 294),\n",
              " ('among', 294),\n",
              " ('towards', 293),\n",
              " ('channel', 293),\n",
              " ('sick', 293),\n",
              " ('talented', 292),\n",
              " ('cause', 292),\n",
              " ('particular', 292),\n",
              " ('van', 292),\n",
              " ('hair', 292),\n",
              " ('bottom', 291),\n",
              " ('reasons', 291),\n",
              " ('mediocre', 290),\n",
              " ('cat', 290),\n",
              " ('telling', 290),\n",
              " ('supporting', 289),\n",
              " ('store', 289),\n",
              " ('hoping', 288),\n",
              " ('waiting', 288),\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBOP2ZyNU60A",
        "colab_type": "text"
      },
      "source": [
        "Como pudemos ver, palavras comuns como `the` aparecem muito frequentemente nas reviews positivas e nas negativas. Em vez que encontrar as palavras mais frequente em cada tipo de review, o que realmente queremos são as palavras encontradas nas reviews positivas que são mais comuns do que nas negativas (e vice versa). Para isso, iremos calcular as **frequências** com que cada palavra aparece em cada classe de review.\n",
        "\n",
        ">Importante: a taxa positive-to-negative para uma dada palavra será calculada como `positive_counts[word] / float(negative_counts[word]+1)`. Note que o `+1` no denominador garante que não iremos realizar uma divisão por zero (caso em que uma palavra aparece apenas em reviews positivas).\n",
        "\n",
        "Iremos realizar esse cálculo apenas para as palavras mais comuns (consideraremos como comuns apenas as palavras que apareceram pelo menos 100 vezes)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MT3J0NupU60B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pos_neg_ratios = Counter()\n",
        "\n",
        "for term,cnt in list(total_counts.most_common()):\n",
        "    if(cnt > 100):\n",
        "        pos_neg_ratio = positive_counts[term] / float(negative_counts[term]+1)\n",
        "        pos_neg_ratios[term] = pos_neg_ratio"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwS7y8IaU60H",
        "colab_type": "text"
      },
      "source": [
        "Vamos visualizar as frequências de algumas palavras:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0aVasEdU60I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e4f06ec3-1ec7-4b47-9c4c-4806599ed76b"
      },
      "source": [
        "print(\"Pos-to-neg ratio for 'the' = {}\".format(pos_neg_ratios[\"the\"]))\n",
        "print(\"Pos-to-neg ratio for 'amazing' = {}\".format(pos_neg_ratios[\"amazing\"]))\n",
        "print(\"Pos-to-neg ratio for 'terrible' = {}\".format(pos_neg_ratios[\"terrible\"]))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pos-to-neg ratio for 'the' = 1.0607993145235326\n",
            "Pos-to-neg ratio for 'amazing' = 4.022813688212928\n",
            "Pos-to-neg ratio for 'terrible' = 0.17744252873563218\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgK2A7b_U60W",
        "colab_type": "text"
      },
      "source": [
        "Vemos o seguinte:\n",
        "\n",
        "* Palavras que a gente esperaria ver com mais frequência em críticas positivas - como \"amazing\" - têm uma proporção maior que 1. Quanto maior a relação dessa palavra com reviews positivas, mais distante de 1 será sua taxa.\n",
        "* Palavras que a gente esperaria ver com mais frequência em críticas negativas - como \"terrible\" - têm taxas inferiores a 1. Quanto maior a relação dessa palavra com reviews negativas, mais próxima de zero será sua relação positivo / negativo.\n",
        "* Palavras neutras, que realmente não transmitem nenhum sentimento, porque você esperaria vê-las em todos os tipos de críticas - como \"the\" - têm valores muito próximos de 1. Uma palavra perfeitamente neutra - usada exatamente na mesma frequência de críticas positivas como críticas negativas - seria quase exatamente 1. O `+ 1` que sugerimos que você adicione ao denominador enviesa levemente as palavras para negativas, mas isso não fará diferença porque será um pequeno viés e mais tarde estaremos ignorando palavras que são muito próximas de neutras.\n",
        "\n",
        "Ok, as frequências nos dizem quais palavras são usadas com mais frequência em reviews positivas ou negativas, mas os valores específicos que calculamos são um pouco difíceis de trabalhar. Uma palavra muito positiva como \"amazing\" tem um valor acima de 4, enquanto uma palavra muito negativa como \"terrible\" tem um valor em torno de 0,18. Esses valores não são fáceis de comparar por alguns motivos:\n",
        "\n",
        "* No momento, 1 é considerado neutro, mas o valor absoluto das taxas postive-to-negative de palavras muito positivas é maior que o valor absoluto das taxas para as palavras muito negativas. Entretanto, não há como comparar diretamente dois números e ver se uma palavra transmite a mesma magnitude de sentimento positivo que outra palavra transmite sentimento negativo. Portanto, devemos centralizar todos os valores ao redor do valor neutro. Assim, poderemos comparar o quanto de positividade ou de negatividade uma palavra transmite.\n",
        "* Ao comparar valores absolutos, é mais fácil fazer isso em torno de zero do que de 1.\n",
        "\n",
        "Para corrigir esses problemas, converteremos todas as nossas taxas para novos valores utilizando logaritmos (`np.log(ratio)`).\n",
        "\n",
        "No final, palavras extremamente positivas e extremamente negativas terão proporções positivas-negativas com magnitudes semelhantes, mas sinais opostos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOTr1vIzU60Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "a4f59be8-b34b-4aaa-bcf4-51d634634388"
      },
      "source": [
        "for word, ratio in pos_neg_ratios.most_common():\n",
        "    pos_neg_ratios[word] = np.log(ratio)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in log\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTRIwBo1U60g",
        "colab_type": "text"
      },
      "source": [
        "Vamos dar uma olhada nos novos valores para as mesmas palavras que vimos anteriormente:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Xm-VC-JU60h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c2312946-cf29-4aa9-b749-8daa837733ed"
      },
      "source": [
        "print(\"Pos-to-neg ratio for 'the' = {}\".format(pos_neg_ratios[\"the\"]))\n",
        "print(\"Pos-to-neg ratio for 'amazing' = {}\".format(pos_neg_ratios[\"amazing\"]))\n",
        "print(\"Pos-to-neg ratio for 'terrible' = {}\".format(pos_neg_ratios[\"terrible\"]))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pos-to-neg ratio for 'the' = 0.05902269426102881\n",
            "Pos-to-neg ratio for 'amazing' = 1.3919815802404802\n",
            "Pos-to-neg ratio for 'terrible' = -1.7291085042663878\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8Lm3T1yU60n",
        "colab_type": "text"
      },
      "source": [
        "Agora, vemos que palavras neutras possuem taxas próximas de zero (`the`). A palavra `amazing` possui um valor bem maior do que zero, indicando sua clara significância positiva. `terrible` possui um valor bem baixo, também indicando que ela tem um forte sentido negativo. Note, entretanto, que `terrible` é uma palavra mais relacionada a reviews negativas do que `amazing` se relaciona com reviews positivas.\n",
        "\n",
        "Vamos, agora, ver as taxas das demais palavras. A próxima célula exibe todas as palavras, ordenadas de acordo com sua relação com reviews positivas. A célula seguinte, por sua vez, exibe as 30 palavras mais relacionadas com reviews negativas.\n",
        "\n",
        "Relembrando: palavras neutras terão valores próximos de `0`. Palavras positivas terão valores maiores (até maiores do que `1`). Palavras relacionadas a reviews negativas terão valores negativos (até menores do que `-1`). Esses novos valores são a razão para termos utilizado o logaritmo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ws9NFwOTU60o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9f504a76-a093-4907-bf5b-87e88c565373"
      },
      "source": [
        "pos_neg_ratios.most_common()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('', nan),\n",
              " ('loved', 0.14528245890406116),\n",
              " ('the', -2.8298332605426704),\n",
              " ('.', nan),\n",
              " ('and', -1.674252477599604),\n",
              " ('to', nan),\n",
              " ('br', nan),\n",
              " ('it', nan),\n",
              " ('i', nan),\n",
              " ('this', nan),\n",
              " ('that', nan),\n",
              " ('was', nan),\n",
              " ('as', -1.4133751189249417),\n",
              " ('is', -2.0126153400118763),\n",
              " ('of', -2.2286925693498265),\n",
              " ('with', -2.2308951204645417),\n",
              " ('a', -2.92657791456808),\n",
              " ('movie', nan),\n",
              " ('but', nan),\n",
              " ('film', -2.4578532477038557),\n",
              " ('you', nan),\n",
              " ('on', nan),\n",
              " ('t', nan),\n",
              " ('not', nan),\n",
              " ('his', -1.0517858419568342),\n",
              " ('he', -1.8246772654209993),\n",
              " ('in', -1.9824545891751846),\n",
              " ('s', -2.6674939322585116),\n",
              " ('for', -3.816229733505857),\n",
              " ('are', -4.387157598400469),\n",
              " ('have', nan),\n",
              " ('be', nan),\n",
              " ('one', -3.2486823642488063),\n",
              " ('all', nan),\n",
              " ('at', nan),\n",
              " ('they', nan),\n",
              " ('who', -1.9687512871713466),\n",
              " ('by', -2.0515870366701274),\n",
              " ('an', -2.350339992429906),\n",
              " ('so', nan),\n",
              " ('from', -2.2919768012950548),\n",
              " ('like', nan),\n",
              " ('there', nan),\n",
              " ('her', -1.2874425650932688),\n",
              " ('or', nan),\n",
              " ('just', nan),\n",
              " ('about', nan),\n",
              " ('out', nan),\n",
              " ('if', nan),\n",
              " ('what', nan),\n",
              " ('some', nan),\n",
              " ('can', nan),\n",
              " ('very', -1.007713855383062),\n",
              " ('she', -1.670658430458523),\n",
              " ('more', -2.1984468426570287),\n",
              " ('when', -2.274096782555312),\n",
              " ('up', nan),\n",
              " ('no', nan),\n",
              " ('even', nan),\n",
              " ('would', nan),\n",
              " ('story', -1.3333634362622448),\n",
              " ('has', -1.6666716575048397),\n",
              " ('only', nan),\n",
              " ('really', nan),\n",
              " ('their', -1.9262126461113385),\n",
              " ('see', -2.3034101856475417),\n",
              " ('which', -2.516755360632551),\n",
              " ('my', -2.583202443920954),\n",
              " ('time', -3.0376733391795367),\n",
              " ('good', -3.241725692191893),\n",
              " ('had', nan),\n",
              " ('we', -3.2913113736661597),\n",
              " ('were', nan),\n",
              " ('me', nan),\n",
              " ('well', -0.8940788197189502),\n",
              " ('than', nan),\n",
              " ('much', nan),\n",
              " ('get', nan),\n",
              " ('bad', nan),\n",
              " ('been', nan),\n",
              " ('people', nan),\n",
              " ('do', nan),\n",
              " ('into', nan),\n",
              " ('great', -0.11866562738442617),\n",
              " ('also', -0.8431164763428014),\n",
              " ('will', -1.3193052761147461),\n",
              " ('other', -2.115131464190393),\n",
              " ('first', -2.3130891688893658),\n",
              " ('because', nan),\n",
              " ('how', nan),\n",
              " ('him', -1.3370343439629537),\n",
              " ('don', nan),\n",
              " ('most', -1.5335446130689114),\n",
              " ('made', nan),\n",
              " ('its', -1.4504794161567471),\n",
              " ('then', nan),\n",
              " ('make', nan),\n",
              " ('way', -4.468642024076318),\n",
              " ('them', nan),\n",
              " ('could', nan),\n",
              " ('too', nan),\n",
              " ('movies', nan),\n",
              " ('any', nan),\n",
              " ('after', nan),\n",
              " ('think', -5.804449421020215),\n",
              " ('characters', nan),\n",
              " ('character', -6.217603276059524),\n",
              " ('watch', nan),\n",
              " ('life', -0.6033884829397731),\n",
              " ('being', nan),\n",
              " ('plot', nan),\n",
              " ('acting', nan),\n",
              " ('never', nan),\n",
              " ('best', -0.32369572807782304),\n",
              " ('love', -0.3681905151795135),\n",
              " ('many', -1.3553371233528027),\n",
              " ('films', -1.541142805520789),\n",
              " ('two', -1.818804442807698),\n",
              " ('seen', -3.116350152545024),\n",
              " ('where', nan),\n",
              " ('over', nan),\n",
              " ('did', nan),\n",
              " ('show', -1.7077502388707841),\n",
              " ('little', -2.5791787051670907),\n",
              " ('know', nan),\n",
              " ('off', nan),\n",
              " ('ever', nan),\n",
              " ('man', -1.1450724197872604),\n",
              " ('does', -2.7129151490199592),\n",
              " ('here', nan),\n",
              " ('better', nan),\n",
              " ('your', nan),\n",
              " ('end', nan),\n",
              " ('still', -0.9674721504283418),\n",
              " ('these', nan),\n",
              " ('say', nan),\n",
              " ('scene', nan),\n",
              " ('why', nan),\n",
              " ('while', -1.709194060209367),\n",
              " ('scenes', nan),\n",
              " ('go', nan),\n",
              " ('ve', nan),\n",
              " ('such', -4.043999852187725),\n",
              " ('something', nan),\n",
              " ('should', nan),\n",
              " ('m', nan),\n",
              " ('through', nan),\n",
              " ('watching', nan),\n",
              " ('though', -1.7830781621481595),\n",
              " ('now', -2.8741678252228184),\n",
              " ('doesn', nan),\n",
              " ('thing', nan),\n",
              " ('years', -0.8003049317873023),\n",
              " ('real', -1.7841515136650883),\n",
              " ('those', -2.1548239386661625),\n",
              " ('back', -3.014208480134588),\n",
              " ('re', nan),\n",
              " ('actors', nan),\n",
              " ('director', nan),\n",
              " ('work', -1.9264687129451024),\n",
              " ('old', -3.1706438543835413),\n",
              " ('another', nan),\n",
              " ('before', -4.311709421948229),\n",
              " ('didn', nan),\n",
              " ('new', -1.2677531593105322),\n",
              " ('nothing', nan),\n",
              " ('funny', nan),\n",
              " ('actually', nan),\n",
              " ('makes', -1.2902284733834573),\n",
              " ('look', nan),\n",
              " ('find', -1.6117078708474428),\n",
              " ('going', nan),\n",
              " ('few', nan),\n",
              " ('same', nan),\n",
              " ('part', -2.5671844388716747),\n",
              " ('every', nan),\n",
              " ('down', nan),\n",
              " ('want', nan),\n",
              " ('pretty', nan),\n",
              " ('seems', nan),\n",
              " ('around', nan),\n",
              " ('horror', nan),\n",
              " ('got', nan),\n",
              " ('fact', nan),\n",
              " ('take', nan),\n",
              " ('big', -2.4365478071269058),\n",
              " ('enough', nan),\n",
              " ('long', nan),\n",
              " ('thought', nan),\n",
              " ('both', -0.41175982683489804),\n",
              " ('always', -0.5196862505221234),\n",
              " ('young', -0.5528345664490961),\n",
              " ('world', -0.6024981246912223),\n",
              " ('series', -0.6300204255123992),\n",
              " ('may', -1.0171526550218999),\n",
              " ('give', nan),\n",
              " ('original', nan),\n",
              " ('own', -0.8844689515280694),\n",
              " ('between', -0.9606041813875542),\n",
              " ('quite', -1.2239039592023115),\n",
              " ('us', -1.2620702839015085),\n",
              " ('times', -1.3320013156331156),\n",
              " ('cast', -1.4703427198663326),\n",
              " ('again', -1.8708271720262495),\n",
              " ('comedy', -1.9224994684167045),\n",
              " ('action', -2.021219116502604),\n",
              " ('must', -2.0692330542997404),\n",
              " ('lot', -2.3272389586893403),\n",
              " ('without', -3.0808164051878952),\n",
              " ('right', -3.285512945241637),\n",
              " ('however', -3.4885424492740844),\n",
              " ('point', nan),\n",
              " ('gets', nan),\n",
              " ('family', -0.5957256945852399),\n",
              " ('come', nan),\n",
              " ('role', -0.7440138655332088),\n",
              " ('isn', nan),\n",
              " ('almost', nan),\n",
              " ('interesting', nan),\n",
              " ('least', nan),\n",
              " ('whole', nan),\n",
              " ('d', nan),\n",
              " ('bit', -1.1972849793767457),\n",
              " ('music', -1.2823719042777644),\n",
              " ('saw', -1.9908774474043438),\n",
              " ('guy', nan),\n",
              " ('script', nan),\n",
              " ('far', nan),\n",
              " ('making', nan),\n",
              " ('minutes', nan),\n",
              " ('feel', -1.8059065799538274),\n",
              " ('anything', nan),\n",
              " ('might', nan),\n",
              " ('performance', -0.3740044311894145),\n",
              " ('last', -2.2661985804753284),\n",
              " ('since', -2.4138724729591785),\n",
              " ('done', -3.0273583487382423),\n",
              " ('ll', nan),\n",
              " ('girl', nan),\n",
              " ('probably', nan),\n",
              " ('am', nan),\n",
              " ('woman', -2.8357918656951675),\n",
              " ('things', -5.322736955673541),\n",
              " ('kind', nan),\n",
              " ('away', nan),\n",
              " ('rather', nan),\n",
              " ('worst', nan),\n",
              " ('fun', -0.6647722051134808),\n",
              " ('sure', nan),\n",
              " ('hard', nan),\n",
              " ('anyone', nan),\n",
              " ('each', -0.6183714305165982),\n",
              " ('having', nan),\n",
              " ('especially', -0.7153668626887539),\n",
              " ('day', -1.0388918275706),\n",
              " ('yet', -1.3382663952341138),\n",
              " ('course', -1.7890369969019089),\n",
              " ('tv', -4.404911798719404),\n",
              " ('believe', nan),\n",
              " ('looking', nan),\n",
              " ('trying', nan),\n",
              " ('goes', nan),\n",
              " ('book', nan),\n",
              " ('looks', nan),\n",
              " ('different', -0.6215735759586375),\n",
              " ('played', -0.8350047024951279),\n",
              " ('although', -0.9947939784073092),\n",
              " ('our', -1.142928906580364),\n",
              " ('place', -2.3581205885778123),\n",
              " ('screen', -2.3741839671652576),\n",
              " ('set', -2.51707256869903),\n",
              " ('found', -3.5224103921393097),\n",
              " ('comes', -3.5694665663502994),\n",
              " ('actor', -4.252252743433498),\n",
              " ('put', nan),\n",
              " ('money', nan),\n",
              " ('year', -1.6241500766681995),\n",
              " ('ending', nan),\n",
              " ('maybe', nan),\n",
              " ('let', nan),\n",
              " ('someone', nan),\n",
              " ('sense', nan),\n",
              " ('everything', nan),\n",
              " ('reason', nan),\n",
              " ('wasn', nan),\n",
              " ('job', -0.5134029066887639),\n",
              " ('shows', -0.6555562135607036),\n",
              " ('dvd', -1.351328974681687),\n",
              " ('main', nan),\n",
              " ('watched', nan),\n",
              " ('effects', nan),\n",
              " ('audience', nan),\n",
              " ('said', nan),\n",
              " ('takes', -1.3206407121633954),\n",
              " ('instead', nan),\n",
              " ('beautiful', -0.027100922297459393),\n",
              " ('true', -0.5748038542368509),\n",
              " ('plays', -0.6936276189741954),\n",
              " ('john', -0.7938719037976418),\n",
              " ('later', -0.8841947491527995),\n",
              " ('together', -1.2604582835673435),\n",
              " ('american', -1.424099704627237),\n",
              " ('once', -1.4878938611881782),\n",
              " ('play', -1.490972433063259),\n",
              " ('everyone', -1.6262042545542021),\n",
              " ('worth', -1.94288569548223),\n",
              " ('three', -2.381626354130956),\n",
              " ('house', -4.357610486241082),\n",
              " ('seem', nan),\n",
              " ('night', nan),\n",
              " ('high', -3.653381394180589),\n",
              " ('left', nan),\n",
              " ('special', nan),\n",
              " ('half', nan),\n",
              " ('excellent', 0.3816872082649481),\n",
              " ('father', -0.42813532141730226),\n",
              " ('war', -0.5648715223442893),\n",
              " ('himself', -1.2744494109352122),\n",
              " ('wife', -1.4458193079789834),\n",
              " ('shot', nan),\n",
              " ('idea', nan),\n",
              " ('black', nan),\n",
              " ('nice', -1.383563561840814),\n",
              " ('less', nan),\n",
              " ('else', nan),\n",
              " ('simply', nan),\n",
              " ('read', nan),\n",
              " ('men', -1.279340644082749),\n",
              " ('hollywood', -1.5332580295966092),\n",
              " ('star', -1.579390159068697),\n",
              " ('version', -1.7997318427563895),\n",
              " ('seeing', -2.151397730742147),\n",
              " ('fan', -2.19829294540859),\n",
              " ('during', -2.208019392654216),\n",
              " ('death', -2.7331771554657998),\n",
              " ('poor', nan),\n",
              " ('help', -2.411560709851853),\n",
              " ('completely', nan),\n",
              " ('used', nan),\n",
              " ('home', -1.6487841759504471),\n",
              " ('second', -2.8814870636461056),\n",
              " ('mind', -4.2666795474834105),\n",
              " ('dead', nan),\n",
              " ('line', nan),\n",
              " ('short', -1.8194744813460744),\n",
              " ('either', nan),\n",
              " ('given', nan),\n",
              " ('kids', nan),\n",
              " ('budget', nan),\n",
              " ('try', nan),\n",
              " ('classic', -0.469924329541232),\n",
              " ('wrong', nan),\n",
              " ('performances', -0.28923991838303864),\n",
              " ('women', nan),\n",
              " ('enjoy', -0.6138872074033546),\n",
              " ('boring', nan),\n",
              " ('need', nan),\n",
              " ('use', nan),\n",
              " ('rest', nan),\n",
              " ('low', nan),\n",
              " ('production', nan),\n",
              " ('camera', nan),\n",
              " ('truly', -1.0915793225097108),\n",
              " ('top', -1.1976806882295814),\n",
              " ('full', -1.4298775089754863),\n",
              " ('video', nan),\n",
              " ('awful', nan),\n",
              " ('couple', nan),\n",
              " ('tell', nan),\n",
              " ('remember', -1.238779691094211),\n",
              " ('along', -1.4763675120269375),\n",
              " ('friends', -1.6591303549219785),\n",
              " ('until', -1.897313700817265),\n",
              " ('next', -inf),\n",
              " ('stupid', nan),\n",
              " ('start', nan),\n",
              " ('perhaps', -2.3167234139986284),\n",
              " ('stars', -2.336561776404905),\n",
              " ('sex', nan),\n",
              " ('mean', nan),\n",
              " ('school', nan),\n",
              " ('wonderful', 0.44759350812580423),\n",
              " ('episode', -0.7438170848502131),\n",
              " ('understand', nan),\n",
              " ('terrible', nan),\n",
              " ('getting', nan),\n",
              " ('written', nan),\n",
              " ('early', -0.9457950481217625),\n",
              " ('name', nan),\n",
              " ('doing', nan),\n",
              " ('perfect', 0.22053413431605004),\n",
              " ('person', nan),\n",
              " ('definitely', -0.41249290305171366),\n",
              " ('gives', -0.4559655354321367),\n",
              " ('human', -0.6567880342289975),\n",
              " ('often', -0.8476823538106614),\n",
              " ('small', -0.9165344746996515),\n",
              " ('recommend', -0.9175989215223699),\n",
              " ('others', -1.1046745483938207),\n",
              " ('style', -1.323392411371226),\n",
              " ('moments', -1.4218821656078435),\n",
              " ('won', -1.7201000600006886),\n",
              " ('came', -2.0098301764247224),\n",
              " ('playing', -2.0992184179351656),\n",
              " ('keep', -2.5910485200622735),\n",
              " ('face', -3.9403359808830953),\n",
              " ('itself', nan),\n",
              " ('lines', nan),\n",
              " ('live', -0.88825939283349),\n",
              " ('become', -0.9742477633909776),\n",
              " ('boy', -1.4132699172538628),\n",
              " ('lost', -2.543408159159639),\n",
              " ('dialogue', nan),\n",
              " ('head', nan),\n",
              " ('piece', nan),\n",
              " ('case', nan),\n",
              " ('yes', nan),\n",
              " ('felt', nan),\n",
              " ('liked', -0.8192776502391568),\n",
              " ('mother', -1.0421588154462478),\n",
              " ('finally', -2.00821170993701),\n",
              " ('supposed', nan),\n",
              " ('children', -1.5732874400802237),\n",
              " ('title', nan),\n",
              " ('couldn', nan),\n",
              " ('cinema', -0.9548528404990552),\n",
              " ('white', -2.9769231534851377),\n",
              " ('absolutely', nan),\n",
              " ('picture', -1.406541639878907),\n",
              " ('against', -1.5778756667030653),\n",
              " ('sort', nan),\n",
              " ('worse', nan),\n",
              " ('certainly', -1.3325087292158688),\n",
              " ('went', nan),\n",
              " ('entire', nan),\n",
              " ('waste', nan),\n",
              " ('killer', nan),\n",
              " ('problem', nan),\n",
              " ('oh', nan),\n",
              " ('mr', -1.1703327715871048),\n",
              " ('hope', -1.9581609871072212),\n",
              " ('evil', nan),\n",
              " ('entertaining', -1.0025473117432309),\n",
              " ('friend', -1.1845051601135859),\n",
              " ('overall', -1.1856281111363023),\n",
              " ('called', nan),\n",
              " ('based', -1.7033887246262365),\n",
              " ('direction', nan),\n",
              " ('care', nan),\n",
              " ('already', nan),\n",
              " ('laugh', nan),\n",
              " ('example', nan),\n",
              " ('seemed', nan),\n",
              " ('turn', nan),\n",
              " ('unfortunately', nan),\n",
              " ('wanted', nan),\n",
              " ('fine', -0.5025770326579434),\n",
              " ('final', -1.4833704551281823),\n",
              " ('child', -1.7312387247118048),\n",
              " ('sound', nan),\n",
              " ('amazing', 0.33072832923881457),\n",
              " ('heart', -0.04878269148992608),\n",
              " ('lives', -0.5428364179277883),\n",
              " ('drama', -0.7293548687794968),\n",
              " ('history', -0.7395478961273945),\n",
              " ('michael', -0.8678486150274423),\n",
              " ('dark', -0.8967880159352528),\n",
              " ('son', -0.9428307477054504),\n",
              " ('throughout', -1.058273894722725),\n",
              " ('fans', -1.4142699376232744),\n",
              " ('several', -1.5719525272733839),\n",
              " ('despite', -1.9916125954217956),\n",
              " ('becomes', -2.192572882525771),\n",
              " ('guess', nan),\n",
              " ('lead', nan),\n",
              " ('humor', -1.4430482414531847),\n",
              " ('beginning', -2.220793784899801),\n",
              " ('under', -3.094530481327404),\n",
              " ('totally', nan),\n",
              " ('writing', nan),\n",
              " ('guys', nan),\n",
              " ('quality', nan),\n",
              " ('behind', nan),\n",
              " ('tries', nan),\n",
              " ('able', -1.1804054559425572),\n",
              " ('flick', nan),\n",
              " ('hand', nan),\n",
              " ('act', nan),\n",
              " ('today', 0.09988441022292016),\n",
              " ('kill', nan),\n",
              " ('favorite', 0.23656952208641818),\n",
              " ('enjoyed', -0.13845554264332788),\n",
              " ('works', -0.3938502511478404),\n",
              " ('genre', -0.9931172432829408),\n",
              " ('past', -1.0266296658237846),\n",
              " ('town', -1.169492328385279),\n",
              " ('game', -1.2650241941466167),\n",
              " ('art', -1.2810672106446455),\n",
              " ('side', -1.372382006923606),\n",
              " ('viewer', -1.5389958015019498),\n",
              " ('turns', -1.873037364872722),\n",
              " ('close', -2.667598046413476),\n",
              " ('car', nan),\n",
              " ('soon', -1.2164982001328206),\n",
              " ('wants', -2.497618955675441),\n",
              " ('starts', nan),\n",
              " ('run', nan),\n",
              " ('sometimes', -0.7860737376852868),\n",
              " ('days', -1.6375085129560072),\n",
              " ('gave', nan),\n",
              " ('eyes', -2.2520137502314466),\n",
              " ('actress', -2.6753940807546774),\n",
              " ('b', nan),\n",
              " ('late', -0.8792348118980322),\n",
              " ('girls', nan),\n",
              " ('etc', nan),\n",
              " ('god', nan),\n",
              " ('horrible', nan),\n",
              " ('kid', nan),\n",
              " ('brilliant', 0.20600179838324917),\n",
              " ('parts', -1.803013472387631),\n",
              " ('directed', -2.195363147947235),\n",
              " ('hour', nan),\n",
              " ('blood', nan),\n",
              " ('self', nan),\n",
              " ('stories', -1.0183213273282408),\n",
              " ('themselves', -2.9501354797577912),\n",
              " ('thinking', nan),\n",
              " ('expect', nan),\n",
              " ('stuff', nan),\n",
              " ('obviously', nan),\n",
              " ('decent', nan),\n",
              " ('writer', nan),\n",
              " ('highly', 0.13279938003530176),\n",
              " ('city', -0.4031554291522925),\n",
              " ('myself', nan),\n",
              " ('feeling', -1.826942864673775),\n",
              " ('slow', nan),\n",
              " ('except', nan),\n",
              " ('age', -0.6943214062646972),\n",
              " ('voice', -2.0686498510720677),\n",
              " ('matter', -2.114333260061522),\n",
              " ('type', -2.9318455574633977),\n",
              " ('fight', -3.788508067189951),\n",
              " ('daughter', -4.735314446905754),\n",
              " ('anyway', nan),\n",
              " ('roles', -0.6963579987090108),\n",
              " ('moment', -1.6088072042350317),\n",
              " ('killed', nan),\n",
              " ('heard', nan),\n",
              " ('says', nan),\n",
              " ('strong', -0.1723656523674762),\n",
              " ('brother', -1.1022172737576972),\n",
              " ('cannot', nan),\n",
              " ('violence', nan),\n",
              " ('stop', nan),\n",
              " ('happens', nan),\n",
              " ('particularly', -1.894310549787399),\n",
              " ('involved', nan),\n",
              " ('happened', nan),\n",
              " ('extremely', nan),\n",
              " ('james', -0.6964081664100337),\n",
              " ('obvious', nan),\n",
              " ('murder', -0.8316783167947872),\n",
              " ('chance', -1.126060998670087),\n",
              " ('known', -1.1474216260471755),\n",
              " ('living', -1.1938571317626105),\n",
              " ('hit', -1.2819253215541437),\n",
              " ('told', -1.3148330315233716),\n",
              " ('coming', -2.197137508648104),\n",
              " ('police', -2.2127572372750897),\n",
              " ('leave', -3.9183335648355744),\n",
              " ('alone', nan),\n",
              " ('experience', -0.846049274143382),\n",
              " ('lack', nan),\n",
              " ('hero', nan),\n",
              " ('wouldn', nan),\n",
              " ('including', -0.9506047969919367),\n",
              " ('took', -2.6995324089415167),\n",
              " ('attempt', nan),\n",
              " ('please', nan),\n",
              " ('happen', nan),\n",
              " ('gore', nan),\n",
              " ('crap', nan),\n",
              " ('wonder', nan),\n",
              " ('cut', nan),\n",
              " ('group', nan),\n",
              " ('complete', nan),\n",
              " ('interest', nan),\n",
              " ('none', nan),\n",
              " ('hell', nan),\n",
              " ('save', nan),\n",
              " ('simple', -0.2924745919273338),\n",
              " ('david', -0.5853135250186908),\n",
              " ('score', -0.8888549010987331),\n",
              " ('ago', -1.0224040341784293),\n",
              " ('husband', -1.474725338302989),\n",
              " ('ok', nan),\n",
              " ('looked', nan),\n",
              " ('song', -0.969943548754456),\n",
              " ('number', -1.2953830775937754),\n",
              " ('seriously', nan),\n",
              " ('possible', nan),\n",
              " ('annoying', nan),\n",
              " ('sad', nan),\n",
              " ('exactly', nan),\n",
              " ('shown', -1.3285763975848843),\n",
              " ('king', -1.3830857459698018),\n",
              " ('running', nan),\n",
              " ('musical', -0.6355035403894466),\n",
              " ('career', -1.655192695034585),\n",
              " ('yourself', nan),\n",
              " ('serious', nan),\n",
              " ('scary', nan),\n",
              " ('reality', -0.7826486322680739),\n",
              " ('released', -0.9692739737598637),\n",
              " ('cinematography', -1.0427350341262371),\n",
              " ('whose', -1.0617249323211626),\n",
              " ('english', -1.6122346245530859),\n",
              " ('ends', nan),\n",
              " ('hours', nan),\n",
              " ('usually', nan),\n",
              " ('opening', nan),\n",
              " ('jokes', nan),\n",
              " ('cool', nan),\n",
              " ('body', nan),\n",
              " ('relationship', -0.294582729370592),\n",
              " ('hilarious', -0.6988724992392009),\n",
              " ('happy', -0.7135545005216958),\n",
              " ('light', -0.8236105297449164),\n",
              " ('somewhat', -1.2108471640297689),\n",
              " ('usual', -1.4570106124286764),\n",
              " ('ridiculous', nan),\n",
              " ('level', nan),\n",
              " ('started', nan),\n",
              " ('change', -1.1302760839955848),\n",
              " ('opinion', -1.468904189820596),\n",
              " ('middle', nan),\n",
              " ('talking', nan),\n",
              " ('documentary', -0.917906485312585),\n",
              " ('robert', -0.9267314344520325),\n",
              " ('view', -0.9685743140024848),\n",
              " ('ones', -1.8204800429058907),\n",
              " ('novel', -1.9505097700803706),\n",
              " ('wish', -2.482239581287543),\n",
              " ('taking', -2.9112675627697446),\n",
              " ('order', -2.985048172444278),\n",
              " ('across', -4.394436453146855),\n",
              " ('taken', -4.410763754495197),\n",
              " ('shots', nan),\n",
              " ('finds', -0.6905844052648609),\n",
              " ('power', -1.727764473813162),\n",
              " ('saying', nan),\n",
              " ('female', nan),\n",
              " ('huge', nan),\n",
              " ('room', -2.723691669170713),\n",
              " ('mostly', nan),\n",
              " ('episodes', -0.171693688185054),\n",
              " ('five', nan),\n",
              " ('talent', nan),\n",
              " ('rating', nan),\n",
              " ('modern', -0.7874524187417047),\n",
              " ('earth', nan),\n",
              " ('major', nan),\n",
              " ('word', nan),\n",
              " ('turned', nan),\n",
              " ('call', nan),\n",
              " ('apparently', nan),\n",
              " ('single', nan),\n",
              " ('disappointed', nan),\n",
              " ('events', -0.8499319586504092),\n",
              " ('songs', -1.0926079464719285),\n",
              " ('basically', nan),\n",
              " ('future', -0.4180236605289721),\n",
              " ('jack', -0.4488167132253821),\n",
              " ('important', -0.8939351664522318),\n",
              " ('country', -1.0064532765002943),\n",
              " ('comic', -1.161709715475763),\n",
              " ('television', -1.2391918390923218),\n",
              " ('attention', -1.3922652077463902),\n",
              " ('knows', -1.6540779008941118),\n",
              " ('due', -1.9288852557790053),\n",
              " ('four', -2.170163894480828),\n",
              " ('strange', -2.3104232549262944),\n",
              " ('non', nan),\n",
              " ('supporting', -0.2962815633952062),\n",
              " ('clearly', nan),\n",
              " ('fast', nan),\n",
              " ('paul', -0.3713415760950168),\n",
              " ('thriller', -1.1571920287899358),\n",
              " ('cheap', nan),\n",
              " ('silly', nan),\n",
              " ('aren', nan),\n",
              " ('words', nan),\n",
              " ('beyond', nan),\n",
              " ('straight', nan),\n",
              " ('oscar', -0.012863327890703595),\n",
              " ('romantic', -0.3614724175613839),\n",
              " ('predictable', nan),\n",
              " ('moving', -0.15587503727672308),\n",
              " ('tells', -0.6459753187296858),\n",
              " ('miss', -0.8438206230401584),\n",
              " ('british', -1.0415869238987883),\n",
              " ('similar', -1.188099730862763),\n",
              " ('rock', -1.1917728150995988),\n",
              " ('george', -1.3187128553064982),\n",
              " ('sequence', -1.4527000261204712),\n",
              " ('o', -1.5829422812335339),\n",
              " ('sets', -1.6581456669279537),\n",
              " ('entertainment', -1.7803452859239932),\n",
              " ('class', -1.823724309673311),\n",
              " ('easily', -2.092839683081416),\n",
              " ('bring', -2.162220464576808),\n",
              " ('upon', -2.2341889591421555),\n",
              " ('whether', -2.448750995495042),\n",
              " ('problems', -2.4576275100796146),\n",
              " ('knew', -2.8890021684614196),\n",
              " ('local', -2.9932476192143067),\n",
              " ('review', nan),\n",
              " ('falls', nan),\n",
              " ('richard', -1.0760881123525619),\n",
              " ('talk', nan),\n",
              " ('enjoyable', -0.2844024460221266),\n",
              " ('appears', nan),\n",
              " ('giving', nan),\n",
              " ('message', -1.459444981083558),\n",
              " ('within', -1.6852222401797083),\n",
              " ('theater', nan),\n",
              " ('ten', nan),\n",
              " ('animation', -0.4587283566365466),\n",
              " ('near', nan),\n",
              " ('team', -0.9801592654723965),\n",
              " ('above', -1.7943503083010222),\n",
              " ('sequel', nan),\n",
              " ('red', -1.1467645066777636),\n",
              " ('sister', -1.3363759832069821),\n",
              " ('mystery', -1.7882577840522806),\n",
              " ('dull', nan),\n",
              " ('theme', -1.053673239308545),\n",
              " ('eye', -1.2107971868434073),\n",
              " ('lady', -1.938313509157708),\n",
              " ('stand', -1.9843423613808657),\n",
              " ('needs', -2.304132940595158),\n",
              " ('nearly', -inf),\n",
              " ('lee', nan),\n",
              " ('bunch', nan),\n",
              " ('points', nan),\n",
              " ('mention', nan),\n",
              " ('add', nan),\n",
              " ('york', -0.6103101799488199),\n",
              " ('feels', nan),\n",
              " ('herself', -0.9361752861868465),\n",
              " ('storyline', nan),\n",
              " ('easy', -0.4020310787232902),\n",
              " ('using', nan),\n",
              " ('fantastic', 0.40868883089622865),\n",
              " ('lots', nan),\n",
              " ('begins', -0.6730377304681715),\n",
              " ('die', nan),\n",
              " ('actual', nan),\n",
              " ('effort', nan),\n",
              " ('tale', -0.2612242263530655),\n",
              " ('french', -0.5584952302330339),\n",
              " ('working', -1.1008385093502788),\n",
              " ('minute', nan),\n",
              " ('hate', nan),\n",
              " ('stay', nan),\n",
              " ('among', -0.6864892945263376),\n",
              " ('ways', -0.7481639250718274),\n",
              " ('surprised', -0.7908902494733291),\n",
              " ('release', -0.9791164629529833),\n",
              " ('tom', -1.1856755911272663),\n",
              " ('viewers', -1.1882597944460909),\n",
              " ('follow', -1.2879066860120534),\n",
              " ('elements', -1.5608787057112674),\n",
              " ('feature', -1.5711050662980444),\n",
              " ('haven', -1.7820229208223324),\n",
              " ('comments', nan),\n",
              " ('typical', -1.4494285409545864),\n",
              " ('editing', nan),\n",
              " ('avoid', nan),\n",
              " ('showing', -1.6009138902251936),\n",
              " ('named', -1.6458156242513557),\n",
              " ('clear', -5.975080529674955),\n",
              " ('tried', nan),\n",
              " ('season', -0.1072821092128388),\n",
              " ('famous', -0.6516436695858429),\n",
              " ('sorry', nan),\n",
              " ('dialog', nan),\n",
              " ('check', nan),\n",
              " ('weak', nan),\n",
              " ('material', nan),\n",
              " ('realistic', -0.21309774576899126),\n",
              " ('figure', nan),\n",
              " ('doubt', nan),\n",
              " ('somehow', nan),\n",
              " ('space', nan),\n",
              " ('kept', nan),\n",
              " ('greatest', 0.024589888067112424),\n",
              " ('viewing', -0.6871225684862413),\n",
              " ('dance', -0.7015423553362258),\n",
              " ('crime', -0.7138148641904459),\n",
              " ('disney', -0.8495907100519672),\n",
              " ('parents', -0.977902305228853),\n",
              " ('soundtrack', -1.001089133957972),\n",
              " ('form', -1.0098072379824239),\n",
              " ('peter', -1.036256255677817),\n",
              " ('th', -1.0437857305168017),\n",
              " ('certain', -1.0707777401519625),\n",
              " ('period', -1.2249883639314325),\n",
              " ('leads', -1.4919049888604354),\n",
              " ('filmed', -2.8109330106355657),\n",
              " ('buy', -3.0020840001949787),\n",
              " ('means', -3.3036295103058744),\n",
              " ('fall', -3.757271192161042),\n",
              " ('gone', -4.834953017057884),\n",
              " ('lame', nan),\n",
              " ('suspense', -3.2770260484900122),\n",
              " ('general', -5.946728083684636),\n",
              " ('zombie', nan),\n",
              " ('imagine', nan),\n",
              " ('atmosphere', -0.34609153106015317),\n",
              " ('brought', -0.697479695886733),\n",
              " ('third', -3.203963182558743),\n",
              " ('hear', nan),\n",
              " ('whatever', nan),\n",
              " ('particular', -0.9111880194846436),\n",
              " ('move', nan),\n",
              " ('rent', nan),\n",
              " ('average', nan),\n",
              " ('wait', nan),\n",
              " ('reviews', nan),\n",
              " ('stage', -1.0569576199947563),\n",
              " ('america', -1.1276691486835906),\n",
              " ('deal', -1.138627599625078),\n",
              " ('sexual', -3.6818579668527884),\n",
              " ('poorly', nan),\n",
              " ('okay', nan),\n",
              " ('premise', nan),\n",
              " ('believable', -0.5180840335148457),\n",
              " ('surprise', -1.3314495919674927),\n",
              " ('sit', nan),\n",
              " ('nature', -0.4845973672513697),\n",
              " ('de', -0.8213607343018188),\n",
              " ('indeed', -1.1020005735049443),\n",
              " ('japanese', -1.1107747105494665),\n",
              " ('note', -1.4971351366520973),\n",
              " ('learn', -1.5663799456674317),\n",
              " ('eventually', -1.7103859358860842),\n",
              " ('sequences', -2.258658430314338),\n",
              " ('forget', -5.88471353236661),\n",
              " ('possibly', nan),\n",
              " ('subject', -1.7424331421196553),\n",
              " ('decided', nan),\n",
              " ('expected', nan),\n",
              " ('imdb', nan),\n",
              " ('free', nan),\n",
              " ('screenplay', nan),\n",
              " ('romance', -0.7130254773258267),\n",
              " ('killing', nan),\n",
              " ('joe', -1.3636732711929844),\n",
              " ('difficult', -1.4408282858139951),\n",
              " ('dog', -2.285884911264753),\n",
              " ('baby', -3.9019386579358333),\n",
              " ('nor', nan),\n",
              " ('hot', nan),\n",
              " ('reading', nan),\n",
              " ('question', nan),\n",
              " ('needed', nan),\n",
              " ('begin', nan),\n",
              " ('society', -0.32250804023856766),\n",
              " ('became', -1.040730211308958),\n",
              " ('directors', -4.215067997498374),\n",
              " ('dr', -4.249905835350891),\n",
              " ('unless', nan),\n",
              " ('credits', nan),\n",
              " ('shame', nan),\n",
              " ('superb', 0.5359970168817135),\n",
              " ('meets', -0.36651292058166435),\n",
              " ('otherwise', nan),\n",
              " ('write', nan),\n",
              " ('meet', -1.0938189233145286),\n",
              " ('dramatic', -1.120196348223614),\n",
              " ('situation', -1.1813534571776223),\n",
              " ('truth', -1.2993125853931229),\n",
              " ('street', -1.3985213498935645),\n",
              " ('leaves', -1.6420398826937184),\n",
              " ('male', nan),\n",
              " ('memorable', -0.1648606207403643),\n",
              " ('badly', nan),\n",
              " ('writers', nan),\n",
              " ('weird', nan),\n",
              " ('forced', nan),\n",
              " ('sci', -2.8528601885741667),\n",
              " ('crazy', nan),\n",
              " ('laughs', nan),\n",
              " ('emotional', -0.30546306624546743),\n",
              " ('jane', -0.455013221368712),\n",
              " ('older', -0.9216538157259934),\n",
              " ('earlier', -1.192329064309452),\n",
              " ('acted', -3.8546591256127773),\n",
              " ('monster', nan),\n",
              " ('beauty', 0.049180742027047204),\n",
              " ('fi', -3.029334887341015),\n",
              " ('open', -4.421836104636559),\n",
              " ('realize', nan),\n",
              " ('comment', nan),\n",
              " ('deep', -0.6479908175937849),\n",
              " ('dream', -0.7633771099062698),\n",
              " ('footage', nan),\n",
              " ('forward', nan),\n",
              " ('interested', -1.6897195674601106),\n",
              " ('ask', nan),\n",
              " ('fantasy', -0.4349955076727748),\n",
              " ('whom', -1.2272341302986973),\n",
              " ('plus', nan),\n",
              " ('sounds', nan),\n",
              " ('mark', -1.496822996403998),\n",
              " ('directing', nan),\n",
              " ('keeps', -0.7933019496439566),\n",
              " ('development', nan),\n",
              " ('features', -1.3984848140992738),\n",
              " ('mess', nan),\n",
              " ('air', nan),\n",
              " ('quickly', nan),\n",
              " ('box', nan),\n",
              " ('towards', -1.8511153472690478),\n",
              " ('girlfriend', nan),\n",
              " ('perfectly', 0.17997974801609698),\n",
              " ('worked', nan),\n",
              " ('cheesy', nan),\n",
              " ('unique', 0.08447072533316785),\n",
              " ('creepy', -2.7218746566185654),\n",
              " ('total', nan),\n",
              " ('hands', nan),\n",
              " ('result', nan),\n",
              " ('fire', nan),\n",
              " ('brings', -0.3127546221301261),\n",
              " ('personal', -0.4407551828557835),\n",
              " ('bill', -1.037403987012854),\n",
              " ('previous', -4.655538858598834),\n",
              " ('incredibly', nan),\n",
              " ('business', -0.8799773876998926),\n",
              " ('plenty', -1.0559621966593118),\n",
              " ('setting', -1.3118069926070424),\n",
              " ('effect', -2.762837645928659),\n",
              " ('joke', nan),\n",
              " ('doctor', nan),\n",
              " ('casting', nan),\n",
              " ('return', -1.013612826576873),\n",
              " ('apart', nan),\n",
              " ('christmas', nan),\n",
              " ('leading', -1.4463357790741755),\n",
              " ('rate', -2.5283932334519954),\n",
              " ('e', nan),\n",
              " ('admit', nan),\n",
              " ('powerful', 0.2623383238474925),\n",
              " ('appear', nan),\n",
              " ('cop', nan),\n",
              " ('present', -0.6666617196088679),\n",
              " ('meant', nan),\n",
              " ('hardly', nan),\n",
              " ('break', nan),\n",
              " ('masterpiece', -0.06022740217062515),\n",
              " ('era', -0.4314650802749518),\n",
              " ('battle', -0.90272045571788),\n",
              " ('boys', -1.0331326785234836),\n",
              " ('ben', -1.9474582293914804),\n",
              " ('background', -1.950700181501435),\n",
              " ('telling', -2.2310065800331316),\n",
              " ('potential', nan),\n",
              " ('pay', nan),\n",
              " ('political', -0.9196986747259168),\n",
              " ('secret', -1.893055900328939),\n",
              " ('create', -2.7804584106186176),\n",
              " ('gay', nan),\n",
              " ('fighting', nan),\n",
              " ('dumb', nan),\n",
              " ('fails', nan),\n",
              " ('twist', nan),\n",
              " ('various', -1.0875567190484465),\n",
              " ('villain', nan),\n",
              " ('nudity', nan),\n",
              " ('william', -0.4449888969188091),\n",
              " ('western', -1.069053504869794),\n",
              " ('portrayed', -1.1997771830068658),\n",
              " ('inside', -1.5271415486004494),\n",
              " ('reasons', -3.1336624847734598),\n",
              " ('co', -3.3107638023561687),\n",
              " ('outside', -3.7561587438221706),\n",
              " ('ideas', nan),\n",
              " ('front', nan),\n",
              " ('match', -0.3628809471261959),\n",
              " ('missing', nan),\n",
              " ('married', -0.8287721533159782),\n",
              " ('expecting', nan),\n",
              " ('fairly', nan),\n",
              " ('list', nan),\n",
              " ('unlike', -0.3776582993405231),\n",
              " ('success', -0.6752958930482996),\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "LBqaK_XKU60y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "f09ac37a-07ec-4575-c177-c1bad1c0764a"
      },
      "source": [
        "list(reversed(pos_neg_ratios.most_common()))[0:30]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('hartley', 0.47588499532711054),\n",
              " ('melting', nan),\n",
              " ('niro', nan),\n",
              " ('angeles', -3.238550274970754),\n",
              " ('specifically', -1.85032811106515),\n",
              " ('cinematographer', -1.2864368031365205),\n",
              " ('cure', nan),\n",
              " ('amanda', -2.1389110278431644),\n",
              " ('authority', -1.15094614041988),\n",
              " ('emphasis', -1.15094614041988),\n",
              " ('blew', -1.030930433158723),\n",
              " ('eighties', -1.030930433158723),\n",
              " ('oz', -0.8249545038324405),\n",
              " ('planned', -0.7348589869664729),\n",
              " ('ruthless', -0.4317917973011473),\n",
              " ('matches', -0.2449299875154264),\n",
              " ('adorable', 0.0701199182449436),\n",
              " ('soccer', 0.5640959754919254),\n",
              " ('senseless', nan),\n",
              " ('disappear', nan),\n",
              " ('practice', nan),\n",
              " ('advance', nan),\n",
              " ('outfit', nan),\n",
              " ('peoples', -0.5006512197182454),\n",
              " ('saga', -0.4317917973011473),\n",
              " ('backgrounds', -0.2449299875154264),\n",
              " ('awe', -0.02854291790698025),\n",
              " ('diana', 0.0701199182449436),\n",
              " ('april', 0.30005030723545273),\n",
              " ('simplistic', nan)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd6Re8EWU606",
        "colab_type": "text"
      },
      "source": [
        "# Fim do Projeto 1.\n",
        "\n",
        "# Transformando Texto em Números<a id='lesson_3'></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0q4r4V-PU607",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "5f957665-d2c4-4212-e50c-431f6c50c215"
      },
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "review = \"This was a horrible, terrible movie.\"\n",
        "\n",
        "Image(filename='sentiment_network.png')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-df0825d5913f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"This was a horrible, terrible movie.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sentiment_network.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munconfined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munconfined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mretina\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename)\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0municode_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    614\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1041\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1044\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retina_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_flags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sentiment_network.png'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-foJcKZ5U61F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "review = \"The movie was excellent\"\n",
        "\n",
        "Image(filename='sentiment_network_pos.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wXWiCt8U61M",
        "colab_type": "text"
      },
      "source": [
        "# Projeto 2: Criando Dados de Entrada/Saída<a id='project_2'></a>\n",
        "\n",
        "Vamos crias um [set](https://docs.python.org/3/tutorial/datastructures.html#sets) chamado `vocab` que contém todas as palavras do vocabulário."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4fApKA6U61O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = set(total_counts.keys())"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnS_1WR1U61V",
        "colab_type": "text"
      },
      "source": [
        "Vamos ver o tamanho do vocabulário (deveria haver **74074** palavras)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExyKi0E2U61W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "21e3065c-8b64-4959-a528-118077ef2cd1"
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "print(vocab_size)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "74074\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AK1i90-U61b",
        "colab_type": "text"
      },
      "source": [
        "A imagem abaixo representa as camadas de uma rede neural, que iremos construir mais adiante. `layer_0` é a camada de entrada, `layer_1` é uma camada oculta e `layer_2` é a camada de saída."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1EL1ZrcU61e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import Image\n",
        "Image(filename='sentiment_network_2.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhfmR2c2U61n",
        "colab_type": "text"
      },
      "source": [
        "Vamos criar um array numpy chamado `layer_0` e inicializá-lo com zeros. Esse array possuirá 1 linha e `vocab_size` colunas. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPm4COXfU61o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layer_0 = np.zeros((1, vocab_size))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJ9MDGOaU61v",
        "colab_type": "text"
      },
      "source": [
        "A célula abaixo deve resulta em `(1, 74074)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_OujM4nU61w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8e8f9aca-0ca3-419f-b4a7-ed50312b2615"
      },
      "source": [
        "layer_0.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 74074)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cq78WyF8U613",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import Image\n",
        "Image(filename='sentiment_network.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8KH2VgjU618",
        "colab_type": "text"
      },
      "source": [
        "`layer_0` possui uma entrada para cada palavra do vocabulário. Precisamos saber o índice de cada palavra, então iremos executar a célula abaixo para criar uma lookup table que armazena o índice de cada palavra."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJqpg3Y4U619",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "197d6b67-9100-4845-8219-8b53589c7327"
      },
      "source": [
        "word2index = {}\n",
        "for i, word in enumerate(vocab):\n",
        "    word2index[word] = i\n",
        "\n",
        "word2index"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'': 0,\n",
              " 'sacchi': 1,\n",
              " 'switchblade': 2,\n",
              " 'cels': 3,\n",
              " 'reprises': 4,\n",
              " 'dilemma': 5,\n",
              " 'surfers': 6,\n",
              " 'arrrrrggghhhhhh': 7,\n",
              " 'lenghts': 8,\n",
              " 'disappoint': 9,\n",
              " 'inveigh': 10,\n",
              " 'lenge': 11,\n",
              " 'chuckling': 12,\n",
              " 'huzzah': 13,\n",
              " 'prequels': 14,\n",
              " 'undated': 15,\n",
              " 'aesthetics': 16,\n",
              " 'doodads': 17,\n",
              " 'sugary': 18,\n",
              " 'upstanding': 19,\n",
              " 'punt': 20,\n",
              " 'feinting': 21,\n",
              " 'qv': 22,\n",
              " 'minium': 23,\n",
              " 'sherlock': 24,\n",
              " 'headdress': 25,\n",
              " 'useless': 26,\n",
              " 'cheesefest': 27,\n",
              " 'belmore': 28,\n",
              " 'blythe': 29,\n",
              " 'remand': 30,\n",
              " 'comme': 31,\n",
              " 'deceptiveness': 32,\n",
              " 'resurrection': 33,\n",
              " 'ago': 34,\n",
              " 'statute': 35,\n",
              " 'scapegoat': 36,\n",
              " 'polluted': 37,\n",
              " 'descent': 38,\n",
              " 'hippies': 39,\n",
              " 'allegation': 40,\n",
              " 'forcibly': 41,\n",
              " 'potter': 42,\n",
              " 'dos': 43,\n",
              " 'text': 44,\n",
              " 'broadens': 45,\n",
              " 'shabbiness': 46,\n",
              " 'femi': 47,\n",
              " 'pinnacles': 48,\n",
              " 'menjou': 49,\n",
              " 'wnk': 50,\n",
              " 'galleons': 51,\n",
              " 'shallowly': 52,\n",
              " 'rishi': 53,\n",
              " 'venessa': 54,\n",
              " 'cambodian': 55,\n",
              " 'mastodon': 56,\n",
              " 'pretagonist': 57,\n",
              " 'sunlight': 58,\n",
              " 'shunned': 59,\n",
              " 'hightlight': 60,\n",
              " 'rousset': 61,\n",
              " 'kellie': 62,\n",
              " 'banaras': 63,\n",
              " 'twisty': 64,\n",
              " 'screwing': 65,\n",
              " 'drown': 66,\n",
              " 'hysterectomies': 67,\n",
              " 'annis': 68,\n",
              " 'allover': 69,\n",
              " 'nelsan': 70,\n",
              " 'cattivi': 71,\n",
              " 'duff': 72,\n",
              " 'misspelled': 73,\n",
              " 'briefcases': 74,\n",
              " 'tykes': 75,\n",
              " 'pachelbel': 76,\n",
              " 'absurdly': 77,\n",
              " 'pliers': 78,\n",
              " 'naivety': 79,\n",
              " 'prowl': 80,\n",
              " 'mihescu': 81,\n",
              " 'bridgette': 82,\n",
              " 'timothy': 83,\n",
              " 'weren': 84,\n",
              " 'lowpoint': 85,\n",
              " 'mindful': 86,\n",
              " 'kata': 87,\n",
              " 'gauguin': 88,\n",
              " 'calvero': 89,\n",
              " 'ery': 90,\n",
              " 'jobyna': 91,\n",
              " 'xeroxing': 92,\n",
              " 'shutting': 93,\n",
              " 'unachieving': 94,\n",
              " 'snobs': 95,\n",
              " 'intensity': 96,\n",
              " 'ramps': 97,\n",
              " 'gurdebeke': 98,\n",
              " 'cylinder': 99,\n",
              " 'lemmons': 100,\n",
              " 'typecasted': 101,\n",
              " 'doris': 102,\n",
              " 'updike': 103,\n",
              " 'foreknowledge': 104,\n",
              " 'bugs': 105,\n",
              " 'suicidees': 106,\n",
              " 'closups': 107,\n",
              " 'methinks': 108,\n",
              " 'wang': 109,\n",
              " 'ambushers': 110,\n",
              " 'furthermore': 111,\n",
              " 'toppled': 112,\n",
              " 'scuffle': 113,\n",
              " 'interpretations': 114,\n",
              " 'gamorrean': 115,\n",
              " 'unskilled': 116,\n",
              " 'stratten': 117,\n",
              " 'soubrette': 118,\n",
              " 'coghlan': 119,\n",
              " 'carbide': 120,\n",
              " 'fte': 121,\n",
              " 'bajo': 122,\n",
              " 'craps': 123,\n",
              " 'systems': 124,\n",
              " 'pamphleteering': 125,\n",
              " 'katha': 126,\n",
              " 'freighting': 127,\n",
              " 'satellites': 128,\n",
              " 'survivial': 129,\n",
              " 'oz': 130,\n",
              " 'tolerantly': 131,\n",
              " 'stryker': 132,\n",
              " 'unreadable': 133,\n",
              " 'kindle': 134,\n",
              " 'wean': 135,\n",
              " 'vehicular': 136,\n",
              " 'miscasting': 137,\n",
              " 'connely': 138,\n",
              " 'soma': 139,\n",
              " 'mehmood': 140,\n",
              " 'allyn': 141,\n",
              " 'concoct': 142,\n",
              " 'jagger': 143,\n",
              " 'igniminiously': 144,\n",
              " 'caricatured': 145,\n",
              " 'tiomkin': 146,\n",
              " 'taxi': 147,\n",
              " 'antivirus': 148,\n",
              " 'suse': 149,\n",
              " 'waxed': 150,\n",
              " 'visualizing': 151,\n",
              " 'anthropomorphics': 152,\n",
              " 'quint': 153,\n",
              " 'ish': 154,\n",
              " 'valerie': 155,\n",
              " 'booting': 156,\n",
              " 'decorative': 157,\n",
              " 'unilaterally': 158,\n",
              " 'intermingled': 159,\n",
              " 'orleans': 160,\n",
              " 'strombel': 161,\n",
              " 'sledding': 162,\n",
              " 'incorporate': 163,\n",
              " 'proletariat': 164,\n",
              " 'billy': 165,\n",
              " 'welds': 166,\n",
              " 'invulnerable': 167,\n",
              " 'privatizing': 168,\n",
              " 'supernatural': 169,\n",
              " 'hopers': 170,\n",
              " 'cohere': 171,\n",
              " 'dorma': 172,\n",
              " 'maneuverability': 173,\n",
              " 'dell': 174,\n",
              " 'regrets': 175,\n",
              " 'overstating': 176,\n",
              " 'hooper': 177,\n",
              " 'centenary': 178,\n",
              " 'denote': 179,\n",
              " 'msg': 180,\n",
              " 'madre': 181,\n",
              " 'rahs': 182,\n",
              " 'plasticized': 183,\n",
              " 'sacco': 184,\n",
              " 'surperb': 185,\n",
              " 'stowaway': 186,\n",
              " 'render': 187,\n",
              " 'noirometer': 188,\n",
              " 'jells': 189,\n",
              " 'bunnies': 190,\n",
              " 'ec': 191,\n",
              " 'mirror': 192,\n",
              " 'reactivated': 193,\n",
              " 'zinnemann': 194,\n",
              " 'darlian': 195,\n",
              " 'international': 196,\n",
              " 'holocaust': 197,\n",
              " 'battling': 198,\n",
              " 'prune': 199,\n",
              " 'deduction': 200,\n",
              " 'strenghtens': 201,\n",
              " 'dumps': 202,\n",
              " 'vivienne': 203,\n",
              " 'nourishing': 204,\n",
              " 'cluny': 205,\n",
              " 'grasped': 206,\n",
              " 'preposterous': 207,\n",
              " 'arends': 208,\n",
              " 'hema': 209,\n",
              " 'tomatoey': 210,\n",
              " 'dilatory': 211,\n",
              " 'detected': 212,\n",
              " 'swansong': 213,\n",
              " 'pillows': 214,\n",
              " 'bulbous': 215,\n",
              " 'homogeneous': 216,\n",
              " 'spanking': 217,\n",
              " 'timeslip': 218,\n",
              " 'bilgewater': 219,\n",
              " 'spots': 220,\n",
              " 'revisionism': 221,\n",
              " 'booz': 222,\n",
              " 'cumulative': 223,\n",
              " 'spotlights': 224,\n",
              " 'sonatine': 225,\n",
              " 'deciphered': 226,\n",
              " 'natsu': 227,\n",
              " 'invalidate': 228,\n",
              " 'settings': 229,\n",
              " 'classical': 230,\n",
              " 'stepmother': 231,\n",
              " 'gosh': 232,\n",
              " 'stien': 233,\n",
              " 'treading': 234,\n",
              " 'cyher': 235,\n",
              " 'figtings': 236,\n",
              " 'ludlow': 237,\n",
              " 'tia': 238,\n",
              " 'yvan': 239,\n",
              " 'harron': 240,\n",
              " 'sidewalk': 241,\n",
              " 'damen': 242,\n",
              " 'confusathon': 243,\n",
              " 'cline': 244,\n",
              " 'jacks': 245,\n",
              " 'durians': 246,\n",
              " 'edgerton': 247,\n",
              " 'renovation': 248,\n",
              " 'shaheen': 249,\n",
              " 'cherubs': 250,\n",
              " 'explosion': 251,\n",
              " 'delinquents': 252,\n",
              " 'forged': 253,\n",
              " 'krebs': 254,\n",
              " 'laugher': 255,\n",
              " 'shakespear': 256,\n",
              " 'flixmedia': 257,\n",
              " 'fobby': 258,\n",
              " 'progressional': 259,\n",
              " 'aslan': 260,\n",
              " 'nods': 261,\n",
              " 'blueprint': 262,\n",
              " 'novelle': 263,\n",
              " 'naziism': 264,\n",
              " 'skippy': 265,\n",
              " 'whitworth': 266,\n",
              " 'purnell': 267,\n",
              " 'battles': 268,\n",
              " 'inflections': 269,\n",
              " 'gasped': 270,\n",
              " 'costumer': 271,\n",
              " 'truehart': 272,\n",
              " 'antic': 273,\n",
              " 'abundant': 274,\n",
              " 'eulogized': 275,\n",
              " 'slalom': 276,\n",
              " 'substantiates': 277,\n",
              " 'flavors': 278,\n",
              " 'motorists': 279,\n",
              " 'elizabethtown': 280,\n",
              " 'sachetti': 281,\n",
              " 'safety': 282,\n",
              " 'breaking': 283,\n",
              " 'riget': 284,\n",
              " 'dispassionate': 285,\n",
              " 'dvds': 286,\n",
              " 'walthal': 287,\n",
              " 'wtn': 288,\n",
              " 'poil': 289,\n",
              " 'oks': 290,\n",
              " 'graves': 291,\n",
              " 'badnam': 292,\n",
              " 'boerner': 293,\n",
              " 'callarn': 294,\n",
              " 'hallelujah': 295,\n",
              " 'fuflo': 296,\n",
              " 'enthusiast': 297,\n",
              " 'blubbering': 298,\n",
              " 'scanners': 299,\n",
              " 'informant': 300,\n",
              " 'dullllllllllll': 301,\n",
              " 'blooming': 302,\n",
              " 'ivana': 303,\n",
              " 'ernie': 304,\n",
              " 'bowery': 305,\n",
              " 'nouns': 306,\n",
              " 'unlovable': 307,\n",
              " 'flashman': 308,\n",
              " 'colom': 309,\n",
              " 'somersault': 310,\n",
              " 'opportunists': 311,\n",
              " 'tapped': 312,\n",
              " 'invent': 313,\n",
              " 'cotangent': 314,\n",
              " 'bullshot': 315,\n",
              " 'relentlessy': 316,\n",
              " 'videotaping': 317,\n",
              " 'binoche': 318,\n",
              " 'shuffling': 319,\n",
              " 'hitters': 320,\n",
              " 'chale': 321,\n",
              " 'adele': 322,\n",
              " 'jackass': 323,\n",
              " 'recapitulate': 324,\n",
              " 'climes': 325,\n",
              " 'conway': 326,\n",
              " 'karenina': 327,\n",
              " 'ligabue': 328,\n",
              " 'sufi': 329,\n",
              " 'dogsbody': 330,\n",
              " 'sum': 331,\n",
              " 'lenghtened': 332,\n",
              " 'deadens': 333,\n",
              " 'namesake': 334,\n",
              " 'redeemed': 335,\n",
              " 'clerics': 336,\n",
              " 'sprang': 337,\n",
              " 'lusted': 338,\n",
              " 'davidlynch': 339,\n",
              " 'merit': 340,\n",
              " 'tightly': 341,\n",
              " 'puckish': 342,\n",
              " 'analytics': 343,\n",
              " 'hippos': 344,\n",
              " 'eyelashes': 345,\n",
              " 'ksc': 346,\n",
              " 'stairsteps': 347,\n",
              " 'nastier': 348,\n",
              " 'hairdewed': 349,\n",
              " 'odes': 350,\n",
              " 'tattered': 351,\n",
              " 'spoilerokay': 352,\n",
              " 'byner': 353,\n",
              " 'honhyol': 354,\n",
              " 'incongruous': 355,\n",
              " 'probable': 356,\n",
              " 'scalded': 357,\n",
              " 'hee': 358,\n",
              " 'majd': 359,\n",
              " 'several': 360,\n",
              " 'blossoming': 361,\n",
              " 'barres': 362,\n",
              " 'greets': 363,\n",
              " 'fictionalizations': 364,\n",
              " 'wayback': 365,\n",
              " 'proba': 366,\n",
              " 'cornflakes': 367,\n",
              " 'lim': 368,\n",
              " 'whined': 369,\n",
              " 'mickie': 370,\n",
              " 'malcom': 371,\n",
              " 'rethink': 372,\n",
              " 'singles': 373,\n",
              " 'jarre': 374,\n",
              " 'current': 375,\n",
              " 'vertebrae': 376,\n",
              " 'pounded': 377,\n",
              " 'sittings': 378,\n",
              " 'vouched': 379,\n",
              " 'whisperish': 380,\n",
              " 'nomad': 381,\n",
              " 'parenthood': 382,\n",
              " 'puzzle': 383,\n",
              " 'lightpost': 384,\n",
              " 'hare': 385,\n",
              " 'conspirital': 386,\n",
              " 'thoe': 387,\n",
              " 'ooze': 388,\n",
              " 'tempo': 389,\n",
              " 'hitchhiker': 390,\n",
              " 'turret': 391,\n",
              " 'deschanel': 392,\n",
              " 'existentialist': 393,\n",
              " 'contortions': 394,\n",
              " 'dereliction': 395,\n",
              " 'imposition': 396,\n",
              " 'bernarda': 397,\n",
              " 'lugosi': 398,\n",
              " 'receive': 399,\n",
              " 'nastiness': 400,\n",
              " 'jordana': 401,\n",
              " 'carson': 402,\n",
              " 'premedical': 403,\n",
              " 'timonn': 404,\n",
              " 'ropey': 405,\n",
              " 'superbrains': 406,\n",
              " 'chips': 407,\n",
              " 'shetan': 408,\n",
              " 'slowing': 409,\n",
              " 'practised': 410,\n",
              " 'orry': 411,\n",
              " 'regularity': 412,\n",
              " 'insult': 413,\n",
              " 'marchand': 414,\n",
              " 'newtypes': 415,\n",
              " 'gamekeeper': 416,\n",
              " 'konigin': 417,\n",
              " 'stroh': 418,\n",
              " 'airport': 419,\n",
              " 'ghatak': 420,\n",
              " 'fumblingly': 421,\n",
              " 'kisses': 422,\n",
              " 'smker': 423,\n",
              " 'parcel': 424,\n",
              " 'blown': 425,\n",
              " 'hessians': 426,\n",
              " 'outriders': 427,\n",
              " 'accommodations': 428,\n",
              " 'sec': 429,\n",
              " 'struggled': 430,\n",
              " 'nebula': 431,\n",
              " 'hasten': 432,\n",
              " 'columbusland': 433,\n",
              " 'enforce': 434,\n",
              " 'manic': 435,\n",
              " 'khali': 436,\n",
              " 'cockfighting': 437,\n",
              " 'charly': 438,\n",
              " 'milkman': 439,\n",
              " 'reincarnated': 440,\n",
              " 'suave': 441,\n",
              " 'servicemen': 442,\n",
              " 'tramps': 443,\n",
              " 'clint': 444,\n",
              " 'endowed': 445,\n",
              " 'hatosy': 446,\n",
              " 'wink': 447,\n",
              " 'pelletier': 448,\n",
              " 'entardecer': 449,\n",
              " 'eagle': 450,\n",
              " 'amsterdam': 451,\n",
              " 'bills': 452,\n",
              " 'ikeda': 453,\n",
              " 'dramatically': 454,\n",
              " 'pnico': 455,\n",
              " 'truncheons': 456,\n",
              " 'apostrophe': 457,\n",
              " 'overcompensate': 458,\n",
              " 'fucky': 459,\n",
              " 'ebsen': 460,\n",
              " 'pugh': 461,\n",
              " 'porel': 462,\n",
              " 'ranikhet': 463,\n",
              " 'isildur': 464,\n",
              " 'acedemy': 465,\n",
              " 'afar': 466,\n",
              " 'daze': 467,\n",
              " 'unconventional': 468,\n",
              " 'oboe': 469,\n",
              " 'gazongas': 470,\n",
              " 'casualties': 471,\n",
              " 'painters': 472,\n",
              " 'merde': 473,\n",
              " 'raise': 474,\n",
              " 'adopter': 475,\n",
              " 'ingratiate': 476,\n",
              " 'tint': 477,\n",
              " 'yul': 478,\n",
              " 'brights': 479,\n",
              " 'sukumari': 480,\n",
              " 'vii': 481,\n",
              " 'fenced': 482,\n",
              " 'discontinued': 483,\n",
              " 'overplay': 484,\n",
              " 'wilnona': 485,\n",
              " 'gloomily': 486,\n",
              " 'holcomb': 487,\n",
              " 'war': 488,\n",
              " 'damaris': 489,\n",
              " 'cheaten': 490,\n",
              " 'apartments': 491,\n",
              " 'spoons': 492,\n",
              " 'dillion': 493,\n",
              " 'preventing': 494,\n",
              " 'badness': 495,\n",
              " 'balan': 496,\n",
              " 'melnik': 497,\n",
              " 'cockamamie': 498,\n",
              " 'treck': 499,\n",
              " 'smallville': 500,\n",
              " 'cobbling': 501,\n",
              " 'pomade': 502,\n",
              " 'rigoberta': 503,\n",
              " 'rollerblades': 504,\n",
              " 'meanest': 505,\n",
              " 'maelstrm': 506,\n",
              " 'auroras': 507,\n",
              " 'disheartened': 508,\n",
              " 'condemnatory': 509,\n",
              " 'hujan': 510,\n",
              " 'anynomous': 511,\n",
              " 'pelicule': 512,\n",
              " 'astros': 513,\n",
              " 'spinell': 514,\n",
              " 'unstoppably': 515,\n",
              " 'slumps': 516,\n",
              " 'johny': 517,\n",
              " 'apidistra': 518,\n",
              " 'chan': 519,\n",
              " 'halal': 520,\n",
              " 'batonzilla': 521,\n",
              " 'forties': 522,\n",
              " 'poverty': 523,\n",
              " 'meshes': 524,\n",
              " 'soval': 525,\n",
              " 'divulging': 526,\n",
              " 'rossilini': 527,\n",
              " 'otami': 528,\n",
              " 'benson': 529,\n",
              " 'thundering': 530,\n",
              " 'verona': 531,\n",
              " 'uncoventional': 532,\n",
              " 'citt': 533,\n",
              " 'eurovision': 534,\n",
              " 'slaver': 535,\n",
              " 'rainbows': 536,\n",
              " 'chugs': 537,\n",
              " 'adjacent': 538,\n",
              " 'terri': 539,\n",
              " 'hercule': 540,\n",
              " 'precision': 541,\n",
              " 'expansion': 542,\n",
              " 'jell': 543,\n",
              " 'kyser': 544,\n",
              " 'palace': 545,\n",
              " 'gaffe': 546,\n",
              " 'icegun': 547,\n",
              " 'laroque': 548,\n",
              " 'squadroom': 549,\n",
              " 'remorseless': 550,\n",
              " 'akras': 551,\n",
              " 'disrepair': 552,\n",
              " 'afghanistan': 553,\n",
              " 'solitude': 554,\n",
              " 'ebonics': 555,\n",
              " 'thursdays': 556,\n",
              " 'gourgous': 557,\n",
              " 'jospeh': 558,\n",
              " 'chowdhry': 559,\n",
              " 'hamilton': 560,\n",
              " 'oradour': 561,\n",
              " 'oxide': 562,\n",
              " 'prototypical': 563,\n",
              " 'obscene': 564,\n",
              " 'bumbles': 565,\n",
              " 'hulk': 566,\n",
              " 'eleonora': 567,\n",
              " 'puertorican': 568,\n",
              " 'breastfeeding': 569,\n",
              " 'burnett': 570,\n",
              " 'unbielevable': 571,\n",
              " 'licitates': 572,\n",
              " 'willingness': 573,\n",
              " 'millican': 574,\n",
              " 'toilets': 575,\n",
              " 'schematically': 576,\n",
              " 'auditions': 577,\n",
              " 'dominicana': 578,\n",
              " 'wail': 579,\n",
              " 'dinky': 580,\n",
              " 'creams': 581,\n",
              " 'purist': 582,\n",
              " 'vholes': 583,\n",
              " 'byrosanne': 584,\n",
              " 'fogbound': 585,\n",
              " 'done': 586,\n",
              " 'defensa': 587,\n",
              " 'domicile': 588,\n",
              " 'riped': 589,\n",
              " 'shunji': 590,\n",
              " 'fillers': 591,\n",
              " 'protecting': 592,\n",
              " 'candlelit': 593,\n",
              " 'banyo': 594,\n",
              " 'sanctimonious': 595,\n",
              " 'fuzzies': 596,\n",
              " 'gollam': 597,\n",
              " 'foam': 598,\n",
              " 'abstain': 599,\n",
              " 'preachiest': 600,\n",
              " 'tech': 601,\n",
              " 'wayyyyy': 602,\n",
              " 'ubaldo': 603,\n",
              " 'chancers': 604,\n",
              " 'vw': 605,\n",
              " 'amusements': 606,\n",
              " 'idea': 607,\n",
              " 'spirituality': 608,\n",
              " 'noisier': 609,\n",
              " 'harlow': 610,\n",
              " 'bedding': 611,\n",
              " 'hugwagon': 612,\n",
              " 'eriko': 613,\n",
              " 'trade': 614,\n",
              " 'thundercloud': 615,\n",
              " 'oozed': 616,\n",
              " 'branded': 617,\n",
              " 'mustn': 618,\n",
              " 'unmoored': 619,\n",
              " 'detecting': 620,\n",
              " 'subtleness': 621,\n",
              " 'roger': 622,\n",
              " 'wiseman': 623,\n",
              " 'fried': 624,\n",
              " 'renters': 625,\n",
              " 'testators': 626,\n",
              " 'stanis': 627,\n",
              " 'gutsy': 628,\n",
              " 'survey': 629,\n",
              " 'litreture': 630,\n",
              " 'lighting': 631,\n",
              " 'thirsting': 632,\n",
              " 'chuckleheads': 633,\n",
              " 'feminine': 634,\n",
              " 'hereon': 635,\n",
              " 'clairvoyant': 636,\n",
              " 'whistler': 637,\n",
              " 'symptom': 638,\n",
              " 'honeymooners': 639,\n",
              " 'heah': 640,\n",
              " 'dolenz': 641,\n",
              " 'ad': 642,\n",
              " 'highs': 643,\n",
              " 'jaubert': 644,\n",
              " 'paparazzi': 645,\n",
              " 'pars': 646,\n",
              " 'maelstrom': 647,\n",
              " 'historyish': 648,\n",
              " 'orphanage': 649,\n",
              " 'hush': 650,\n",
              " 'secede': 651,\n",
              " 'jeannie': 652,\n",
              " 'jog': 653,\n",
              " 'pupi': 654,\n",
              " 'kou': 655,\n",
              " 'repubhlic': 656,\n",
              " 'lifesaver': 657,\n",
              " 'jolt': 658,\n",
              " 'vividly': 659,\n",
              " 'whiny': 660,\n",
              " 'denominator': 661,\n",
              " 'chet': 662,\n",
              " 'unforced': 663,\n",
              " 'fumbler': 664,\n",
              " 'encircle': 665,\n",
              " 'thingie': 666,\n",
              " 'oder': 667,\n",
              " 'evanescence': 668,\n",
              " 'quinnn': 669,\n",
              " 'campsites': 670,\n",
              " 'randomized': 671,\n",
              " 'antwortet': 672,\n",
              " 'backward': 673,\n",
              " 'electricians': 674,\n",
              " 'mowbray': 675,\n",
              " 'warpath': 676,\n",
              " 'trustee': 677,\n",
              " 'flaunted': 678,\n",
              " 'starand': 679,\n",
              " 'gawi': 680,\n",
              " 'ziv': 681,\n",
              " 'communications': 682,\n",
              " 'billiard': 683,\n",
              " 'creaky': 684,\n",
              " 'reels': 685,\n",
              " 'seducer': 686,\n",
              " 'cinematheque': 687,\n",
              " 'flattest': 688,\n",
              " 'feelgood': 689,\n",
              " 'unpalatable': 690,\n",
              " 'decaune': 691,\n",
              " 'intricacy': 692,\n",
              " 'linclon': 693,\n",
              " 'pojar': 694,\n",
              " 'jumper': 695,\n",
              " 'pouter': 696,\n",
              " 'qc': 697,\n",
              " 'detectable': 698,\n",
              " 'inexplicable': 699,\n",
              " 'prosthetics': 700,\n",
              " 'sweary': 701,\n",
              " 'countrydifferent': 702,\n",
              " 'bihari': 703,\n",
              " 'nightie': 704,\n",
              " 'fanatstic': 705,\n",
              " 'loosed': 706,\n",
              " 'ants': 707,\n",
              " 'maughm': 708,\n",
              " 'induction': 709,\n",
              " 'alselmo': 710,\n",
              " 'horrorvision': 711,\n",
              " 'puddy': 712,\n",
              " 'shiko': 713,\n",
              " 'thadblog': 714,\n",
              " 'clocks': 715,\n",
              " 'alphabet': 716,\n",
              " 'feasted': 717,\n",
              " 'deacon': 718,\n",
              " 'persbrandt': 719,\n",
              " 'flubber': 720,\n",
              " 'product': 721,\n",
              " 'vrits': 722,\n",
              " 'fascism': 723,\n",
              " 'recoup': 724,\n",
              " 'bused': 725,\n",
              " 'offer': 726,\n",
              " 'denounces': 727,\n",
              " 'frf': 728,\n",
              " 'coats': 729,\n",
              " 'moragn': 730,\n",
              " 'marton': 731,\n",
              " 'malade': 732,\n",
              " 'burtis': 733,\n",
              " 'odd': 734,\n",
              " 'willful': 735,\n",
              " 'guarontee': 736,\n",
              " 'sipping': 737,\n",
              " 'authorlittlehammer': 738,\n",
              " 'conspirators': 739,\n",
              " 'fairview': 740,\n",
              " 'silas': 741,\n",
              " 'drury': 742,\n",
              " 'rages': 743,\n",
              " 'crosses': 744,\n",
              " 'altioklar': 745,\n",
              " 'leeway': 746,\n",
              " 'wobbly': 747,\n",
              " 'mcnamara': 748,\n",
              " 'strauli': 749,\n",
              " 'ungifted': 750,\n",
              " 'preteens': 751,\n",
              " 'protractor': 752,\n",
              " 'cloth': 753,\n",
              " 'pellet': 754,\n",
              " 'roa': 755,\n",
              " 'hollyweird': 756,\n",
              " 'skimpier': 757,\n",
              " 'fanny': 758,\n",
              " 'hallucinogens': 759,\n",
              " 'intonation': 760,\n",
              " 'scrambling': 761,\n",
              " 'yum': 762,\n",
              " 'se': 763,\n",
              " 'chapterplays': 764,\n",
              " 'downtrodden': 765,\n",
              " 'parlablane': 766,\n",
              " 'gryphons': 767,\n",
              " 'raid': 768,\n",
              " 'wanky': 769,\n",
              " 'fraternization': 770,\n",
              " 'splat': 771,\n",
              " 'corroboration': 772,\n",
              " 'soros': 773,\n",
              " 'diary': 774,\n",
              " 'boastful': 775,\n",
              " 'univeristy': 776,\n",
              " 'autumnal': 777,\n",
              " 'vadim': 778,\n",
              " 'generalities': 779,\n",
              " 'mediatic': 780,\n",
              " 'bhaer': 781,\n",
              " 'airtime': 782,\n",
              " 'lumpy': 783,\n",
              " 'masacism': 784,\n",
              " 'lamplit': 785,\n",
              " 'elsewere': 786,\n",
              " 'vegetarians': 787,\n",
              " 'secretery': 788,\n",
              " 'jt': 789,\n",
              " 'kara': 790,\n",
              " 'brunettes': 791,\n",
              " 'inlcuded': 792,\n",
              " 'doobie': 793,\n",
              " 'luftens': 794,\n",
              " 'squalor': 795,\n",
              " 'stranded': 796,\n",
              " 'administrators': 797,\n",
              " 'convenient': 798,\n",
              " 'blowup': 799,\n",
              " 'hartmann': 800,\n",
              " 'arngrim': 801,\n",
              " 'misleadingly': 802,\n",
              " 'stonewall': 803,\n",
              " 'kinkle': 804,\n",
              " 'blaringly': 805,\n",
              " 'ludwing': 806,\n",
              " 'exhilaration': 807,\n",
              " 'swimmers': 808,\n",
              " 'intensional': 809,\n",
              " 'dazzy': 810,\n",
              " 'bengal': 811,\n",
              " 'dolled': 812,\n",
              " 'arrondissement': 813,\n",
              " 'peckenpah': 814,\n",
              " 'caspers': 815,\n",
              " 'deathmatch': 816,\n",
              " 'attentiontisserand': 817,\n",
              " 'plod': 818,\n",
              " 'techicolor': 819,\n",
              " 'dullea': 820,\n",
              " 'coles': 821,\n",
              " 'multiple': 822,\n",
              " 'transvestite': 823,\n",
              " 'ific': 824,\n",
              " 'performative': 825,\n",
              " 'obv': 826,\n",
              " 'joxs': 827,\n",
              " 'stymie': 828,\n",
              " 'iberia': 829,\n",
              " 'huffing': 830,\n",
              " 'muddah': 831,\n",
              " 'californian': 832,\n",
              " 'regents': 833,\n",
              " 'icb': 834,\n",
              " 'ricky': 835,\n",
              " 'velma': 836,\n",
              " 'ruler': 837,\n",
              " 'yowsa': 838,\n",
              " 'postmodern': 839,\n",
              " 'wretchedness': 840,\n",
              " 'tobin': 841,\n",
              " 'cements': 842,\n",
              " 'biron': 843,\n",
              " 'mains': 844,\n",
              " 'chores': 845,\n",
              " 'flavin': 846,\n",
              " 'eardrums': 847,\n",
              " 'pivotal': 848,\n",
              " 'ruled': 849,\n",
              " 'macabre': 850,\n",
              " 'solange': 851,\n",
              " 'sycophantically': 852,\n",
              " 'strangle': 853,\n",
              " 'unescapable': 854,\n",
              " 'screenplays': 855,\n",
              " 'ebony': 856,\n",
              " 'candle': 857,\n",
              " 'swathes': 858,\n",
              " 'teff': 859,\n",
              " 'eleanore': 860,\n",
              " 'slingblade': 861,\n",
              " 'leaud': 862,\n",
              " 'corrugated': 863,\n",
              " 'aa': 864,\n",
              " 'ry': 865,\n",
              " 'zasu': 866,\n",
              " 'yukfest': 867,\n",
              " 'margaux': 868,\n",
              " 'trelkovski': 869,\n",
              " 'dansu': 870,\n",
              " 'supervised': 871,\n",
              " 'posest': 872,\n",
              " 'torturous': 873,\n",
              " 'mambo': 874,\n",
              " 'exudes': 875,\n",
              " 'orifices': 876,\n",
              " 'pollack': 877,\n",
              " 'sails': 878,\n",
              " 'brims': 879,\n",
              " 'acquitted': 880,\n",
              " 'tushes': 881,\n",
              " 'solves': 882,\n",
              " 'slavery': 883,\n",
              " 'reuniting': 884,\n",
              " 'cucumbers': 885,\n",
              " 'beetleborgs': 886,\n",
              " 'splits': 887,\n",
              " 'malkovich': 888,\n",
              " 'conceded': 889,\n",
              " 'cowlishaw': 890,\n",
              " 'subtler': 891,\n",
              " 'skin': 892,\n",
              " 'shameometer': 893,\n",
              " 'bourne': 894,\n",
              " 'markel': 895,\n",
              " 'wasting': 896,\n",
              " 'newcomer': 897,\n",
              " 'archaeologically': 898,\n",
              " 'rayguns': 899,\n",
              " 'phobic': 900,\n",
              " 'crampton': 901,\n",
              " 'yochobel': 902,\n",
              " 'quieter': 903,\n",
              " 'lonely': 904,\n",
              " 'warlocks': 905,\n",
              " 'irked': 906,\n",
              " 'realllllllly': 907,\n",
              " 'bagpipes': 908,\n",
              " 'romany': 909,\n",
              " 'suffering': 910,\n",
              " 'katt': 911,\n",
              " 'carrefour': 912,\n",
              " 'iisimba': 913,\n",
              " 'pressings': 914,\n",
              " 'strickland': 915,\n",
              " 'dames': 916,\n",
              " 'parroting': 917,\n",
              " 'leads': 918,\n",
              " 'celine': 919,\n",
              " 'tete': 920,\n",
              " 'kit': 921,\n",
              " 'replacements': 922,\n",
              " 'sciusci': 923,\n",
              " 'luva': 924,\n",
              " 'deen': 925,\n",
              " 'ciccolina': 926,\n",
              " 'ifit': 927,\n",
              " 'wideboy': 928,\n",
              " 'broaching': 929,\n",
              " 'sighted': 930,\n",
              " 'compulsive': 931,\n",
              " 'unschooled': 932,\n",
              " 'mohammedan': 933,\n",
              " 'executors': 934,\n",
              " 'churning': 935,\n",
              " 'ominous': 936,\n",
              " 'vibe': 937,\n",
              " 'lenses': 938,\n",
              " 'bolster': 939,\n",
              " 'poitier': 940,\n",
              " 'everrrryone': 941,\n",
              " 'spaz': 942,\n",
              " 'lbp': 943,\n",
              " 'beaut': 944,\n",
              " 'shatta': 945,\n",
              " 'kyle': 946,\n",
              " 'sweetness': 947,\n",
              " 'leeson': 948,\n",
              " 'egyptologistic': 949,\n",
              " 'tudors': 950,\n",
              " 'hig': 951,\n",
              " 'catcalls': 952,\n",
              " 'juicier': 953,\n",
              " 'liquid': 954,\n",
              " 'arvanitis': 955,\n",
              " 'potemkin': 956,\n",
              " 'chokeslamming': 957,\n",
              " 'crooner': 958,\n",
              " 'ingesting': 959,\n",
              " 'implicitly': 960,\n",
              " 'nayland': 961,\n",
              " 'corona': 962,\n",
              " 'spellcasting': 963,\n",
              " 'reconnaissance': 964,\n",
              " 'holywell': 965,\n",
              " 'onibaba': 966,\n",
              " 'ione': 967,\n",
              " 'charecters': 968,\n",
              " 'cartwrightbrideyahoo': 969,\n",
              " 'germans': 970,\n",
              " 'energized': 971,\n",
              " 'nooooooo': 972,\n",
              " 'essanay': 973,\n",
              " 'expiration': 974,\n",
              " 'traders': 975,\n",
              " 'brassieres': 976,\n",
              " 'anglophobe': 977,\n",
              " 'endnote': 978,\n",
              " 'men': 979,\n",
              " 'fkin': 980,\n",
              " 'spooning': 981,\n",
              " 'carve': 982,\n",
              " 'absoutley': 983,\n",
              " 'traverse': 984,\n",
              " 'dawg': 985,\n",
              " 'sren': 986,\n",
              " 'arzner': 987,\n",
              " 'gigi': 988,\n",
              " 'lit': 989,\n",
              " 'scot': 990,\n",
              " 'draaaaaaaawl': 991,\n",
              " 'overwhlelming': 992,\n",
              " 'ensued': 993,\n",
              " 'groom': 994,\n",
              " 'verit': 995,\n",
              " 'metamorphosing': 996,\n",
              " 'santoshi': 997,\n",
              " 'capitalistic': 998,\n",
              " 'amphlett': 999,\n",
              " ...}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAtkzlaUU62F",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "`update_input_layer` é uma função que conta quantas vezes cada palavra é usada em uma review e guarda essa contagem nos índices apropriados dentro de `layer_0`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2W7ovqNU62G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_input_layer(review):\n",
        "    global layer_0\n",
        "    \n",
        "    # limpamos o estado anterior, resetando tudo para 0.\n",
        "    layer_0 *= 0\n",
        "    \n",
        "    # contamos quantas vezes cada palavra é utilizada na review e armazena os resultados em layer_0 \n",
        "    for word in review.split(' '):\n",
        "        layer_0[0][word2index[word]] += 1"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0keg-2n0U62M",
        "colab_type": "text"
      },
      "source": [
        "A próxima célula atualiza a camada de entrada com a primeira review."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWchsYCuU62O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f2fbd23a-e715-4164-8394-c4fdea3f62de"
      },
      "source": [
        "update_input_layer(reviews[6])\n",
        "layer_0"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[15.,  0.,  0., ...,  0.,  0.,  0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsnDhfkoU62W",
        "colab_type": "text"
      },
      "source": [
        "`get_target_for_labels` retorna `0` ou `1`, para reviews `NEGATIVE` ou `POSITIVE`, respectivamente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dt6ZqojZU62X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_target_for_label(label):\n",
        "    if(label == 'POSITIVE'):\n",
        "        return 1\n",
        "    else:\n",
        "        return 0"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nl6ea2QLU62d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2e685c54-85dd-4c5a-d276-4f9b3c44deb6"
      },
      "source": [
        "labels[0]"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'POSITIVE'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGCrMmdTU62i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "558e854d-6f68-46c8-f846-40e5d2949101"
      },
      "source": [
        "get_target_for_label(labels[0])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPcOekWeU62n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "aef5b994-9945-4aca-8132-636e78889dd7"
      },
      "source": [
        "labels[1]"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'NEGATIVE'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y45CblDeU62t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "733929da-355e-4ac1-ae3e-4f0b7cd49752"
      },
      "source": [
        "get_target_for_label(labels[1])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIXPdRHLU620",
        "colab_type": "text"
      },
      "source": [
        "# Fim do Projeto 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oiTFImQU622",
        "colab_type": "text"
      },
      "source": [
        "# Projeto 3: Construindo Nossa Rede Neural<a id='project_3'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjPHmgkVU623",
        "colab_type": "text"
      },
      "source": [
        "Iremos construir uma rede neural sem o uso de qualquer framework de ML. Iremos implementar tudo na mão.\n",
        "\n",
        "Entretanto, não se preocupe aqui. O objetivo não é que você entenda a implementação de uma rede neural, apenas que possamos a utilizar. Você terá a oportunidade de montar essa rede utilizando PyTorch mais para frente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdwhiNZvU624",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "class SentimentNetwork:\n",
        "    def __init__(self, reviews, labels, hidden_nodes=10, learning_rate=0.1):\n",
        "        # para que possamos ter sempre os mesmos resultados, iremos manter\n",
        "        # a inicialização aleatória como sendo fixa\n",
        "        np.random.seed(1)\n",
        "\n",
        "        self.pre_process_data(reviews, labels)\n",
        "        \n",
        "        self.init_network(len(self.review_vocab),hidden_nodes, 1, learning_rate)\n",
        "\n",
        "    def pre_process_data(self, reviews, labels):\n",
        "        # populando review_vocab com as palavras da review\n",
        "        review_vocab = set()\n",
        "        for review in reviews:\n",
        "            for word in review.split(' '):\n",
        "                review_vocab.add(word)\n",
        "\n",
        "        # convertendo para uma lista para que possamos acessar as palavras por seus índices\n",
        "        self.review_vocab = list(review_vocab)\n",
        "        \n",
        "        # populando label_vocab com todas as palavras nas labels\n",
        "        label_vocab = set()\n",
        "        for label in labels:\n",
        "            label_vocab.add(label)\n",
        "        \n",
        "        # convertendo em lista para acessar por índice\n",
        "        self.label_vocab = list(label_vocab)\n",
        "        \n",
        "        self.review_vocab_size = len(self.review_vocab)\n",
        "        self.label_vocab_size = len(self.label_vocab)\n",
        "        \n",
        "        # criando um dicionário das palavras no vocabulário mapeadas para índices\n",
        "        self.word2index = {}\n",
        "        for i, word in enumerate(self.review_vocab):\n",
        "            self.word2index[word] = i\n",
        "        \n",
        "        # criando um dicionário de labels mapeadas em índices\n",
        "        self.label2index = {}\n",
        "        for i, label in enumerate(self.label_vocab):\n",
        "            self.label2index[label] = i\n",
        "        \n",
        "    def init_network(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
        "        self.input_nodes = input_nodes\n",
        "        self.hidden_nodes = hidden_nodes\n",
        "        self.output_nodes = output_nodes\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.weights_0_1 = np.zeros((self.input_nodes,self.hidden_nodes))\n",
        "    \n",
        "        self.weights_1_2 = np.random.normal(0.0, self.hidden_nodes**-0.5, \n",
        "                                                (self.hidden_nodes, self.output_nodes))\n",
        "        \n",
        "        self.layer_0 = np.zeros((1,input_nodes))\n",
        "    \n",
        "    def update_input_layer(self,review):\n",
        "        self.layer_0 *= 0\n",
        "        \n",
        "        for word in review.split(\" \"):\n",
        "            if(word in self.word2index.keys()):\n",
        "                self.layer_0[0][self.word2index[word]] += 1\n",
        "                \n",
        "    def get_target_for_label(self,label):\n",
        "        if(label == 'POSITIVE'):\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "        \n",
        "    def sigmoid(self,x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "    \n",
        "    def sigmoid_output_2_derivative(self,output):\n",
        "        return output * (1 - output)\n",
        "    \n",
        "    def train(self, training_reviews, training_labels):\n",
        "        assert(len(training_reviews) == len(training_labels))\n",
        "        \n",
        "        correct_so_far = 0\n",
        "\n",
        "        start = time.time()\n",
        "        \n",
        "        for i in range(len(training_reviews)):\n",
        "            review = training_reviews[i]\n",
        "            label = training_labels[i]\n",
        "\n",
        "            ### Forward pass ###\n",
        "\n",
        "            # Input Layer\n",
        "            self.update_input_layer(review)\n",
        "\n",
        "            # Hidden layer\n",
        "            layer_1 = self.layer_0.dot(self.weights_0_1)\n",
        "\n",
        "            # Output layer\n",
        "            layer_2 = self.sigmoid(layer_1.dot(self.weights_1_2))\n",
        "            \n",
        "            ### Backward pass ###\n",
        "\n",
        "            # Output error\n",
        "            layer_2_error = layer_2 - self.get_target_for_label(label) # Output layer error is the difference between desired target and actual output.\n",
        "            layer_2_delta = layer_2_error * self.sigmoid_output_2_derivative(layer_2)\n",
        "\n",
        "            # Backpropagated error\n",
        "            layer_1_error = layer_2_delta.dot(self.weights_1_2.T) # errors propagated to the hidden layer\n",
        "            layer_1_delta = layer_1_error # hidden layer gradients - no nonlinearity so it's the same as the error\n",
        "\n",
        "            # Update the weights\n",
        "            self.weights_1_2 -= layer_1.T.dot(layer_2_delta) * self.learning_rate # update hidden-to-output weights with gradient descent step\n",
        "            self.weights_0_1 -= self.layer_0.T.dot(layer_1_delta) * self.learning_rate # update input-to-hidden weights with gradient descent step\n",
        "\n",
        "            # Keep track of correct predictions.\n",
        "            if(layer_2 >= 0.5 and label == 'POSITIVE'):\n",
        "                correct_so_far += 1\n",
        "            elif(layer_2 < 0.5 and label == 'NEGATIVE'):\n",
        "                correct_so_far += 1\n",
        "            \n",
        "            elapsed_time = float(time.time() - start)\n",
        "            reviews_per_second = i / elapsed_time if elapsed_time > 0 else 0\n",
        "            \n",
        "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(training_reviews)))[:4] \\\n",
        "                             + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] \\\n",
        "                             + \" #Correct:\" + str(correct_so_far) + \" #Trained:\" + str(i+1) \\\n",
        "                             + \" Training Accuracy:\" + str(correct_so_far * 100 / float(i+1))[:4] + \"%\")\n",
        "            if(i % 2500 == 0):\n",
        "                print(\"\")\n",
        "    \n",
        "    def test(self, testing_reviews, testing_labels):\n",
        "        # keep track of how many correct predictions we make\n",
        "        correct = 0\n",
        "\n",
        "        # we'll time how many predictions per second we make\n",
        "        start = time.time()\n",
        "\n",
        "        # Loop through each of the given reviews and call run to predict\n",
        "        # its label. \n",
        "        for i in range(len(testing_reviews)):\n",
        "            pred = self.run(testing_reviews[i])\n",
        "            if(pred == testing_labels[i]):\n",
        "                correct += 1\n",
        "            \n",
        "            # For debug purposes, print out our prediction accuracy and speed \n",
        "            # throughout the prediction process. \n",
        "\n",
        "            elapsed_time = float(time.time() - start)\n",
        "            reviews_per_second = i / elapsed_time if elapsed_time > 0 else 0\n",
        "            \n",
        "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(testing_reviews)))[:4] \\\n",
        "                             + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] \\\n",
        "                             + \" #Correct:\" + str(correct) + \" #Tested:\" + str(i+1) \\\n",
        "                             + \" Testing Accuracy:\" + str(correct * 100 / float(i+1))[:4] + \"%\")\n",
        "    \n",
        "    def run(self, review):\n",
        "        # Run a forward pass through the network, like in the \"train\" function.\n",
        "        \n",
        "        # Input Layer\n",
        "        self.update_input_layer(review.lower())\n",
        "\n",
        "        # Hidden layer\n",
        "        layer_1 = self.layer_0.dot(self.weights_0_1)\n",
        "\n",
        "        # Output layer\n",
        "        layer_2 = self.sigmoid(layer_1.dot(self.weights_1_2))\n",
        "        \n",
        "        # Return POSITIVE for values above greater-than-or-equal-to 0.5 in the output layer;\n",
        "        # return NEGATIVE for other values\n",
        "        if(layer_2[0] >= 0.5):\n",
        "            return \"POSITIVE\"\n",
        "        else:\n",
        "            return \"NEGATIVE\"\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3EMVMxnU62-",
        "colab_type": "text"
      },
      "source": [
        "A célula abaixo irá criar um objeto do tipo `SentimentNetwork` que irá treinar sobre todos os dados que possuímos, exceto pelas últimas 1000 reviews (que serão utilizadas para teste)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1pOskpAU62_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp = SentimentNetwork(reviews[:-1000],labels[:-1000], learning_rate=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrS2eoIMU63E",
        "colab_type": "text"
      },
      "source": [
        "A célula abaixo irá testar a rede sobre o conjunto de testes nas últimas 1000 reviews que separamos acima.\n",
        "\n",
        "**Ainda não treinamos a rede, então os resultados provavelmente serão em torno de 50%.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Zu3GwOz7U63F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp.test(reviews[-1000:],labels[-1000:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUz8LIEtU63O",
        "colab_type": "text"
      },
      "source": [
        "A célula abaixo realiza o treinamento da rede. Iremos exibir também a acurácia para que possamos ver a evolução."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fePUlzIMU63O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp.train(reviews[:-1000], labels[:-1000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynYFVfLwU63c",
        "colab_type": "text"
      },
      "source": [
        "Observe que o treinamento não foi bom. Parte da razão pode ser por conta da taxa de aprendizado que está muito alta.\n",
        "\n",
        "A célula abaixo irá refazer o treinamento com uma taxa menor (de `0.01`). Vamos ver se melhoramos nossos resultados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRinX7g8U63d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp = SentimentNetwork(reviews[:-1000], labels[:-1000], learning_rate=0.01)\n",
        "mlp.train(reviews[:-1000],labels[:-1000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRVK2uXjU63o",
        "colab_type": "text"
      },
      "source": [
        "### Diferentes Inicializações, Diferentes Resultados\n",
        "\n",
        "Com uma inicialização um pouco melhor (`hidden_nodes**-0.5` em vez de `output_nodes**-0.5`), nós realmente vemos uma melhoria com um learning rate de 0.01. Essa solução não é perfeita, mas claramente mostra potencial e iremos melhorar isso daqui a pouco.\n",
        "\n",
        "Porque essa estratégia de inicialização é melhor? Ainda vamos chegar lá, mas é suficiente dizer que as melhores inicializações de pesos são uma função de $1/\\sqrt(n)$, onde n é o número de nós naquela camada. Sendo assim, os pesos entre a camada oculta e a camada de saída devem ser uma função do tamanho da camada escondida. Assim, utilizar `hidden_nodes**-0.5` em vez de `output_nodes**-0.5` é uma melhor estratégia de inicialização."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5tKnxesU63p",
        "colab_type": "text"
      },
      "source": [
        "# Fim do Projeto 3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tY7r3842U63r",
        "colab_type": "text"
      },
      "source": [
        "# Entendendo Ruído da Rede Neural<a id='lesson_4'></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GWH1w8MU63s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import Image\n",
        "Image(filename='sentiment_network.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CVcuLfKU630",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_input_layer(review):\n",
        "    global layer_0\n",
        "    \n",
        "    layer_0 *= 0\n",
        "    for word in review.split(\" \"):\n",
        "        layer_0[0][word2index[word]] += 1\n",
        "\n",
        "update_input_layer(reviews[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiNKd7DUU639",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layer_0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHUMIxCCU64D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "review_counter = Counter()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVP_VG6zU64K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for word in reviews[0].split(\" \"):\n",
        "    review_counter[word] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "alEwP6dqU64P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "review_counter.most_common()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fh5KIclXU64W",
        "colab_type": "text"
      },
      "source": [
        "# Projeto 4: Reduzindo Ruído nos Dados de Entrada<a id='project_4'></a>\n",
        "\n",
        "Faremos o seguinte para reduzir o ruído nos dados de entrada:\n",
        "* Copiaremos a classe `SentimentNetwork` que fizemos anteriormente.\n",
        "* Modificaremos o método `update_input_layer` para que não seja contada quantas vezes cada palavra é usada, mas, em vez disso, indicaremos apenas se a palavra foi utilizada ou não."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDIgEEe9U64X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "class SentimentNetwork:\n",
        "    def __init__(self, reviews,labels,hidden_nodes = 10, learning_rate = 0.1):\n",
        "        np.random.seed(1)\n",
        "\n",
        "        self.pre_process_data(reviews, labels)\n",
        "        \n",
        "        self.init_network(len(self.review_vocab), hidden_nodes, 1, learning_rate)\n",
        "\n",
        "    def pre_process_data(self, reviews, labels):\n",
        "        review_vocab = set()\n",
        "        for review in reviews:\n",
        "            for word in review.split(\" \"):\n",
        "                review_vocab.add(word)\n",
        "\n",
        "        self.review_vocab = list(review_vocab)\n",
        "        \n",
        "        label_vocab = set()\n",
        "        for label in labels:\n",
        "            label_vocab.add(label)\n",
        "        \n",
        "        self.label_vocab = list(label_vocab)\n",
        "        \n",
        "        self.review_vocab_size = len(self.review_vocab)\n",
        "        self.label_vocab_size = len(self.label_vocab)\n",
        "        \n",
        "        self.word2index = {}\n",
        "        for i, word in enumerate(self.review_vocab):\n",
        "            self.word2index[word] = i\n",
        "        \n",
        "        self.label2index = {}\n",
        "        for i, label in enumerate(self.label_vocab):\n",
        "            self.label2index[label] = i\n",
        "        \n",
        "    def init_network(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
        "        self.input_nodes = input_nodes\n",
        "        self.hidden_nodes = hidden_nodes\n",
        "        self.output_nodes = output_nodes\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.weights_0_1 = np.zeros((self.input_nodes,self.hidden_nodes))\n",
        "    \n",
        "        self.weights_1_2 = np.random.normal(0.0, self.hidden_nodes**-0.5, \n",
        "                                                (self.hidden_nodes, self.output_nodes))\n",
        "        \n",
        "        self.layer_0 = np.zeros((1,input_nodes))\n",
        "    \n",
        "        \n",
        "    def update_input_layer(self,review):\n",
        "        self.layer_0 *= 0\n",
        "        \n",
        "        for word in review.split(' '):\n",
        "            if(word in self.word2index.keys()):\n",
        "                ## ALTERADO PARA ESTE PROJETO: em vez de somar 1, setamos para 1\n",
        "                self.layer_0[0][self.word2index[word]] = 1\n",
        "                \n",
        "    def get_target_for_label(self,label):\n",
        "        if(label == 'POSITIVE'):\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "        \n",
        "    def sigmoid(self,x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "    \n",
        "    def sigmoid_output_2_derivative(self,output):\n",
        "        return output * (1 - output)\n",
        "    \n",
        "    def train(self, training_reviews, training_labels):\n",
        "        assert(len(training_reviews) == len(training_labels))\n",
        "        \n",
        "        correct_so_far = 0\n",
        "\n",
        "        start = time.time()\n",
        "        \n",
        "        for i in range(len(training_reviews)):\n",
        "            review = training_reviews[i]\n",
        "            label = training_labels[i]\n",
        "            \n",
        "            ### Forward pass ###\n",
        "\n",
        "            # Input Layer\n",
        "            self.update_input_layer(review)\n",
        "\n",
        "            # Hidden layer\n",
        "            layer_1 = self.layer_0.dot(self.weights_0_1)\n",
        "\n",
        "            # Output layer\n",
        "            layer_2 = self.sigmoid(layer_1.dot(self.weights_1_2))\n",
        "            \n",
        "            ### Backward pass ###\n",
        "\n",
        "            # Output error\n",
        "            layer_2_error = layer_2 - self.get_target_for_label(label) # Output layer error is the difference between desired target and actual output.\n",
        "            layer_2_delta = layer_2_error * self.sigmoid_output_2_derivative(layer_2)\n",
        "\n",
        "            # Backpropagated error\n",
        "            layer_1_error = layer_2_delta.dot(self.weights_1_2.T) # errors propagated to the hidden layer\n",
        "            layer_1_delta = layer_1_error # hidden layer gradients - no nonlinearity so it's the same as the error\n",
        "\n",
        "            # Update the weights\n",
        "            self.weights_1_2 -= layer_1.T.dot(layer_2_delta) * self.learning_rate # update hidden-to-output weights with gradient descent step\n",
        "            self.weights_0_1 -= self.layer_0.T.dot(layer_1_delta) * self.learning_rate # update input-to-hidden weights with gradient descent step\n",
        "\n",
        "            # Keep track of correct predictions.\n",
        "            if(layer_2 >= 0.5 and label == 'POSITIVE'):\n",
        "                correct_so_far += 1\n",
        "            elif(layer_2 < 0.5 and label == 'NEGATIVE'):\n",
        "                correct_so_far += 1\n",
        "            \n",
        "            # For debug purposes, print out our prediction accuracy and speed \n",
        "            # throughout the training process. \n",
        "            elapsed_time = float(time.time() - start)\n",
        "            reviews_per_second = i / elapsed_time if elapsed_time > 0 else 0\n",
        "            \n",
        "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(training_reviews)))[:4] \\\n",
        "                             + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] \\\n",
        "                             + \" #Correct:\" + str(correct_so_far) + \" #Trained:\" + str(i+1) \\\n",
        "                             + \" Training Accuracy:\" + str(correct_so_far * 100 / float(i+1))[:4] + \"%\")\n",
        "            if(i % 2500 == 0):\n",
        "                print(\"\")\n",
        "    \n",
        "    def test(self, testing_reviews, testing_labels):\n",
        "        # keep track of how many correct predictions we make\n",
        "        correct = 0\n",
        "\n",
        "        # we'll time how many predictions per second we make\n",
        "        start = time.time()\n",
        "\n",
        "        # Loop through each of the given reviews and call run to predict\n",
        "        # its label. \n",
        "        for i in range(len(testing_reviews)):\n",
        "            pred = self.run(testing_reviews[i])\n",
        "            if(pred == testing_labels[i]):\n",
        "                correct += 1\n",
        "            \n",
        "            # For debug purposes, print out our prediction accuracy and speed \n",
        "            # throughout the prediction process. \n",
        "\n",
        "            elapsed_time = float(time.time() - start)\n",
        "            reviews_per_second = i / elapsed_time if elapsed_time > 0 else 0\n",
        "            \n",
        "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(testing_reviews)))[:4] \\\n",
        "                             + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] \\\n",
        "                             + \" #Correct:\" + str(correct) + \" #Tested:\" + str(i+1) \\\n",
        "                             + \" Testing Accuracy:\" + str(correct * 100 / float(i+1))[:4] + \"%\")\n",
        "    \n",
        "    def run(self, review):\n",
        "        \"\"\"\n",
        "        Returns a POSITIVE or NEGATIVE prediction for the given review.\n",
        "        \"\"\"\n",
        "        # Run a forward pass through the network, like in the \"train\" function.\n",
        "        \n",
        "        # Input Layer\n",
        "        self.update_input_layer(review.lower())\n",
        "\n",
        "        # Hidden layer\n",
        "        layer_1 = self.layer_0.dot(self.weights_0_1)\n",
        "\n",
        "        # Output layer\n",
        "        layer_2 = self.sigmoid(layer_1.dot(self.weights_1_2))\n",
        "        \n",
        "        # Return POSITIVE for values above greater-than-or-equal-to 0.5 in the output layer;\n",
        "        # return NEGATIVE for other values\n",
        "        if(layer_2[0] >= 0.5):\n",
        "            return \"POSITIVE\"\n",
        "        else:\n",
        "            return \"NEGATIVE\"\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGom9apMU64d",
        "colab_type": "text"
      },
      "source": [
        "A célula abaixo recria a rede e a treina. Note que retornamos ao learning rate de `0.1`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "FyY70-CIU64g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp = SentimentNetwork(reviews[:-1000], labels[:-1000], learning_rate=0.1)\n",
        "mlp.train(reviews[:-1000], labels[:-1000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jakL2nGlU64m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp.test(reviews[-1000:], labels[-1000:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yd2y6eHEU64w",
        "colab_type": "text"
      },
      "source": [
        "# Fim do Projeto 4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Bq_Qxd_U64x",
        "colab_type": "text"
      },
      "source": [
        "# Analisando o Vocabulário"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijMWO85gU64y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pos_neg_ratios.most_common()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcdwaPNnU646",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list(reversed(pos_neg_ratios.most_common()))[0:30]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpDnqP6NU65E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bokeh.models import ColumnDataSource, LabelSet\n",
        "from bokeh.plotting import figure, show, output_file\n",
        "from bokeh.io import output_notebook\n",
        "output_notebook()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdQqJQ_0U65L",
        "colab_type": "text"
      },
      "source": [
        "Agora, vamos dar uma olhada na distribuição das palavras e suas afinidades com reviews positivas e negativas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upQgjyHbU65M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hist, edges = np.histogram(list(map(lambda x:x[1],pos_neg_ratios.most_common())), density=True, bins=100, normed=True)\n",
        "\n",
        "p = figure(tools=\"pan,wheel_zoom,reset,save\",\n",
        "           toolbar_location=\"above\",\n",
        "           title=\"Word Positive/Negative Affinity Distribution\")\n",
        "p.quad(top=hist, bottom=0, left=edges[:-1], right=edges[1:], line_color=\"#555555\")\n",
        "show(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJW_MnxKU65R",
        "colab_type": "text"
      },
      "source": [
        "Ainda, vamos dar uma olhada na distribuição da frequência com que as palavras aparecem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxJs5Ap4U65S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "frequency_frequency = Counter()\n",
        "\n",
        "for word, cnt in total_counts.most_common():\n",
        "    frequency_frequency[cnt] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cw9yY4t0U65X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hist, edges = np.histogram(list(map(lambda x:x[1],frequency_frequency.most_common())), density=True, bins=100, normed=True)\n",
        "\n",
        "p = figure(tools=\"pan,wheel_zoom,reset,save\",\n",
        "           toolbar_location=\"above\",\n",
        "           title=\"The frequency distribution of the words in our corpus\")\n",
        "p.quad(top=hist, bottom=0, left=edges[:-1], right=edges[1:], line_color=\"#555555\")\n",
        "show(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPD1LGLpU65d",
        "colab_type": "text"
      },
      "source": [
        "# Projeto 5: Reduzindo o Ruído através da Redução Estratégica do Vocabulário<a id='project_5'></a>\n",
        "\n",
        "Aqui, iremos reduzir o ruído dos nossos dados através da redução do nosso vocabulário. Faremos o seguinte (a partir da mesma classe `SentimentNetwork`):\n",
        "\n",
        "* Modificaremos o método `pre_process_data`:\n",
        ">* Adicionaremos dois parâmetros adicionais: `min_count` e `polarity_cutoff`.\n",
        ">* Calcularemos a taxa positive-to-negative das palavras da review.\n",
        ">* Adicionaremos ao vocabulário apenas palavras que ocorrem mais do que `min_count` vezes.\n",
        ">* Adicionaremos ao vocabulário apenas palavras cujo valor da taxa postive-to-negative é pelo menos `polarity_cutoff`.\n",
        "* Modificaremos o método `__init__`:\n",
        ">* Adicionaremos os mesmos dois parâmetros (`min_count` e `polarity_cutoff`) e utilizá-los na chamada ao método `pre_process_data`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2OZM29NU65e",
        "colab_type": "text"
      },
      "source": [
        "Procure pelos pontos onde há anotações de alterações de código:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aSgANpdU65f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "class SentimentNetwork:\n",
        "    ## ALTERAÇÃO REALIZADA NESTE PROJETO: adição de min_count e polarity_cutoff\n",
        "    def __init__(self, reviews, labels, min_count=10, polarity_cutoff=0.1, hidden_nodes=10, learning_rate=0.1):\n",
        "        np.random.seed(1)\n",
        "\n",
        "        ## ALTERAÇÃO REALIZADA NESTE PROJETO: adição dos parâmetros min_count e polarity_cutoff\n",
        "        self.pre_process_data(reviews, labels, polarity_cutoff, min_count)\n",
        "        self.init_network(len(self.review_vocab),hidden_nodes, 1, learning_rate)\n",
        "\n",
        "    ## ALTERAÇÃO REALIZADA NESTE PROJETO: adição dos parâmetros min_count e polarity_cutoff\n",
        "    def pre_process_data(self, reviews, labels, polarity_cutoff, min_count):\n",
        "        \n",
        "        ## ----------------------------------------\n",
        "        ## ALTERAÇÃO REALIZADA NESTE PROJETO: cálculo da taxa positive-to-negative para as palavras antes\n",
        "        # da construção do vocabulário\n",
        "        positive_counts = Counter()\n",
        "        negative_counts = Counter()\n",
        "        total_counts = Counter()\n",
        "\n",
        "        for i in range(len(reviews)):\n",
        "            if(labels[i] == 'POSITIVE'):\n",
        "                for word in reviews[i].split(\" \"):\n",
        "                    positive_counts[word] += 1\n",
        "                    total_counts[word] += 1\n",
        "            else:\n",
        "                for word in reviews[i].split(\" \"):\n",
        "                    negative_counts[word] += 1\n",
        "                    total_counts[word] += 1\n",
        "\n",
        "        pos_neg_ratios = Counter()\n",
        "\n",
        "        for term,cnt in list(total_counts.most_common()):\n",
        "            if(cnt >= 50):\n",
        "                pos_neg_ratio = positive_counts[term] / float(negative_counts[term]+1)\n",
        "                pos_neg_ratios[term] = pos_neg_ratio\n",
        "\n",
        "        for word,ratio in pos_neg_ratios.most_common():\n",
        "            if(ratio > 1):\n",
        "                pos_neg_ratios[word] = np.log(ratio)\n",
        "            else:\n",
        "                pos_neg_ratios[word] = -np.log((1 / (ratio + 0.01)))\n",
        "        #\n",
        "        ## ----------------------------------------\n",
        "        \n",
        "        review_vocab = set()\n",
        "        for review in reviews:\n",
        "            for word in review.split(\" \"):\n",
        "                ## ALTERAÇÃO REALIZADA NESTE PROJETO: apenas iremos adicionar palavras que ocorrem\n",
        "                # pelo menos min_count vezes e que possuem uma taxa de pelo menos polarity_cutoff\n",
        "                if(total_counts[word] > min_count):\n",
        "                    if(word in pos_neg_ratios.keys()):\n",
        "                        if((pos_neg_ratios[word] >= polarity_cutoff) or (pos_neg_ratios[word] <= -polarity_cutoff)):\n",
        "                            review_vocab.add(word)\n",
        "                    else:\n",
        "                        review_vocab.add(word)\n",
        "                        \n",
        "        self.review_vocab = list(review_vocab)\n",
        "        \n",
        "        label_vocab = set()\n",
        "        for label in labels:\n",
        "            label_vocab.add(label)\n",
        "        \n",
        "        self.label_vocab = list(label_vocab)\n",
        "        \n",
        "        self.review_vocab_size = len(self.review_vocab)\n",
        "        self.label_vocab_size = len(self.label_vocab)\n",
        "        \n",
        "        self.word2index = {}\n",
        "        for i, word in enumerate(self.review_vocab):\n",
        "            self.word2index[word] = i\n",
        "        \n",
        "        self.label2index = {}\n",
        "        for i, label in enumerate(self.label_vocab):\n",
        "            self.label2index[label] = i\n",
        "\n",
        "    def init_network(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
        "        self.input_nodes = input_nodes\n",
        "        self.hidden_nodes = hidden_nodes\n",
        "        self.output_nodes = output_nodes\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.weights_0_1 = np.zeros((self.input_nodes,self.hidden_nodes))\n",
        "\n",
        "        self.weights_1_2 = np.random.normal(0.0, self.hidden_nodes**-0.5, \n",
        "                                                (self.hidden_nodes, self.output_nodes))\n",
        "        \n",
        "        ## ALTERAÇÃO REALIZADA NESTE PROJETO: removemos layer_0, adicionamos layer_1\n",
        "        self.layer_1 = np.zeros((1,hidden_nodes))\n",
        "    \n",
        "    ## ALTERAÇÃO REALIZADA NESTE PROJETO: removemos o método update_input_layer\n",
        "    \n",
        "    def get_target_for_label(self,label):\n",
        "        if(label == 'POSITIVE'):\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "        \n",
        "    def sigmoid(self,x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "    \n",
        "    def sigmoid_output_2_derivative(self,output):\n",
        "        return output * (1 - output)\n",
        "    \n",
        "    def train(self, training_reviews_raw, training_labels):\n",
        "\n",
        "        ## ALTERAÇÃO REALIZADA NESTE PROJETO: pré-processamos training_reviews\n",
        "        # para que possamos lidar diretamente com os índices de entradas não-nulas.\n",
        "        training_reviews = list()\n",
        "        for review in training_reviews_raw:\n",
        "            indices = set()\n",
        "            for word in review.split(' '):\n",
        "                if(word in self.word2index.keys()):\n",
        "                    indices.add(self.word2index[word])\n",
        "            training_reviews.append(list(indices))\n",
        "\n",
        "        assert(len(training_reviews) == len(training_labels))\n",
        "        \n",
        "        correct_so_far = 0\n",
        "\n",
        "        start = time.time()\n",
        "        \n",
        "        for i in range(len(training_reviews)):\n",
        "            review = training_reviews[i]\n",
        "            label = training_labels[i]\n",
        "            ### Forward pass ###\n",
        "\n",
        "            ## ALTERAÇÃO REALIZADA NESTE PROJETO: removemos a chamada para 'update_input_layer',\n",
        "            # pois 'layer_0' não é mais utilizada.\n",
        "\n",
        "            # Hidden layer\n",
        "            ## ALTERAÇÃO REALIZADA NESTE PROJETO: adição dos pesos apenas para itens não-nulos\n",
        "            self.layer_1 *= 0\n",
        "            for index in review:\n",
        "                self.layer_1 += self.weights_0_1[index]\n",
        "\n",
        "            # Output layer\n",
        "            ## ALTERAÇÃO REALIZADA NESTE PROJETO: utilização de 'self.layer_1' em vez de 'local layer_1'\n",
        "            layer_2 = self.sigmoid(self.layer_1.dot(self.weights_1_2))            \n",
        "            \n",
        "            ### Backward pass ###\n",
        "\n",
        "            # Output error\n",
        "            layer_2_error = layer_2 - self.get_target_for_label(label) # Output layer error is the difference between desired target and actual output.\n",
        "            layer_2_delta = layer_2_error * self.sigmoid_output_2_derivative(layer_2)\n",
        "\n",
        "            # Backpropagated error\n",
        "            layer_1_error = layer_2_delta.dot(self.weights_1_2.T) # errors propagated to the hidden layer\n",
        "            layer_1_delta = layer_1_error # hidden layer gradients - no nonlinearity so it's the same as the error\n",
        "\n",
        "            # Update the weights\n",
        "            ## ALTERAÇÃO REALIZADA NESTE PROJETO: utilização de 'self.layer_1' em vez de 'layer_1'\n",
        "            self.weights_1_2 -= self.layer_1.T.dot(layer_2_delta) * self.learning_rate # update hidden-to-output weights with gradient descent step\n",
        "            \n",
        "            ## ALTERAÇÃO REALIZADA NESTE PROJETO: atualização apenas dos pesos que foram utilizados na passada forward\n",
        "            for index in review:\n",
        "                self.weights_0_1[index] -= layer_1_delta[0] * self.learning_rate # update input-to-hidden weights with gradient descent step\n",
        "\n",
        "            # Keep track of correct predictions.\n",
        "            if(layer_2 >= 0.5 and label == 'POSITIVE'):\n",
        "                correct_so_far += 1\n",
        "            elif(layer_2 < 0.5 and label == 'NEGATIVE'):\n",
        "                correct_so_far += 1\n",
        "            \n",
        "            # For debug purposes, print out our prediction accuracy and speed \n",
        "            # throughout the training process. \n",
        "            elapsed_time = float(time.time() - start)\n",
        "            reviews_per_second = i / elapsed_time if elapsed_time > 0 else 0\n",
        "            \n",
        "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(training_reviews)))[:4] \\\n",
        "                             + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] \\\n",
        "                             + \" #Correct:\" + str(correct_so_far) + \" #Trained:\" + str(i+1) \\\n",
        "                             + \" Training Accuracy:\" + str(correct_so_far * 100 / float(i+1))[:4] + \"%\")\n",
        "            if(i % 2500 == 0):\n",
        "                print(\"\")\n",
        "    \n",
        "    def test(self, testing_reviews, testing_labels):\n",
        "        # keep track of how many correct predictions we make\n",
        "        correct = 0\n",
        "\n",
        "        # we'll time how many predictions per second we make\n",
        "        start = time.time()\n",
        "\n",
        "        # Loop through each of the given reviews and call run to predict\n",
        "        # its label. \n",
        "        for i in range(len(testing_reviews)):\n",
        "            pred = self.run(testing_reviews[i])\n",
        "            if(pred == testing_labels[i]):\n",
        "                correct += 1\n",
        "            \n",
        "            # For debug purposes, print out our prediction accuracy and speed \n",
        "            # throughout the prediction process. \n",
        "\n",
        "            elapsed_time = float(time.time() - start)\n",
        "            reviews_per_second = i / elapsed_time if elapsed_time > 0 else 0\n",
        "            \n",
        "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(testing_reviews)))[:4] \\\n",
        "                             + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] \\\n",
        "                             + \" #Correct:\" + str(correct) + \" #Tested:\" + str(i+1) \\\n",
        "                             + \" Testing Accuracy:\" + str(correct * 100 / float(i+1))[:4] + \"%\")\n",
        "    \n",
        "    def run(self, review):\n",
        "        # Run a forward pass through the network, like in the \"train\" function.\n",
        "        \n",
        "        ## ALTERAÇÃO REALIZADA NESTE PROJETO: remoção da chamada ao método update_input_layer\n",
        "        # pois layer_0 não é mais utilizada\n",
        "\n",
        "        # Hidden layer\n",
        "        ## ALTERAÇÃO REALIZADA NESTE PROJETO: identificação dos índices utilizados na review e\n",
        "        # depois adicionamos apenas esses pesos ao layer_1\n",
        "        self.layer_1 *= 0\n",
        "        unique_indices = set()\n",
        "        for word in review.lower().split(\" \"):\n",
        "            if word in self.word2index.keys():\n",
        "                unique_indices.add(self.word2index[word])\n",
        "        for index in unique_indices:\n",
        "            self.layer_1 += self.weights_0_1[index]\n",
        "        \n",
        "        # Output layer\n",
        "        ## ALTERAÇÃO REALIZADA NESTE PROJETO: alteração para utilizar self.layer_1 em vez de layer_1\n",
        "        layer_2 = self.sigmoid(self.layer_1.dot(self.weights_1_2))\n",
        "         \n",
        "        # Return POSITIVE for values above greater-than-or-equal-to 0.5 in the output layer;\n",
        "        # return NEGATIVE for other values\n",
        "        if(layer_2[0] >= 0.5):\n",
        "            return \"POSITIVE\"\n",
        "        else:\n",
        "            return \"NEGATIVE\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdC0n_xnU65o",
        "colab_type": "text"
      },
      "source": [
        "Vamos treinar a rede com um pequeno `polarity_cutoff`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxQ25zHKU65q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp = SentimentNetwork(reviews[:-1000], labels[:-1000], min_count=20, polarity_cutoff=0.05, learning_rate=0.01)\n",
        "mlp.train(reviews[:-1000], labels[:-1000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_4KgoZKU65x",
        "colab_type": "text"
      },
      "source": [
        "Vamos testar sobre o conjunto de testes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSjf1zDTU65y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp.test(reviews[-1000:], labels[-1000:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7Cx9evmU653",
        "colab_type": "text"
      },
      "source": [
        "Vamos aumentar o valor de `polarity_cutoff`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ge56mh5oU653",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp = SentimentNetwork(reviews[:-1000], labels[:-1000], min_count=20, polarity_cutoff=0.8, learning_rate=0.01)\n",
        "mlp.train(reviews[:-1000], labels[:-1000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaSCvxJyU659",
        "colab_type": "text"
      },
      "source": [
        "Vamos testar o conjunto de testes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_JC9u6KU65-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp.test(reviews[-1000:], labels[-1000:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xthu2oJXU66D",
        "colab_type": "text"
      },
      "source": [
        "# Fim do Projeto 5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoNma-9QU66E",
        "colab_type": "text"
      },
      "source": [
        "# Mini-projeto\n",
        "\n",
        "Agora, que tal a gente entender melhor o que foi feito aqui por meio de uma prática?\n",
        "\n",
        "Implemente essa mesma análise de sentimentos utilizando o PyTorch e obtenha resultados melhores do que os que obtivemos aqui (acima de 85% de acurácia). Faça a matriz de confusão e calcule as métricas que julgar interessantes. Implemente também um conjunto de validação e treine implementando o early stopping.\n",
        "\n",
        "* Desafio: o que fizemos aqui é o básico do básico de NLP. Sendo assim, vá além! Utilize quaisquer métodos de classificação e de NLP para obter a maior taxa de acerto da turma neste dataset. O grupo que obtiver a melhor acurácia receberá 1 ponto extra neste projeto.\n",
        "\n",
        "Utilize sempre as 1000 últimas reviews como sendo o conjunto de teste. Para os conjuntos de treinamento e validação, utilize os dados restantes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttfOrsAin901",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from string import punctuation\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-Y8oGH_oOwR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "with open('data/reviews.txt', 'r') as f:\n",
        "    reviews = f.read()\n",
        "with open('data/labels.txt', 'r') as f:\n",
        "    labels = f.read()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDel7U_sWxYE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8c55f438-0b26-48b7-94ae-7cfb418ecd9a"
      },
      "source": [
        "# First checking if GPU is available\n",
        "train_on_gpu=torch.cuda.is_available()\n",
        "\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU.')\n",
        "else:\n",
        "    print('No GPU available, training on CPU.')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXrW7MxqqeRr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "cd710e73-245a-4669-d9f2-a258ca5c6ed7"
      },
      "source": [
        "\n",
        "from string import punctuation\n",
        "\n",
        "# get rid of punctuation\n",
        "reviews = reviews.lower() # lowercase, standardize\n",
        "all_text = ''.join([c for c in reviews if c not in punctuation])\n",
        "\n",
        "# split by new lines and spaces\n",
        "reviews_split = all_text.split('\\n')\n",
        "all_text = ' '.join(reviews_split)\n",
        "\n",
        "# create a list of words\n",
        "words = all_text.split()\n",
        "\n",
        "words[:30]"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bromwell',\n",
              " 'high',\n",
              " 'is',\n",
              " 'a',\n",
              " 'cartoon',\n",
              " 'comedy',\n",
              " 'it',\n",
              " 'ran',\n",
              " 'at',\n",
              " 'the',\n",
              " 'same',\n",
              " 'time',\n",
              " 'as',\n",
              " 'some',\n",
              " 'other',\n",
              " 'programs',\n",
              " 'about',\n",
              " 'school',\n",
              " 'life',\n",
              " 'such',\n",
              " 'as',\n",
              " 'teachers',\n",
              " 'my',\n",
              " 'years',\n",
              " 'in',\n",
              " 'the',\n",
              " 'teaching',\n",
              " 'profession',\n",
              " 'lead',\n",
              " 'me']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1mvYuumrVf-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Build a dictionary that maps words to integers\n",
        "counts = Counter(words)\n",
        "vocab = sorted(counts, key=counts.get, reverse=True)\n",
        "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
        "\n",
        "## use the dict to tokenize each review in reviews_split\n",
        "## store the tokenized reviews in reviews_ints\n",
        "reviews_ints = []\n",
        "for review in reviews_split:\n",
        "    reviews_ints.append([vocab_to_int[word] for word in review.split()])"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1LKttflokgT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "78c574fb-01cd-4b16-ab83-b3916ea8462e"
      },
      "source": [
        "# stats about vocabulary\n",
        "print('Unique words: ', len((vocab_to_int)))  # should ~ 74000+\n",
        "print()\n",
        "\n",
        "# print tokens in first review\n",
        "print('Tokenized review: \\n', reviews_ints[:1])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique words:  74072\n",
            "\n",
            "Tokenized review: \n",
            " [[21025, 308, 6, 3, 1050, 207, 8, 2138, 32, 1, 171, 57, 15, 49, 81, 5785, 44, 382, 110, 140, 15, 5194, 60, 154, 9, 1, 4975, 5852, 475, 71, 5, 260, 12, 21025, 308, 13, 1978, 6, 74, 2395, 5, 613, 73, 6, 5194, 1, 24103, 5, 1983, 10166, 1, 5786, 1499, 36, 51, 66, 204, 145, 67, 1199, 5194, 19869, 1, 37442, 4, 1, 221, 883, 31, 2988, 71, 4, 1, 5787, 10, 686, 2, 67, 1499, 54, 10, 216, 1, 383, 9, 62, 3, 1406, 3686, 783, 5, 3483, 180, 1, 382, 10, 1212, 13583, 32, 308, 3, 349, 341, 2913, 10, 143, 127, 5, 7690, 30, 4, 129, 5194, 1406, 2326, 5, 21025, 308, 10, 528, 12, 109, 1448, 4, 60, 543, 102, 12, 21025, 308, 6, 227, 4146, 48, 3, 2211, 12, 8, 215, 23]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Gd4NT_CnuGM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels_split = labels.split('\\n')\n",
        "encoded_labels = np.array([1 if label == 'positive' else 0 for label in labels_split])"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjJ4e82EmpAP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "37d53e9a-e4b3-4a9c-84b2-4adc3cea7d24"
      },
      "source": [
        "review_lens = Counter([len(x) for x in reviews_ints])\n",
        "print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
        "print(\"Maximum review length: {}\".format(max(review_lens)))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Zero-length reviews: 1\n",
            "Maximum review length: 2514\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykLbG9b1o2DE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "20fb3d68-dc85-4c7e-c93d-e7cd584cbb76"
      },
      "source": [
        "print('Number of reviews before removing outliers: ', len(reviews_ints))\n",
        "\n",
        "## remove any reviews/labels with zero length from the reviews_ints list.\n",
        "\n",
        "# get indices of any reviews with length 0\n",
        "non_zero_idx = [ii for ii, review in enumerate(reviews_ints) if len(review) != 0]\n",
        "\n",
        "# remove 0-length reviews and their labels\n",
        "reviews_ints = [reviews_ints[ii] for ii in non_zero_idx]\n",
        "encoded_labels = np.array([encoded_labels[ii] for ii in non_zero_idx])\n",
        "\n",
        "print('Number of reviews after removing outliers: ', len(reviews_ints))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of reviews before removing outliers:  25001\n",
            "Number of reviews after removing outliers:  25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7o_wssNSnSBx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_features(reviews_ints, seq_length):\n",
        "    ''' Return features of review_ints, where each review is padded with 0's \n",
        "        or truncated to the input seq_length.\n",
        "    '''\n",
        "    \n",
        "    # getting the correct rows x cols shape\n",
        "    features = np.zeros((len(reviews_ints), seq_length), dtype=int)\n",
        "\n",
        "    # for each review, I grab that review and \n",
        "    for i, row in enumerate(reviews_ints):\n",
        "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
        "    \n",
        "    return features"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOnNybWznSm3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "8aa7710f-c0b1-4724-e6ee-6ceb56facfdd"
      },
      "source": [
        "seq_length = 200\n",
        "\n",
        "features = pad_features(reviews_ints, seq_length=seq_length)\n",
        "\n",
        "print(features[:30,:10])"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [22382    42 46418    15   706 17139  3389    47    77    35]\n",
            " [ 4505   505    15     3  3342   162  8312  1652     6  4819]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [   54    10    14   116    60   798   552    71   364     5]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    1   330   578    34     3   162   748  2731     9   325]\n",
            " [    9    11 10171  5305  1946   689   444    22   280   673]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    1   307 10399  2069  1565  6202  6528  3288 17946 10628]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [   21   122  2069  1565   515  8181    88     6  1325  1182]\n",
            " [    1    20     6    76    40     6    58    81    95     5]\n",
            " [   54    10    84   329 26230 46427    63    10    14   614]\n",
            " [   11    20     6    30  1436 32317  3769   690 15100     6]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [   40    26   109 17952  1422     9     1   327     4   125]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [   10   499     1   307 10399    55    74     8    13    30]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_nd7T_-nSq8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "8d1fde83-e966-4660-e167-ace5f27ad1d5"
      },
      "source": [
        "split_frac = 0.8\n",
        "\n",
        "## split data into training, validation, and test data (features and labels, x and y)\n",
        "\n",
        "split_idx = int(len(features)*0.8)\n",
        "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
        "train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
        "\n",
        "test_idx = int(len(remaining_x)*0.5)\n",
        "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
        "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
        "\n",
        "## print out the shapes of your resultant feature data\n",
        "print(\"\\t\\t\\tFeature Shapes:\")\n",
        "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
        "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
        "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t\t\tFeature Shapes:\n",
            "Train set: \t\t(20000, 200) \n",
            "Validation set: \t(2500, 200) \n",
            "Test set: \t\t(2500, 200)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiZYbUyEo8Z6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSomiwCNnSvs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create Tensor datasets\n",
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
        "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "\n",
        "# dataloaders\n",
        "batch_size = 50\n",
        "\n",
        "# make sure the SHUFFLE your training data\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BI_mrz6hnS0h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "88cee6d5-5552-4585-e695-762dfa77351e"
      },
      "source": [
        ""
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample input size:  torch.Size([50, 200])\n",
            "Sample input: \n",
            " tensor([[    0,     0,     0,  ...,     1,   574,  1860],\n",
            "        [    0,     0,     0,  ...,    16,     3,   494],\n",
            "        [ 1052,     1,  4296,  ...,    80,     5,     1],\n",
            "        ...,\n",
            "        [32123,  3871,     6,  ...,     1,  1284,    30],\n",
            "        [    0,     0,     0,  ...,    32,     1,   130],\n",
            "        [    0,     0,     0,  ...,  1094,     4,   612]])\n",
            "\n",
            "Sample label size:  torch.Size([50])\n",
            "Sample label: \n",
            " tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,\n",
            "        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n",
            "        1, 0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkEnieobUFta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SentimentalAnalytics(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "\n",
        "    super(SentimentalAnalytics, self).__init__()\n",
        "\n",
        "    self.output_size = output_size\n",
        "    self.n_layers = n_layers\n",
        "    self.hidden_dim = hidden_dim\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim=embedding_dim)\n",
        "    self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "    \n",
        "         # dropout layer\n",
        "    self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    # linear and sigmoid layers\n",
        "    self.fc = nn.Linear(hidden_dim, output_size)\n",
        "    self.sig = nn.Sigmoid()\n",
        "  def forward(self, x, hidden):\n",
        "      \"\"\"\n",
        "      Perform a forward pass of our model on some input and hidden state.\n",
        "      \"\"\"\n",
        "      batch_size = x.size(0)\n",
        "\n",
        "      # embeddings and lstm_out\n",
        "      embeds = self.embedding(x)\n",
        "      lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "  \n",
        "      # stack up lstm outputs\n",
        "      lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "      \n",
        "      # dropout and fully-connected layer\n",
        "      out = self.dropout(lstm_out)\n",
        "      out = self.fc(out)\n",
        "      # sigmoid function\n",
        "      sig_out = self.sig(out)\n",
        "      \n",
        "      # reshape to be batch_size first\n",
        "      sig_out = sig_out.view(batch_size, -1)\n",
        "      sig_out = sig_out[:, -1] # get last batch of labels\n",
        "      \n",
        "      # return last sigmoid output and hidden state\n",
        "      return sig_out, hidden\n",
        "    \n",
        "  def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
        "        \n",
        "        return hidden "
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_p-irTZxyK1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "de9d1a96-3fb0-441e-a260-d6002d2293ce"
      },
      "source": [
        "vocab_leng = len(vocab_to_int)+1\n",
        "\n",
        "output_size = 1\n",
        "\n",
        "embedding_dim = 400\n",
        "\n",
        "hidden_dim = 256\n",
        "\n",
        "n_layers = 2\n",
        "\n",
        "model = SentimentalAnalytics(vocab_leng, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.6)\n",
        "\n",
        "print(model)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SentimentalAnalytics(\n",
            "  (embedding): Embedding(74073, 400)\n",
            "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.6)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSDsi6gPyz2p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 0.001\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "optimizer = torch.optim.Adamax(model.parameters(), lr)\n"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31bYRdk3zKAw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "dd5eceb3-cb92-401d-8dff-3e749f5eb601"
      },
      "source": [
        "epochs = 10\n",
        "\n",
        "counter = 0\n",
        "\n",
        "print_every = 200\n",
        "\n",
        "clip=5\n",
        "\n",
        "if(train_on_gpu):\n",
        "    model.cuda()\n",
        "\n",
        "\n",
        "for e in range(epochs):\n",
        "    # initialize hidden state\n",
        "    h = model.init_hidden(batch_size)\n",
        "\n",
        "    # batch loop\n",
        "    for inputs, labels in train_loader:\n",
        "        counter += 1\n",
        "\n",
        "        if(train_on_gpu):\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "        # Creating new variables for the hidden state, otherwise\n",
        "        # we'd backprop through the entire training history\n",
        "        h = tuple([each.data for each in h])\n",
        "\n",
        "        # zero accumulated gradients\n",
        "        model.zero_grad()\n",
        "\n",
        "        # get the output from the model\n",
        "        output, h = model(inputs, h)\n",
        "\n",
        "        # calculate the loss and perform backprop\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        # loss stats\n",
        "        if counter % print_every == 0:\n",
        "            # Get validation loss\n",
        "            val_h = model.init_hidden(batch_size)\n",
        "            val_losses = []\n",
        "            model.eval()\n",
        "            for inputs, labels in valid_loader:\n",
        "\n",
        "                val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                if(train_on_gpu):\n",
        "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "                output, val_h = model(inputs, val_h)\n",
        "                val_loss = criterion(output.squeeze(), labels.float())\n",
        "\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "            model.train()\n",
        "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/10... Step: 200... Loss: 0.328578... Val Loss: 0.483479\n",
            "Epoch: 1/10... Step: 400... Loss: 0.483373... Val Loss: 0.491322\n",
            "Epoch: 2/10... Step: 600... Loss: 0.254385... Val Loss: 0.613720\n",
            "Epoch: 2/10... Step: 800... Loss: 0.280533... Val Loss: 0.429817\n",
            "Epoch: 3/10... Step: 1000... Loss: 0.233427... Val Loss: 0.432853\n",
            "Epoch: 3/10... Step: 1200... Loss: 0.256592... Val Loss: 0.452101\n",
            "Epoch: 4/10... Step: 1400... Loss: 0.191377... Val Loss: 0.442684\n",
            "Epoch: 4/10... Step: 1600... Loss: 0.213202... Val Loss: 0.446288\n",
            "Epoch: 5/10... Step: 1800... Loss: 0.221942... Val Loss: 0.506222\n",
            "Epoch: 5/10... Step: 2000... Loss: 0.249970... Val Loss: 0.449031\n",
            "Epoch: 6/10... Step: 2200... Loss: 0.247803... Val Loss: 0.519646\n",
            "Epoch: 6/10... Step: 2400... Loss: 0.087720... Val Loss: 0.553175\n",
            "Epoch: 7/10... Step: 2600... Loss: 0.068131... Val Loss: 0.581983\n",
            "Epoch: 7/10... Step: 2800... Loss: 0.052763... Val Loss: 0.545926\n",
            "Epoch: 8/10... Step: 3000... Loss: 0.054882... Val Loss: 0.635725\n",
            "Epoch: 8/10... Step: 3200... Loss: 0.086111... Val Loss: 0.581550\n",
            "Epoch: 9/10... Step: 3400... Loss: 0.112004... Val Loss: 0.738883\n",
            "Epoch: 9/10... Step: 3600... Loss: 0.028723... Val Loss: 0.725884\n",
            "Epoch: 10/10... Step: 3800... Loss: 0.008741... Val Loss: 0.778904\n",
            "Epoch: 10/10... Step: 4000... Loss: 0.022817... Val Loss: 0.699795\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTaR4eNhz9rj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b45c5763-57db-4207-c84a-7f8708be295c"
      },
      "source": [
        "test_losses = []\n",
        "\n",
        "num_correct = 0\n",
        "\n",
        "\n",
        "h = model.init_hidden(batch_size)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "for inputs, labels in test_loader:\n",
        "\n",
        "  h = tuple([each.data for each in h])\n",
        "\n",
        "  if(train_on_gpu):\n",
        "    inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "  output, h = model(inputs, h)\n",
        "    \n",
        "  # calculate loss\n",
        "  test_loss = criterion(output.squeeze(), labels.float())\n",
        "  test_losses.append(test_loss.item())\n",
        "  \n",
        "  # convert output probabilities to predicted class (0 or 1)\n",
        "  pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
        "  \n",
        "  # compare predictions to true label\n",
        "  correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "  correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "  num_correct += np.sum(correct)\n",
        "\n",
        "  # -- stats! -- ##\n",
        "  # avg test loss\n",
        "  print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "\n",
        "  # accuracy over all test data\n",
        "  test_acc = num_correct/len(test_loader.dataset)\n",
        "  print(\"Test accuracy: {:.3f}\".format(test_acc))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.878\n",
            "Test accuracy: 0.016\n",
            "Test loss: 0.983\n",
            "Test accuracy: 0.032\n",
            "Test loss: 0.864\n",
            "Test accuracy: 0.049\n",
            "Test loss: 0.776\n",
            "Test accuracy: 0.065\n",
            "Test loss: 0.793\n",
            "Test accuracy: 0.082\n",
            "Test loss: 0.791\n",
            "Test accuracy: 0.097\n",
            "Test loss: 0.716\n",
            "Test accuracy: 0.116\n",
            "Test loss: 0.676\n",
            "Test accuracy: 0.133\n",
            "Test loss: 0.667\n",
            "Test accuracy: 0.150\n",
            "Test loss: 0.655\n",
            "Test accuracy: 0.167\n",
            "Test loss: 0.630\n",
            "Test accuracy: 0.185\n",
            "Test loss: 0.671\n",
            "Test accuracy: 0.200\n",
            "Test loss: 0.686\n",
            "Test accuracy: 0.214\n",
            "Test loss: 0.680\n",
            "Test accuracy: 0.231\n",
            "Test loss: 0.690\n",
            "Test accuracy: 0.247\n",
            "Test loss: 0.691\n",
            "Test accuracy: 0.263\n",
            "Test loss: 0.681\n",
            "Test accuracy: 0.280\n",
            "Test loss: 0.669\n",
            "Test accuracy: 0.298\n",
            "Test loss: 0.671\n",
            "Test accuracy: 0.314\n",
            "Test loss: 0.666\n",
            "Test accuracy: 0.331\n",
            "Test loss: 0.662\n",
            "Test accuracy: 0.348\n",
            "Test loss: 0.666\n",
            "Test accuracy: 0.363\n",
            "Test loss: 0.664\n",
            "Test accuracy: 0.381\n",
            "Test loss: 0.677\n",
            "Test accuracy: 0.396\n",
            "Test loss: 0.684\n",
            "Test accuracy: 0.412\n",
            "Test loss: 0.675\n",
            "Test accuracy: 0.430\n",
            "Test loss: 0.672\n",
            "Test accuracy: 0.446\n",
            "Test loss: 0.669\n",
            "Test accuracy: 0.462\n",
            "Test loss: 0.661\n",
            "Test accuracy: 0.480\n",
            "Test loss: 0.658\n",
            "Test accuracy: 0.498\n",
            "Test loss: 0.650\n",
            "Test accuracy: 0.516\n",
            "Test loss: 0.645\n",
            "Test accuracy: 0.532\n",
            "Test loss: 0.653\n",
            "Test accuracy: 0.548\n",
            "Test loss: 0.653\n",
            "Test accuracy: 0.565\n",
            "Test loss: 0.649\n",
            "Test accuracy: 0.582\n",
            "Test loss: 0.652\n",
            "Test accuracy: 0.597\n",
            "Test loss: 0.653\n",
            "Test accuracy: 0.614\n",
            "Test loss: 0.660\n",
            "Test accuracy: 0.631\n",
            "Test loss: 0.662\n",
            "Test accuracy: 0.646\n",
            "Test loss: 0.665\n",
            "Test accuracy: 0.661\n",
            "Test loss: 0.660\n",
            "Test accuracy: 0.678\n",
            "Test loss: 0.660\n",
            "Test accuracy: 0.695\n",
            "Test loss: 0.659\n",
            "Test accuracy: 0.712\n",
            "Test loss: 0.664\n",
            "Test accuracy: 0.728\n",
            "Test loss: 0.666\n",
            "Test accuracy: 0.744\n",
            "Test loss: 0.670\n",
            "Test accuracy: 0.760\n",
            "Test loss: 0.676\n",
            "Test accuracy: 0.776\n",
            "Test loss: 0.676\n",
            "Test accuracy: 0.792\n",
            "Test loss: 0.680\n",
            "Test accuracy: 0.808\n",
            "Test loss: 0.687\n",
            "Test accuracy: 0.825\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AYfXGcv5vXq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "review = 'this film lacked something i couldn  t put my finger on at first charisma on the part of the leading actress . this inevitably translated to lack of chemistry when she shared the screen with her leading man . even the romantic scenes came across as being merely the actors at play . it could very well have been the director who miscalculated what he needed from the actors . i just don  t know .  br    br   but could it have been the screenplay  just exactly who was the chef in love with  he seemed more enamored of his culinary skills and restaurant  and ultimately of himself and his youthful exploits  than of anybody or anything else . he never convinced me he was in love with the princess .  br    br   i was disappointed in this movie . but  don  t forget it was nominated for an oscar  so judge for yourself .'"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xA6i3HdM7Q71",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(review):\n",
        "\n",
        "  review_test = review.lower()\n",
        "\n",
        "  review_text = ''.join([c for c in review_test if c not in punctuation])\n",
        "\n",
        "  words = review_text.split()\n",
        "\n",
        "  test_ints = []\n",
        "\n",
        "  test_ints.append([vocab_to_int[word] for word in words])\n",
        "\n",
        "  return test_ints"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "961B7V528u9N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(net, test_review, sequence_length=200):\n",
        "    \n",
        "    net.eval()\n",
        "    \n",
        "    # transformando a review em tokens\n",
        "    test_ints = tokenize(test_review)\n",
        "    \n",
        "    seq_length=sequence_length\n",
        "  \n",
        "    #Obtendo as features do tokens da reviews\n",
        "    features = pad_features(test_ints, seq_length)\n",
        "    \n",
        "    # convertendo as features em tensor do pytorch\n",
        "    feature_tensor = torch.from_numpy(features)\n",
        "    \n",
        "    batch_size = feature_tensor.size(0)\n",
        "    \n",
        "    # inicializando a rede neural\n",
        "    h = net.init_hidden(batch_size)\n",
        "    \n",
        "    if(train_on_gpu):\n",
        "        feature_tensor = feature_tensor.cuda()\n",
        "    \n",
        "    # obtendo o output do modelo\n",
        "    output, h = net(feature_tensor, h)\n",
        "    \n",
        "    pred = torch.round(output.squeeze()) \n",
        "    \n",
        "    #retornando a resposta da predição\n",
        "    if(pred.item()==1):\n",
        "        print(\"Review positiva\")\n",
        "    else:\n",
        "        print(\"Review negativa\")"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ajMptxn81ou",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "be822ec7-f972-456f-8ab8-89b8699f5ddf"
      },
      "source": [
        "predict(model, review, 200)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review negativa\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}